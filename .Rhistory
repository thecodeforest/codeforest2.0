filter(product_cd == '943345-001')
sample_prod %>%
tail(13)
b %>%
distinct(retailer_div_id)
b
b <- full_df %>%
filter(pos_bookings_group_name == 'NIKE RETAIL - DIGITAL COMMERCE',
product_cd == '943345-001'
) %>%
View()
b %>%
filter(retailer_div_id == 1006183)
b <- full_df %>%
filter(pos_bookings_group_name == 'NIKE RETAIL - DIGITAL COMMERCE',
product_cd == '943345-001'
)
b %>%
filter(retailer_div_id == 1006183) %>%
tail(13)
sum(b %>%
filter(retailer_div_id == 1006183) %>%
tail(13) %>%
pull(retailer_net_sales_units))
mean(b %>%
filter(retailer_div_id == 1006183) %>%
tail(13) %>%
pull(retailer_net_sales_units))
b %>% tail(13)
b %>% tail(13) %>% pull(activity_end_dt)
b <- b %>% filter(activity_end_dt %in% b %>% tail(13) %>% pull(activity_end_dt))
dts <- b %>% tail(13) %>% pull(activity_end_dt)
b <- b %>% filter(activity_end_dt %in% dts)
b
b$retailer_net_sales_units
sum(b$retailer_net_sales_units)
sum(b$retailer_net_sales_units) / 13
b %>% View()
b %>%
filter(retailer_div_id == 1006183) %>%
View()
b
b %>%
select(-sold_to_nbr)
b %>%
select(-sold_to_nbr) %>%
distinct()
b %>%
select(-sold_to_nbr) %>%
distinct() %>%
pull(retailer_net_sales_units)
sum(b %>%
select(-sold_to_nbr) %>%
distinct() %>%
pull(retailer_net_sales_units)) / 13
pos_df <- read_csv(file.path(base_path, 'SNOWFLAKE_WEEKS_OF_SUPPLY_TS.csv')) %>%
clean_names() %>%
rename(retailer_div_id = retailer_division_id)
pos_df
full_df
bridge_df
bridge_df %>%
filter(pos_bookings_group_name == 'NIKE RETAIL - DIGITAL COMMERCE')
bridge_df %>%
filter(pos_bookings_group_name == 'NIKE RETAIL - DIGITAL COMMERCE') %>%
distinct(retailer_div_id)
pos_df %>%
inner_join(
bridge_df %>%
filter(pos_bookings_group_name == 'NIKE RETAIL - DIGITAL COMMERCE') %>%
distinct(retailer_div_id)
)
pos_df %>%
inner_join(
bridge_df %>%
filter(pos_bookings_group_name == 'NIKE RETAIL - DIGITAL COMMERCE') %>%
distinct(retailer_div_id)
) %>%
filter(product_cd == '943345-001')
pst_13_wks <- pos_df %>%
inner_join(
bridge_df %>%
filter(pos_bookings_group_name == 'NIKE RETAIL - DIGITAL COMMERCE') %>%
distinct(retailer_div_id)
) %>%
filter(product_cd == '943345-001')
pst_13_wks
pst_13_wks %>%
distinct(activity_end_dt) %>%
arrange(activity_end_dt) %>%
tail(13)
pst_13_wks <- pst_13_wks %>%
inner_join(pst_13_wks %>%
distinct(activity_end_dt) %>%
arrange(activity_end_dt) %>%
tail(13))
pst_13_wks
sum(pst_13_wks$retailer_net_sales_units) / 13
5012 / 13
3084 / 13
#Set seed so your answers are all the same
set.seed(9)
# Sample Per class room people
n1 <- 20; n2 <- 20; n3 <- 20; n4 <- 20
N<-n1+n2+n3+n4 # Total N
# Uniform distrobution of proportion of time per classroom
X1 <- runif(n1,  0, .35)
X2 <- runif(n2, .3, .55)
X3 <- runif(n3, .5, .75)
X4 <- runif(n4, .7,1.0)
# noise per classroom
e1 <- rnorm(n1, 0, sd=2.5)
e2 <- rnorm(n2, 0, sd=2.5)
e3 <- rnorm(n3, 0, sd=2.5)
e4 <- rnorm(n4, 0, sd=2.5)
# Intercepts per classroom
B0.1 <- 80
B0.2 <- 70
B0.3 <- 60
B0.4 <- 50
# Same slope per classroom
B1=10
# Our equation to  create Y for each classroom
Y1 = B1*scale(X1,scale=F)  + B0.1 + e1
Y2 = B1*scale(X2,scale=F)  + B0.2 + e2
Y3 = B1*scale(X3,scale=F)  + B0.3 + e3
Y4 = B1*scale(X4,scale=F)  + B0.4 + e4
# Merge classrooms into 1 data.frame
Math.Data<-data.frame(Math=c(Y1,Y2,Y3,Y4),ActiveTime=c(X1,X2,X3,X4),
Classroom=c(rep("C1",n1),rep("C2",n2),rep("C3",n3),rep("C4",n4)),
StudentID=as.factor(1:N))
set.seed(9)
# Sample Per class room people
n1 <- 20; n2 <- 20; n3 <- 20; n4 <- 20
N<-n1+n2+n3+n4 # Total N
# Uniform distrobution of proportion of time per classroom
X1 <- runif(n1,  0, .35)
X2 <- runif(n2, .3, .55)
X3 <- runif(n3, .5, .75)
X4 <- runif(n4, .7,1.0)
# noise per classroom
e1 <- rnorm(n1, 0, sd=2.5)
e2 <- rnorm(n2, 0, sd=2.5)
e3 <- rnorm(n3, 0, sd=2.5)
e4 <- rnorm(n4, 0, sd=2.5)
# Intercepts per classroom
B0.1 <- 80
B0.2 <- 70
B0.3 <- 60
B0.4 <- 50
# Same slope per classroom
B1=10
# Our equation to  create Y for each classroom
Y1 = B1*scale(X1,scale=F)  + B0.1 + e1
Y2 = B1*scale(X2,scale=F)  + B0.2 + e2
Y3 = B1*scale(X3,scale=F)  + B0.3 + e3
Y4 = B1*scale(X4,scale=F)  + B0.4 + e4
# Merge classrooms into 1 data.frame
math_data <-data.frame(Math=c(Y1,Y2,Y3,Y4),ActiveTime=c(X1,X2,X3,X4),
Classroom=c(rep("C1",n1),rep("C2",n2),rep("C3",n3),rep("C4",n4)),
StudentID=as.factor(1:N))
math_data
library(tidyverse)
library(janitor)
library(broom)
lm(Math~ActiveTime, data = math_data) %>% tidy()
math_data
math_data %>%
group_by(Classroom) %>%
summarise(avg_math = mean(Math)) %>%
ungroup()
overall_mean = mean(math_data$Math)
overall_mean = mean(math_data$Math)
math_data %>%
group_by(Classroom) %>%
summarise(avg_math = mean(Math)) %>%
ungroup() %>%
mutate(avg_global = overall_mean)
math_data %>%
group_by(Classroom) %>%
summarise(avg_math = mean(Math)) %>%
ungroup() %>%
mutate(avg_global = overall_mean) %>%
mutate(mu_diff = avg_math - avg_global)
library("lme4")
library("ggplot2")
library("googleVis")
library("stargazer")
library("sjPlot")
library("ggplot2")
sleepstudy <- sleepstudy
90 + 66 + 260 + 635
90 + 66 + 260 + 635 - 100
90 + 66 + 260 + 635 - 70
18300 / 12
271 & 0.27
(271 * 0.27) * 4
28 * 2
50.4 / 0.9
850567096.338 / 1091774709.109435200
79475052.901 / 85862673.387087500
243736112.010 / 332995518.997951400
62043334.412 / 80165690.310330100
getwd()
file.path(getwd(), 'Desktop', 'mbr_data')
base_path = file.path(getwd(), 'Desktop', 'mbr_data')
base_path = file.path(getwd(), 'Desktop', 'mbr_data', 'data')
library(tidyverse)
library(tidyverse)
library(janitor)
base_path = file.path(getwd(), 'Desktop', 'mbr_data', 'data')
# WEEKS ON HAND -----------------------------------------------------------
woh_df <-read_csv(file.path(base_path, 'woh_data.csv')) %>% clean_names()
library(tidyverse)
library(janitor)
base_path = file.path(getwd(), 'Desktop', 'mbr_data', 'data')
# WEEKS ON HAND -----------------------------------------------------------
woh_df <-read_csv(file.path(base_path, 'woh_data.csv')) %>% clean_names()
woh_df
woh_df %>%
mutate(retailer_geo_nm = case_when(retailer_geo_nm == 'GREATER CHINA' ~ 'GC',
retailer_geo_nm == 'NORTH AMERICA' ~ 'NA',
retailer_geo_nm == 'ASIA PACIFIC LATIN AMERICA' ~ 'APLA',
TRUE ~ retailer_geo_nm
))
woh_df %>%
mutate(retailer_geo_nm = case_when(retailer_geo_nm == 'GREATER CHINA' ~ 'GC',
retailer_geo_nm == 'NORTH AMERICA' ~ 'NA',
retailer_geo_nm == 'ASIA PACIFIC LATIN AMERICA' ~ 'APLA',
TRUE ~ retailer_geo_nm
),
division_desc = case_when(division_desc == 'APPAREL DIVISION' ~ 'APP',
division_desc == 'FOOTWEAR DIVISION' ~ 'FTW',
division_desc == 'EQUIPMENT DIVISION' ~ 'EQP',
TRUE ~ division_desc
)
)
WKS = 4
woh_df <-read_csv(file.path(base_path, 'woh_data.csv')) %>% clean_names()
woh_df <- woh_df %>%
mutate(retailer_geo_nm = case_when(retailer_geo_nm == 'GREATER CHINA' ~ 'GC',
retailer_geo_nm == 'NORTH AMERICA' ~ 'NA',
retailer_geo_nm == 'ASIA PACIFIC LATIN AMERICA' ~ 'APLA',
TRUE ~ retailer_geo_nm
),
division_desc = case_when(division_desc == 'APPAREL DIVISION' ~ 'APP',
division_desc == 'FOOTWEAR DIVISION' ~ 'FTW',
division_desc == 'EQUIPMENT DIVISION' ~ 'EQP',
TRUE ~ division_desc
)
)
woh_df
woh_df %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(oh_units, units_sold)
)
woh_df %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(oh_units, units_sold)
) %>%
clean_names()
woh_df
woh_df %>%
mutate(woh = oh_units / (units_sold / 4))
woh_df %>%
mutate(woh = oh_units / (units_sold / 4)) %>%
View()
woh_df %>%
mutate(woh = oh_units / (units_sold / 4)) %>%
filter(express_lane == 'FULFILL') %>%
View()
woh_df %>%
mutate(woh = oh_units / (units_sold / 4)) %>%
filter(express_lane == 'UPDATE') %>%
View()
woh_df_wide <- woh_df %>%
mutate(woh = oh_units / (units_sold / 4)) %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(oh_units, units_sold)
) %>%
clean_names()
woh_df_wide %>% View()
woh_df
woh_df <-read_csv(file.path(base_path, 'woh_data.csv')) %>% clean_names()
woh_df <- woh_df %>%
mutate(retailer_geo_nm = case_when(retailer_geo_nm == 'GREATER CHINA' ~ 'GC',
retailer_geo_nm == 'NORTH AMERICA' ~ 'NA',
retailer_geo_nm == 'ASIA PACIFIC LATIN AMERICA' ~ 'APLA',
TRUE ~ retailer_geo_nm
),
express_lane = ifelse(express_lane == 'NOT EXPRESSLANE', 'INLINE', express_lane),
division_desc = case_when(division_desc == 'APPAREL DIVISION' ~ 'APP',
division_desc == 'FOOTWEAR DIVISION' ~ 'FTW',
division_desc == 'EQUIPMENT DIVISION' ~ 'EQP',
TRUE ~ division_desc
)
)
woh_df_wide <- woh_df %>%
mutate(woh = oh_units / (units_sold / 4)) %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(oh_units, units_sold)
) %>%
clean_names()
woh_df_wide %>% View()
woh_df %>%
mutate(woh = oh_units / (units_sold / 4))
woh_df %>%
mutate(woh = oh_units / (units_sold / 4)) %>%
select(-oh_units, -units_sold)
woh_df %>%
mutate(woh = round(oh_units / (units_sold / 4))) %>%
select(-oh_units, -units_sold)
woh_df_wide <- woh_df %>%
mutate(woh = round(oh_units / (units_sold / 4))) %>%
select(-oh_units, -units_sold) %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(woh)
) %>%
clean_names()
woh_df_wide %>% View()
woh_df %>%
mutate(woh = round(oh_units / (units_sold / 4))) %>%
select(-oh_units, -units_sold) %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(woh)
) %>%
clean_names() %>%
select(retailer_geo_nm,
division_desc,
contains('fulfill'),
contains('update'),
contains('inline')
)
woh_df %>%
mutate(woh = round(oh_units / (units_sold / 4))) %>%
select(-oh_units, -units_sold) %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(woh)
) %>%
clean_names() %>%
select(retailer_geo_nm,
division_desc,
contains('_ty'),
contains('_ly'),
contains('_ly_ly')
)
woh_df_wide <- woh_df %>%
mutate(woh = round(oh_units / (units_sold / 4))) %>%
select(-oh_units, -units_sold) %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(woh)
) %>%
clean_names() %>%
select(retailer_geo_nm,
division_desc,
contains('fulfill'),
contains('update'),
contains('inline')
)
# ISR ---------------------------------------------------------------------
isr_df <- read_csv(file.path(base_path, 'isr_data.csv')) %>% clean_names()
isr_df
woh_df_wide
isr_df
woh_df_wide
isr_df
# ISR ---------------------------------------------------------------------
isr_df <- read_csv(file.path(base_path, 'isr_data.csv')) %>% clean_names()
isr_df <- isr_df %>% mutate(geo = ifelse(is.na(geo), 'NA', geo))
isr_df
woh_df_wide
isr_df
woh_df_wide
woh_df_wide <- woh_df %>%
mutate(woh = round(oh_units / (units_sold / 4))) %>%
select(-oh_units, -units_sold) %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(woh)
) %>%
clean_names() %>%
select(retailer_geo_nm,
division_desc,
contains('fulfill'),
contains('update'),
contains('inline')
) %>%
rename(retailer_geo_nm = geo, division_desc = pe)
woh_df_wide <- woh_df %>%
mutate(woh = round(oh_units / (units_sold / 4))) %>%
select(-oh_units, -units_sold) %>%
pivot_wider(id_cols = c(retailer_geo_nm, division_desc),
names_from = c(express_lane,time_period),
values_from = c(woh)
) %>%
clean_names() %>%
select(retailer_geo_nm,
division_desc,
contains('fulfill'),
contains('update'),
contains('inline')
) %>%
rename(geo = retailer_geo_nm, pe = division_desc)
woh_df_wide
isr_df <- read_csv(file.path(base_path, 'isr_data.csv')) %>% clean_names()
isr_df <- isr_df %>% mutate(geo = ifelse(is.na(geo), 'NA', geo))
isr_df
woh_df_wide
names(woh_df_wide)
col_names[3:length(col_names_woh)]
col_names_woh <- names(woh_df_wide)
col_names[3:length(col_names_woh)]
col_names_woh <- names(woh_df_wide)
col_names[3:length(col_names_woh)]
col_names_woh[3:length(col_names_woh)]
paste0('woh_', col_names_woh[3:length(col_names_woh)])
names(woh_df_wide) = c(col_names_woh[1:2],
paste0('woh_', col_names_woh[3:length(col_names_woh)])
)
woh_df_wide
# PCT_MSRP ----------------------------------------------------------------
pct_msrp_df <- read_csv(file.path(base_path, 'pct_msrp_data.csv')) %>% clean_names()
pct_msrp_df <- pct_msrp_df %>% mutate(geo = ifelse(is.na(geo), 'NA', geo))
pct_msrp_df
isr_df
units_sold_df <- read_csv(file.path(base_path, 'units_sold_data.csv')) %>% clean_names()
units_sold_df <- units_sold_df %>% mutate(geo = ifelse(is.na(geo), 'NA', geo))
units_sold_df
isr_df <- read_csv(file.path(base_path, 'isr_data.csv')) %>% clean_names()
isr_df <- isr_df %>%
mutate(geo = ifelse(is.na(geo), 'NA', geo)) %>%
select(geo,
pe,
contains('fulfill'),
contains('update'),
contains('inline')
)
# UNITS SOLD --------------------------------------------------------------
units_sold_df <- read_csv(file.path(base_path, 'units_sold_data.csv')) %>% clean_names()
units_sold_df <- units_sold_df %>%
mutate(geo = ifelse(is.na(geo), 'NA', geo)) %>%
select(geo,
pe,
contains('fulfill'),
contains('update'),
contains('inline')
)
# PCT_MSRP ----------------------------------------------------------------
pct_msrp_df <- read_csv(file.path(base_path, 'pct_msrp_data.csv')) %>% clean_names()
pct_msrp_df <- pct_msrp_df %>%
mutate(geo = ifelse(is.na(geo), 'NA', geo))  %>%
select(geo,
pe,
contains('fulfill'),
contains('update'),
contains('inline')
)
# FINAL DF ----------------------------------------------------------------
woh_df
woh_df_wide
woh_df_wide %>%
left_join(isr_df) %>%
left_join(units_sold_df) %>%
left_join(pct_msrp_df)
final_df <- woh_df_wide %>%
left_join(isr_df) %>%
left_join(units_sold_df) %>%
left_join(pct_msrp_df)
final_df %>% View()
final_df %>% write_csv(file.path(base_path, 'mbr_data.csv'))
library(tidyverse)
library(blogdown)
# This is ur website
# https://wowchemy.com/docs/getting-started/customization/#website-icon
setwd('/Users/MLeBo1/Desktop/codeforest2.0')
blogdown::serve_site()
data_repo <- "https://raw.githubusercontent.com/thecodeforest/codeforest_datasets/main/state_of_names"
# Create unique path for each state
data_paths <- paste0(file.path(data_repo, datasets::state.abb), '.TXT')
names_raw_df <- data_paths %>%
purrr::map(read_csv, col_names = FALSE) %>%
reduce(rbind)
names_raw_df
knitr::opts_chunk$set(echo = TRUE)
data_repo <- "https://raw.githubusercontent.com/thecodeforest/codeforest_datasets/main/state_of_names"
# Create unique path for each state
data_paths <- paste0(file.path(data_repo, datasets::state.abb), '.TXT')
# Append data from each state into single table
names_raw_df <- data_paths %>%
purrr::map(read_csv, col_names = FALSE) %>%
reduce(rbind)
getwd()
getwd()
getwd()
str_split(getwd(), '/')
str_split(getwd(), '/')[[1:4]]
str_split(getwd(), '/')[1:4]
str_split(getwd(), '/')[[1]][1:4]
str_c(str_split(getwd(), '/')[[1]][1:4])
paste0((str_split(getwd(), '/')[[1]][1:4], sep='/')
paste0(str_split(getwd(), '/')[[1]][1:4], sep='/')
paste0(str_split(getwd(), '/')[[1]][1:4], collapse='/')
# # blogdown::new_site(theme='wowchemy/starter-academic')
blogdown::stop_server()
