time_decompose(Y_VAR,method='stl',frequency = 52, trend='auto') %>%
anomalize(remainder, method = 'iqr', max_anoms = 0.1, alpha = 0.05)
anomaly_detected_df %>%
time_recompose() %>%
plot_anomalies(time_recomposed = TRUE)
perf_df <- bind_cols(anomaly_detected_df,
anomaly_df %>% select(is_anomaly)) %>%
rename(predicted_anomaly = anomaly) %>%
mutate(is_anomaly = ifelse(is.na(is_anomaly), 'No', 'Yes')) %>%
mutate_at(c('predicted_anomaly', 'is_anomaly'), as.factor)
# measure sensitivity and specificity
perf_df %>%
conf_mat(truth=is_anomaly, estimate=predicted_anomaly)
anomaly_detected_df %>%
plot_anomaly_decomposition()
anomaly_detected_df %>%
time_recompose()
# replace all anomalous values with new values
anomaly_detected_df %>%
time_recompose()
# replace all anomalous values with new values
help("time_recompose")
anomaly_detected_df %>%
time_recompose() %>%
mutate(y_var_clean = ifelse(anomaly == 'Yes', season + trend, observed))
anomaly_df$weekly_sales_clean <- anomaly_detected_df %>%
time_recompose() %>%
mutate(y_var_clean = ifelse(anomaly == 'Yes', season + trend, observed)) %>%
pull(y_var_clean)
anomaly_df
anomaly_detected_df %>%
time_recompose()
# forecast with de-anomalized data
anomaly_df
# forecast with de-anomalized data
anomaly_df %>%
pull(weekly_sales_clean)
# forecast with de-anomalized data
anomaly_df %>%
pull(weekly_sales_clean) %>%
ts()
# forecast with de-anomalized data
anomaly_df %>%
pull(weekly_sales_clean) %>%
ts(frequency = 52)
# forecast with de-anomalized data
anomaly_df %>%
pull(weekly_sales_clean) %>%
ts(frequency = 52) %>%
auto.arima()
ar_fit <- anomaly_df %>%
pull(weekly_sales_clean) %>%
ts(frequency = 52) %>%
auto.arima()
forecast(ar_fit, h = TEST_LENGTH)
forecast(ar_fit, h = TEST_LENGTH) %>%
as_tibble()
forecast(ar_fit, h = TEST_LENGTH) %>%
as_tibble() %>%
clean_names()
test$deanom_fcast_values <-
forecast(ar_fit, h = TEST_LENGTH) %>%
as_tibble() %>%
clean_names() %>%
pull(point_forecast)
test
ar_fit <- train %>%
pull(Y_VAR) %>%
ts(frequency = 52) %>%
auto.arima()
test$og_fcast_values <-
forecast(ar_fit, h = TEST_LENGTH) %>%
as_tibble() %>%
clean_names() %>%
pull(point_forecast)
# compare between two methods
test
# compare between two methods
# deanom values
?mae
deanom_mae <- test %>% yardstick::mae(truth=Y_Var, estimate=deanom_fcast_values)
og_mae <- test %>% yardstick::mae(truth=Y_Var, estimate=og_fcast_values)
# deanom values
deanom_mae <- test %>% yardstick::mae(truth=Y_VAR, estimate=deanom_fcast_values)
og_mae <- test %>% yardstick::mae(truth=Y_VAR, estimate=og_fcast_values)
test
# compare between two methods
# deanom values
deanom_mae <- test %>% yardstick::mae(truth=weekly_sales, estimate=deanom_fcast_values)
og_mae <- test %>% yardstick::mae(truth=weekly_sales, estimate=og_fcast_values)
deanom_mae
og_mae
sample_df <- read_csv(DATA_URL) %>%
clean_names() %>%
select(FIELDS) %>%
filter(store %in% STORES,
dept %in% DEPTS
) %>%
mutate(row_index = row_number(),
is_anomaly = ifelse(row_index %in% c(ANOMALY_VALUE_INDEXES), 'Yes', NA)
)
train <- head(sample_df, nrow(sample_df) - test_len)
test <- tail(sample_df, test_len)
anomaly_df <- train
# add in anomalous values
train <- create_anomaly(df=train,
anomaly_index_vector=ANOMALY_VALUE_INDEXES,
y_var=Y_VAR)
anomaly_df <- create_anomaly(df=anomaly_df,
anomaly_index_vector=ANOMALY_VALUE_INDEXES,
y_var=Y_VAR
)
anomaly_df %>%
ggplot(aes(date, weekly_sales, color = is_anomaly)) +
geom_point()
anomaly_detected_df <- anomaly_df %>%
time_decompose(Y_VAR,method='stl',frequency = 52, trend='auto') %>%
anomalize(remainder, method = 'iqr', max_anoms = 0.1, alpha = 0.05)
anomaly_detected_df %>%
plot_anomaly_decomposition()
anomaly_detected_df %>%
time_recompose() %>%
plot_anomalies(time_recomposed = TRUE)
perf_df <- bind_cols(anomaly_detected_df,
anomaly_df %>% select(is_anomaly)) %>%
rename(predicted_anomaly = anomaly) %>%
mutate(is_anomaly = ifelse(is.na(is_anomaly), 'No', 'Yes')) %>%
mutate_at(c('predicted_anomaly', 'is_anomaly'), as.factor)
# measure sensitivity and specificity
perf_df %>%
conf_mat(truth=is_anomaly, estimate=predicted_anomaly)
# replace all anomalous values with new values and assign back to df
anomaly_df$weekly_sales_clean <- anomaly_detected_df %>%
time_recompose() %>%
mutate(y_var_clean = ifelse(anomaly == 'Yes', season + trend, observed)) %>%
pull(y_var_clean)
# forecast with de-anomalized data
ar_fit <- anomaly_df %>%
pull(weekly_sales_clean) %>%
ts(frequency = 52) %>%
auto.arima()
test$deanom_fcast_values <-
forecast(ar_fit, h = TEST_LENGTH) %>%
as_tibble() %>%
clean_names() %>%
pull(point_forecast)
# forecast with orignal data
ar_fit <- train %>%
pull(Y_VAR) %>%
ts(frequency = 52) %>%
auto.arima()
test$og_fcast_values <-
forecast(ar_fit, h = TEST_LENGTH) %>%
as_tibble() %>%
clean_names() %>%
pull(point_forecast)
# compare between two methods
# deanom values
deanom_mae <- test %>% yardstick::mae(truth=weekly_sales, estimate=deanom_fcast_values)
og_mae <- test %>% yardstick::mae(truth=weekly_sales, estimate=og_fcast_values)
# deanom values
deanom_mae <- test %>% yardstick::mae(truth=weekly_sales, estimate=deanom_fcast_values)
print(deanom_mae)
og_mae <- test %>% yardstick::mae(truth=weekly_sales, estimate=og_fcast_values)
print(og_mae)
log(0.9)
-log(0.9)
-log(0.1)
-log(0.001)
-log(0.2)
-log(0.8)
log(0.8)
log(0.1)
log(0.001)
log(0.00001)
library(tidyverse)
base_dir = '/Users/MLeBo1/Desktop/codeforest2.0/content/post/2021-03-26-pyspark-forecasting'
df = read_csv(file.path(base_dir, 'test_df.csv'))
head(df)
df %>%
filter(store == 1, dept == 1)
df %>%
filter(store == 1, dept == 1) %>%
View()
df_input = read_csv(file.path(base_dir, 'input_df.csv'))
df_input %>%
filter(store == 1, dept == 1) %>%
View()
df_input = read_csv(file.path(base_dir, 'input_df.csv'))
df_input %>%
filter(store == 1, dept == 1) %>%
View()
df = read_csv(file.path(base_dir, 'test_df.csv'))
df %>%
filter(store == 1, dept == 1) %>%
View()
library(timetk)
df %>%
filter(store == 1, dept == 1)
df %>%
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = date, .value = y)
df %>%
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = date, .value = y, color = part)
df %>%
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = date, .value = y, .color = part)
?plot_time_series
df %>%
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = date, .value = y, .color_var = part)
df
df %>%
filter(store == 1, dept == 1, part == 'actuals') %>%
plot_time_series(.date_var = date, .value = y, .color_var = part)
df = read_csv(file.path(base_dir, 'test_df.csv'))
df %>%
filter(store == 1, dept == 1, part == 'actuals') %>%
plot_time_series(.date_var = date, .value = y, .color_var = part)
df
df = read_csv(file.path(base_dir, 'test_df.csv'))
df_input = read_csv(file.path(base_dir, 'input_df.csv'))
df_input %>%
filter(store == 1, dept == 1) %>%
plot_time_series()
df_input
df_input %>%
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = ds, .value = y)
df_input %>%
mutate(y = expm1(y))
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = ds, .value = y)
df_input %>%
mutate(y = expm1(y)) %>%
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = ds, .value = y)
df = read_csv(file.path(base_dir, 'test_df.csv'))
df %>%
filter(store == 1, dept == 1, part == 'actuals')
df %>%
filter(store == 1, dept == 1, part == 'actuals') %>%
View()
df_input = read_csv(file.path(base_dir, 'input_df.csv'))
df_input %>%
mutate(y = expm1(y)) %>%
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = ds, .value = y)
df_input
df = read_csv(file.path(base_dir, 'test_df.csv'))
df %>%
filter(store == 1, dept == 1, part == 'actuals')
df %>%
filter(store == 1, dept == 1, part == 'actuals') %>%
View()
df %>%
filter(store == 1, dept == 1, part == 'actuals') %>%
plot_time_series(.date_var = date, .value = y, .color_var = part)
df %>%
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = date, .value = y, .color_var = part)
plot_time_series
df %>%
filter(store == 1, dept == 1) %>%
plot_time_series(.date_var = date, .value = y, .color_var = part, .smooth = FALSE)
df %>%
filter(store == 2, dept == 1) %>%
plot_time_series(.date_var = date, .value = y, .color_var = part, .smooth = FALSE)
df %>%
filter(store == 2, dept == 5) %>%
plot_time_series(.date_var = date, .value = y, .color_var = part, .smooth = FALSE)
df %>%
filter(store == 2, dept == 7) %>%
plot_time_series(.date_var = date, .value = y, .color_var = part, .smooth = FALSE)
df %>%
filter(store == 2, dept == 10) %>%
plot_time_series(.date_var = date, .value = y, .color_var = part, .smooth = FALSE)
df %>%
filter(store == 2, dept == 15) %>%
plot_time_series(.date_var = date, .value = y, .color_var = part, .smooth = FALSE)
df %>%
filter(store == 2, dept == 10) %>%
plot_time_series(.date_var = date, .value = y, .color_var = part, .smooth = FALSE)
df
df %>%
filter(store == 2, dept == 10) %>%
select(date, y:y_ub)
df %>%
filter(store == 2, dept == 10) %>%
select(date, y:y_ub) %>%
pivot_longer(-date, -part)
df %>%
filter(store == 2, dept == 10) %>%
select(date, y:y_ub)
df %>%
filter(store == 2, dept == 10) %>%
select(date, y:y_ub)
df %>%
filter(store == 2, dept == 10)
df %>%
filter(store == 2, dept == 10) %>%
select(-part)
df %>%
filter(store == 2, dept == 10) %>%
select(-part) %>%
select(date, y:y_ub)
df %>%
filter(store == 2, dept == 10) %>%
select(-part) %>%
select(date, y:y_ub) %>%
pivot_longer(-date)
df %>%
filter(store == 2, dept == 10) %>%
select(-part) %>%
select(date, y:y_ub) %>%
pivot_longer(-date) %>%
plot_time_series(.date_var = date, .value = y, .color_var = name, .smooth = FALSE)
df %>%
filter(store == 2, dept == 10) %>%
select(-part) %>%
select(date, y:y_ub) %>%
pivot_longer(-date) %>%
plot_time_series(.date_var = date, .value = value, .color_var = name, .smooth = FALSE)
df %>%
filter(store == 2, dept == 2) %>%
select(-part) %>%
select(date, y:y_ub) %>%
pivot_longer(-date) %>%
plot_time_series(.date_var = date, .value = value, .color_var = name, .smooth = FALSE)
df %>%
filter(store == 2, dept == 2) %>%
select(-part)
df
df = read_csv(file.path(base_dir, 'data', 'output', 'sales_data_raw.csv'))
df
df %>%
filter(store == 2, dept == 2) %>%
select(-part) %>%
select(date, y:y_ub) %>%
pivot_longer(-date) %>%
plot_time_series(.date_var = date, .value = value, .color_var = name, .smooth = FALSE)
df
df = read_csv(file.path(base_dir, 'data', 'output', 'sales_data_raw.csv'))
df %>%
filter(store == 2, dept == 2)
df %>%
filter(store == 2, dept == 2) %>%
select(-part) %>%
select(date, y:y_ub) %>%
pivot_longer(-date) %>%
plot_time_series(.date_var = date, .value = value, .color_var = name, .smooth = FALSE)
df %>%
filter(store == 2, dept == 2) %>%
select(-part)
df %>%
filter(store == 2, dept == 2) %>%
select(-part) %>%
select(date, contains('weekly')) %>%
pivot_longer(-date)
df %>%
filter(store == 2, dept == 2) %>%
select(-part) %>%
select(date, contains('weekly')) %>%
pivot_longer(-date) %>%
plot_time_series(.date_var = date, .value = value, .color_var = name, .smooth = FALSE)
df %>%
filter(store == 1, dept == 2) %>%
select(-part) %>%
select(date, contains('weekly')) %>%
pivot_longer(-date) %>%
plot_time_series(.date_var = date, .value = value, .color_var = name, .smooth = FALSE)
df %>%
filter(store == 1, dept == c(2, 3, 4)) %>%
select(-part)
df
df %>%
distinct(store, dept)
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4))
plot_time_series
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_'))
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_'))
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_'))
select(-part) %>%
select(date, contains('weekly')) %>%
pivot_longer(-date, -store_id)
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_')) %>%
select(-part) %>%
select(date, contains('weekly')) %>%
pivot_longer(-date, -store_id)
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_')) %>%
select(-part) %>%
select(date, contains('weekly'))
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_')) %>%
select(-part) %>%
select(date, store_id, contains('weekly')) %>%
pivot_longer(-date, -store_id)
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_')) %>%
select(-part) %>%
select(date, store_id, contains('weekly'))
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_')) %>%
select(-part) %>%
select(date, store_id, contains('weekly')) %>%
View()
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_')) %>%
select(-part) %>%
select(date, store_id, contains('weekly'))
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_')) %>%
select(-part) %>%
select(date, store_id, contains('weekly')) %>%
pivot_longer(contains('weekly'))
plot_time_series
df %>%
filter(store == 1, dept %in% c(1, 2, 3, 4)) %>%
mutate(store_id = paste(store, dept, sep='_')) %>%
select(-part) %>%
select(date, store_id, contains('weekly')) %>%
pivot_longer(contains('weekly')) %>%
plot_time_series(.date_var = date,
.value = value,
.color_var = name,
.facet_vars = store_id,
.smooth = FALSE)
library(here)
rbinom(10, prob=0.5)
rbinom(10, size = 5,prob=0.5)
rbinom(1, size = 5,prob=0.5)
rbinom(10, size = 5,prob=0.5)
rbinom(10, size = 5,prob=0.5)
rbinom(10, size = 5,prob=0.5)
rbinom(10, 2,prob=0.5)
rbinom(10, 1,prob=0.5)
c(rbinom(10, 1,prob=0.02), rbinom(10, 1, 0.01))
c(rbinom(10, 1,prob=0.1),
rbinom(10, 1, 0.05),
)
c(rbinom(10, 1,prob=0.1),
rbinom(10, 1, 0.05)
)
c(rbinom(100, 1,prob=0.1),
rbinom(100, 1, 0.05)
)
time_presentation = c(rep('pre-5pm', 100),
rep('post-5pm', 100))
library(tidyverse)
library(timetk)
library(here)
library(rjson)
library(blogdown)
library(lubridate)
# Use this as a reference for ur blog
# https://github.com/lmyint/personal_site/blob/master/config/_default/params.toml
# config_params = fromJSON(file = file.path(here::here(), 'config', 'conf.json'))
# df = read_csv(config_params$input_data_path)
setwd('/Users/MLeBo1/Desktop/codeforest2.0')
serve_site('/Users/MLeBo1/Desktop/codeforest2.0')
# build_site('/Users/MLeBo1/Desktop/codeforest2.0')
stop_server()
# Use this as a reference for ur blog
# https://github.com/lmyint/personal_site/blob/master/config/_default/params.toml
# config_params = fromJSON(file = file.path(here::here(), 'config', 'conf.json'))
# df = read_csv(config_params$input_data_path)
setwd('/Users/MLeBo1/Desktop/codeforest2.0')
serve_site('/Users/MLeBo1/Desktop/codeforest2.0')
# build_site('/Users/MLeBo1/Desktop/codeforest2.0')
stop_server()
# build_site('/Users/MLeBo1/Desktop/codeforest2.0')
stop_server()
setwd('/Users/MLeBo1/Desktop/codeforest2.0')
serve_site('/Users/MLeBo1/Desktop/codeforest2.0')
# build_site('/Users/MLeBo1/Desktop/codeforest2.0')
stop_server()
# Use this as a reference for ur blog
# https://github.com/lmyint/personal_site/blob/master/config/_default/params.toml
# config_params = fromJSON(file = file.path(here::here(), 'config', 'conf.json'))
# df = read_csv(config_params$input_data_path)
setwd('/Users/MLeBo1/Desktop/codeforest2.0')
serve_site('/Users/MLeBo1/Desktop/codeforest2.0')
# blogdown::new_site(theme='wowchemy/starter-academic')
blogdown::stop_server()
setwd('/Users/MLeBo1/Desktop/codeforest2.0')
blogdown::serve_site()
# blogdown::new_site(theme='wowchemy/starter-academic')
blogdown::stop_server()
library(tidyverse)
library(blogdown)
library(timetk)
# This is ur website
# https://wowchemy.com/docs/getting-started/customization/#website-icon
setwd('/Users/MLeBo1/Desktop/codeforest2.0')
blogdown::serve_site()
