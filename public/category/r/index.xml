<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | The Code Forest</title>
    <link>http://example.org/category/r/</link>
      <atom:link href="http://example.org/category/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Mark LeBoeuf</copyright><lastBuildDate>Mon, 15 Mar 2021 21:13:14 -0500</lastBuildDate>
    <image>
      <url>http://example.org/media/icon_huc737709a4be44af6221d1cabfe197959_22580_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>http://example.org/category/r/</link>
    </image>
    
    <item>
      <title>Causal Inference with Propensity Scores</title>
      <link>http://example.org/post/2021-05-01-propensity-scores/causal_inference_propensity_scores/</link>
      <pubDate>Mon, 15 Mar 2021 21:13:14 -0500</pubDate>
      <guid>http://example.org/post/2021-05-01-propensity-scores/causal_inference_propensity_scores/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;http://example.org/post/2021-03-15-causal-inference-pt-1/images/dag.png&#34; width=&#34;700&#34; height=&#34;600&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Causal inference attempts to answer ‚Äúwhat-if‚Äù questions. For example, if the minimum wage were increased, what effect would it have on unemployment rates? Or if an entertainment company launched a marketing campaign for a new movie, what effect would it have on box-office sales? The objective in each of these examples is to quantify the impact of an intervention ‚Äì a change in wages or a targeted marketing campaign ‚Äì on an outcome ‚Äì increasing employment or bolstering revenue. Estimating how a particular action can affect an end-state falls within the realm of &lt;em&gt;prescriptive&lt;/em&gt; analytics and can inform decision-making in the face of multiple possible actions.&lt;/p&gt;
&lt;p&gt;However, most analytics efforts are applied to either &lt;em&gt;describing&lt;/em&gt; or &lt;em&gt;predicting&lt;/em&gt; an outcome rather than understanding what drives it. For example, imagine you work for a cheese shop. You might be asked to &lt;em&gt;describe&lt;/em&gt; how sales of cheese have changed over the past year. Or perhaps you want to &lt;em&gt;predict&lt;/em&gt; how much cheese will sell over the next 12 months. Descriptive analytics can reveal if existing operational or strategic decisions are impacting the business (i.e., cheese sales) as anticipated. Predictive analytics can inform operational planning (e.g., how much cheese to manufacture), improve consumer experiences (e.g., an online cheese recommendation system), or automate repetitive tasks (e.g., automatically detecting defective cheese wheels during production with computer vision). While all of the applications can provide valuable answers to different questions, none can provide insight into the source of variation or root cause(s) of change in an outcome. Without this knowledge, it can be difficult to know where resources should be focused or how to grow and improve the business.&lt;/p&gt;
&lt;p&gt;Accordingly, the goal of this post is to highlight one approach to conducting prescriptive analytics and generating causal inferences with observational data. We‚Äôll first walk through some of the basics of causal inference and propensity scores, followed by a practical example that brings these concepts together. At the end of this post, you should have a solid understanding of how propensity scores can be used in the real world to guide decision-making.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-inference-propensity-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Causal Inference &amp;amp; Propensity Scores&lt;/h3&gt;
&lt;p&gt;When people hear the words ‚Äúcausal inference‚Äù, they often think ‚ÄúA/B Test‚Äù. Indeed, the traditional way of answering causal questions is to randomly assign individuals to a treatment or control condition. The treatment is exposed to the intervention, while the control is not. The average difference is then calculated between the two conditions on some measure of interest to understand if the intervention had the desired effect.&lt;/p&gt;
&lt;p&gt;While A/B testing is considered the most rigorous way of inferring causation, it is not practical or possible in many situations. For example, if you were interested in the effect of a membership program on future purchasing behavior, you cannot assign customers to be a member or non-member; customers would enroll in the program under their own volition. Further, customers who enrolled as members are probably more interested in the product than those who did not enroll. This fact ‚Äúconfounds‚Äù the relationship between the effect of our member program on purchasing behavior.&lt;/p&gt;
&lt;p&gt;Propensity score matching attempts to address this issue, known as &lt;em&gt;selection bias&lt;/em&gt;, by adjusting for factors that relate both to the treatment and outcome (i.e., confounding variables). A propensity score is scaled from 0 - 1 and indicates the probability of receiving treatment. Continuing with our previous membership example, a propensity score indicates the probability that a customer joins our membership program after seeing a banner on our website or receiving a promotional email. It does not indicate their probability of making a future purchase. Formalizing the roles of individual variables that increase/decrease membership enrollment and their interrelations is the topic of the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-graphs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Causal Graphs&lt;/h3&gt;
&lt;p&gt;We can codify our beliefs and assumptions about observational data through a &lt;em&gt;causal graph&lt;/em&gt;. This is normally the first step on our journey of causal inference, as it allows us to translate domain knowledge into a formal structure. By creating a diagram about potential confounding variables as well as the direction of causal influence, we make our assumptions about the data generating process explicit.&lt;/p&gt;
&lt;p&gt;In the context of the current example, we assume that a customer in enrolling as a member influences future purchase behavior, not that future purchase behavior influences enrollment in membership. We can then encode this assumption in our causal graph. The exclusion of certain variables from our graph (e.g., age, gender, what types of products someone has previously purchased, etc.) is also an assumption, such that we assume these variables do not directly or indirectly affect purchase frequency or membership.&lt;/p&gt;
&lt;p&gt;These assumptions can and should be verified. If we believe a customer‚Äôs age affects purchase frequency and membership enrollment, we can stratify our customers by age (i.e,., 20-29, 30-39) and test both hypotheses. If there were significant differences between groups, we would include an age variable in our graph and adjust for its influence on the treatment and outcome.&lt;/p&gt;
&lt;p&gt;This is a contrived example, so we‚Äôll keep things simple and formalize the main components of our analysis as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2021-03-15-causal-inference-pt-1/images/dag.png&#34; width=&#34;700&#34; height=&#34;600&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Purchase Frequency&lt;/strong&gt; - the total number of purchases six months following the launch of our membership program. This is our outcome variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Membership&lt;/strong&gt; - if a customer enrolled as a member since the launch of the membership program. This is our treatment variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Engagement&lt;/strong&gt; - this is an example of a latent variable. We would use several variables in practice but, to keep things simple, we‚Äôll only use prior purchase history, defined as the total number of purchases in the six months before the launch of the membership program. This variable will serve as a proxy for Engagement. We assume that customers who have made more purchases in the past six months will be inclined to make more purchases in the future ‚Äì that is, more engaged in the past translates into more engaged in the future. We also assume that this (partially) motivates membership enrollment. Engaged customers will not only purchase more frequently but also be more interested in exclusive offers and discounts ‚Äì a few benefits provided to members ‚Äì relative to customers that have historically purchased infrequently.&lt;/p&gt;
&lt;p&gt;The image above was created via the &lt;a href=&#34;http://dagitty.net/&#34;&gt;daggity website&lt;/a&gt;, which makes it easy to create Causal DAGs or Directed Acyclic Graphs. Note the goal of creating a propensity score is to block the arrow from &lt;strong&gt;Engagement&lt;/strong&gt; to &lt;strong&gt;Purchase Frequency&lt;/strong&gt;. This addresses the issue of &lt;em&gt;selection bias&lt;/em&gt;, in that our customers can ‚Äúselect into‚Äù the member condition. By adjusting for this pre-existing difference, we are attempting to make this bias &lt;em&gt;strongly ignorable&lt;/em&gt;, similar to a randomized experiment.&lt;/p&gt;
&lt;p&gt;Another aspect to consider is when an individual joined our membership program. We want to allow enough time for differences to emerge, so ideally a few months have elapsed so we can see what happens. Second, membership offers and the quality may change over time, just as the consumer‚Äôs relationship with our brand changes. By narrowing the time frame of analysis, we can further control for time-related factors.&lt;/p&gt;
&lt;p&gt;Last, we want to time-bound prior purchase history. Some customers may have frequently purchased in the past but have not been active for several years (or churned completely). We want to ensure that all customers in our sample have a chance of being exposed to the treatment. Thus, we could apply simple logic to narrow our consideration set, such as ‚Äúall customers that have engaged with the brand in some capacity (e.g., made a purchase, browsed the website, or opened a marketing communication) since the start of our member program‚Äù. This is not a hard-and-fast rule but something to consider when deciding which individuals to include in your analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-the-effect-of-membership-on-purchase-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating the Effect of Membership on Purchase Frequency&lt;/h3&gt;
&lt;p&gt;Now that we have a solid conceptual foundation, let‚Äôs continue to work through our membership example by generating some contrived data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(broom)
library(rsample)
library(janitor)
# set base theme as black &amp;amp; white
theme_set(theme_bw())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2021)
# sample size of members and non-members
n = 5000
# expected purchase frequency
base_lambda = .75
# purchase frequency effect size for &amp;quot;more engaged&amp;quot; customers 
engagement_effect_size = .25

less_engaged = rpois(n=n, lambda = base_lambda)
more_engaged = rpois(n=n, lambda = base_lambda + engagement_effect_size)
# create tibble with number of previous purchases for each customer
purchase_df &amp;lt;- tibble(n_purchase_pre = c(less_engaged, more_engaged))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the code block above, we expect ‚Äúless engaged‚Äù customers to make 0.75 purchases (on average) over six months, and ‚Äúmore engaged‚Äù customers to make one purchase over the same period. The difference in purchase frequency between our customer types is ascribed to our latent variable of Engagement. We assume the data generating process for historical purchase frequency can be represented by the Poisson distribution. Recall that the Poisson distribution models the number of events expected to occur within a given period. It also approximates consumer purchase frequency patterns in the real world, such that most customers make a small number of purchases, while a few customers make a large number of purchases.&lt;/p&gt;
&lt;p&gt;We established our expected purchase frequency and engagement effect size above,so let‚Äôs simulate the effect of Engagement on Membership. We‚Äôll create three bins and assign a probability of enrolling as a member within each bin, such that higher bins (i.e., the top 33% of customers) have a higher probability of enrolling relative to lower bins (i.e., the bottom 33% of customers).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;membership_sim &amp;lt;- function(bin){
  if(bin == 1){
    return(rbinom(1, 1, prob = 0.2))
  } else if (bin == 2){
    return(rbinom(1, 1, prob = 0.3))
  } else {
    return(rbinom(1, 1, prob = 0.4))
  }
}

purchase_df &amp;lt;- purchase_df %&amp;gt;% 
  mutate(bin = ntile(n_purchase_pre, 3),
         member_enrolled = map_int(bin, membership_sim)
         ) %&amp;gt;% 
  select(-bin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While we know the true effect size (given that we generated the numbers), let‚Äôs verify the assumption that our proxy for engagement (Prior Purchases) exhibits the hypothesized effect on membership.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;purchase_df %&amp;gt;% 
  mutate(n_purchase_pre = fct_lump(as.factor(n_purchase_pre), 
                                   n=2,
                                   other_level = &amp;#39;2 or More&amp;#39;
                                   )) %&amp;gt;% 
  group_by(n_purchase_pre) %&amp;gt;% 
  summarise(pct_member = mean(member_enrolled)) %&amp;gt;% 
  ggplot(aes(n_purchase_pre, pct_member)) + 
  theme_bw() + 
  geom_col() + 
  coord_flip() + 
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + 
  labs(x = &amp;#39;N Prior Purchases&amp;#39;,
       y = &amp;#39;Percent Enrolled as Members&amp;#39;
       ) + 
  theme(axis.text.x = element_text(size = 12, angle = 90),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 16),
    strip.text.y = element_text(size = 14)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2021-05-01-propensity-scores/causal_inference_propensity_scores_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have validated the relationship between these variables, we‚Äôll create the joint effect of treatment (Membership) and our single covariate (Engagement) on our outcome (Purchase Frequency).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;purchase_sim &amp;lt;- function(n_purchase_pre, member_enrolled){
  membership_effect_size = .1
  if(member_enrolled == 1){
    post_purchase_freq = rpois(n=1, lambda=n_purchase_pre + membership_effect_size)
    return(post_purchase_freq)
  } else {
    post_purchase_freq = rpois(n=1, lambda=n_purchase_pre)
    return(post_purchase_freq)
  }
}

purchase_df &amp;lt;- purchase_df %&amp;gt;% 
  mutate(n_purchase_post = map2_int(n_purchase_pre, 
                                member_enrolled, 
                                purchase_sim)
         )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;purchase_sim&lt;/code&gt; function accounts for the number of prior purchases as well as if the customer has enrolled as a member. If they have enrolled, the true effect size is .1, in that enrolling a customer as a member leads to .1 additional purchases (on average) during the six months.&lt;/p&gt;
&lt;p&gt;Below, we‚Äôll confirm that members have made more purchases relative to non-members.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg_purchase_freq &amp;lt;- purchase_df %&amp;gt;% 
  group_by(member_enrolled) %&amp;gt;% 
  summarise(avg_post_purchase = mean(n_purchase_post),
            se = sqrt(mean(n_purchase_post) / n())
            ) %&amp;gt;% 
  mutate(is_member = str_to_title(ifelse(member_enrolled == 1, 
                                         &amp;#39;member&amp;#39;, 
                                         &amp;#39;non-member&amp;#39;)),
         is_member = fct_reorder(is_member, avg_post_purchase),
         lb = avg_post_purchase - 1.96 * se,
         ub = avg_post_purchase + 1.96 * se
         )

avg_purchase_freq %&amp;gt;% 
  ggplot(aes(is_member, avg_post_purchase, fill = is_member)) + 
  geom_col(color = &amp;#39;black&amp;#39;) + 
  geom_errorbar(aes(ymin = lb, ymax=ub), width=0.4) + 
  coord_flip() + 
  labs(x = NULL,
       y = &amp;#39;Average Number of Purchases&amp;#39;,
       fill = &amp;#39;Membership Status&amp;#39;
  ) + 
  theme(axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        legend.text = element_text(size = 14),
        legend.title = element_text(size = 16),
        legend.position = &amp;#39;top&amp;#39;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2021-05-01-propensity-scores/causal_inference_propensity_scores_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Indeed, this confirms that the data aligns with our expectations. We could reach the same conclusion by fitting a regression model and then considering the magnitude of the coefficient for membership.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;incorrect_fit &amp;lt;- glm(n_purchase_post ~ member_enrolled,
                     data = purchase_df,
                     family = &amp;quot;poisson&amp;quot;
                     )

incorrect_fit_coef &amp;lt;- tidy(incorrect_fit)

intercept &amp;lt;- incorrect_fit_coef %&amp;gt;% 
  filter(term == &amp;#39;(Intercept)&amp;#39;) %&amp;gt;% 
  pull(estimate)

member_est &amp;lt;- incorrect_fit_coef %&amp;gt;% 
  filter(term == &amp;#39;member_enrolled&amp;#39;) %&amp;gt;% 
  pull(estimate)

effect_size_est &amp;lt;- round(exp(intercept + member_est) - exp(intercept), 3)
print(glue::glue(&amp;#39;Estimated Difference: {effect_size_est}&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated Difference: 0.402&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that if we did not adjust for any confounding variables, we would &lt;strong&gt;mistakenly conclude&lt;/strong&gt; that membership leads to ~.3 extra purchases per customer. That is, we would overestimate the effect size of membership on customer purchase behaviors because we know that Engagement affects both Membership and Purchase Frequency. In the following section, we‚Äôll generate propensity scores to create more balance between our control and treatment groups on our confounding variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Propensity Scores&lt;/h3&gt;
&lt;p&gt;Two common approaches for creating comparison groups with observational data are (1) &lt;em&gt;propensity score matching&lt;/em&gt; and (2) &lt;em&gt;inverse probability of treatment weighting (IPTW)&lt;/em&gt;. Both methods use a propensity score but create groups differently. Matching looks for individuals in the non-treated condition who have similar propensity scores to those in the treated condition. If groups are different sizes, the number of non-treated observations is reduced to the size of the treated condition, as each treated observation is matched with a non-treated observation. In contrast, weighting includes all observations but places more weight on observations with high propensity scores and less weight on observations with low propensity scores. While both approaches can yield similar results, I prefer weighting because you are not discarding any data.&lt;/p&gt;
&lt;p&gt;Below, we‚Äôll specify our model for generating propensity scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specify model for estimating P(treatment | Previous Purchases)
model_spec &amp;lt;- as.formula(member_enrolled ~ n_purchase_pre)
member_model &amp;lt;- glm(model_spec, 
                    data = purchase_df, 
                    family = binomial())
member_prop_df &amp;lt;- member_model %&amp;gt;% 
  augment(type.predict = &amp;#39;response&amp;#39;, data = purchase_df) %&amp;gt;% 
  select(member_enrolled, n_purchase_pre, n_purchase_post, member_prob = .fitted) %&amp;gt;% 
  mutate(iptw = 1 / ifelse(member_enrolled == 0, 1 - member_prob, member_prob))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A few things to note since we‚Äôve created our propensity scores. First, we used logistic regression to estimate membership probability. However, any classification model can generate a propensity score. If there are non-linearities between your covariates and treatment variable, using a model that can better capture these relationships, such as a tree-based model, may yield better estimates.&lt;/p&gt;
&lt;p&gt;Second, we‚Äôll need to be cognizant of the resulting weights. If certain observations receive very large weights, they will have an outsized influence on our coefficient estimates. It is a common practice to truncate large weights at 10 (why 10 I‚Äôm not sure). I‚Äôd prefer to use a point from our actual distribution, so we‚Äôll assign any value above the 99th percentile to the value at the 99th percentile.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iptw_pct_99 &amp;lt;- quantile(member_prop_df %&amp;gt;% pull(iptw), 0.99)[[1]]
member_prop_df &amp;lt;- member_prop_df %&amp;gt;% 
  mutate(iptw = ifelse(iptw &amp;gt; iptw_pct_99, iptw_pct_99, iptw))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we‚Äôve addressed some common pre-modeling issues, let‚Äôs generate an initial estimate of the effect of Membership on Purchase Frequency.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;membership_fit &amp;lt;- glm(n_purchase_post ~ member_enrolled,
              data = member_prop_df,
              family = &amp;#39;poisson&amp;#39;,
              weights = iptw
              )
membership_fit_coef &amp;lt;- tidy(membership_fit)

intercept &amp;lt;- membership_fit_coef %&amp;gt;% 
  filter(term == &amp;#39;(Intercept)&amp;#39;) %&amp;gt;% 
  pull(estimate)

member_est &amp;lt;- membership_fit_coef %&amp;gt;% 
  filter(term == &amp;#39;member_enrolled&amp;#39;) %&amp;gt;% 
  pull(estimate)

effect_size_est_adj &amp;lt;- round(exp(intercept + member_est) - exp(intercept), 3)
print(glue::glue(&amp;#39;Estimated Difference: {effect_size_est_adj}&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated Difference: 0.097&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our initial estimate indicates that membership leads to ~.1 extra purchases, which matches perfectly with the true effect size! This is a great start, but we also need to consider certainty in the estimate. In our example, we have a fairly large sample size, there is only one covariate, and the distribution of treatment (members vs.¬†non-members) is relatively even between groups. Real world data sets, on the other hand, are often small, present a high degree of skew between groups, or exhibit intricate causal structures. The presence of these factors affects how accurately we can estimate a true effect, and using the method above to create a confidence interval can lead to incorrect estimates of our standard error (too small).
To address this issue, we‚Äôll bootstrap the entire process - from generating our propensity score weights to estimating our causal effect - and then use the resulting distribution to better quantify uncertainty in our estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;member_fit_bootstrap &amp;lt;- function(split){
  temp_df &amp;lt;- analysis(split)
  
  temp_model &amp;lt;- glm(member_enrolled ~ n_purchase_pre,
                    family = binomial(),
                    data = temp_df
                  )
  
  temp_df &amp;lt;- temp_model %&amp;gt;% 
    augment(type.predict = &amp;#39;response&amp;#39;, data = temp_df) %&amp;gt;% 
    select(member_enrolled, n_purchase_pre, 
           n_purchase_post, member_prob = .fitted) %&amp;gt;% 
    mutate(iptw = 1 / ifelse(member_enrolled == 0, 
                             1 - member_prob, 
                             member_prob))
  
  temp_iptw_pct_99 &amp;lt;- quantile(temp_df %&amp;gt;% pull(iptw), 0.99)[[1]]
  
  temp_df &amp;lt;- temp_df %&amp;gt;% 
    mutate(iptw = ifelse(iptw &amp;gt; temp_iptw_pct_99, 
                         temp_iptw_pct_99, 
                         iptw))
  
  temp_ret_df &amp;lt;- glm(n_purchase_post ~ member_enrolled,
                data = temp_df,
                family = &amp;#39;poisson&amp;#39;,
                weights = iptw) %&amp;gt;% 
    tidy()
  return(temp_ret_df)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_boot = 500
boot_results &amp;lt;- bootstraps(purchase_df, n_boot, apparent = TRUE) %&amp;gt;% 
  mutate(results = map(splits, member_fit_bootstrap))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code snippet, we created 500 boot-strapped replicates and then fit a model to each. Our next step is to look at the distribution of the resulting estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boot_results_unnest &amp;lt;- 
  boot_results %&amp;gt;% 
  select(-splits) %&amp;gt;% 
  unnest(cols=results)

boot_results_est &amp;lt;- 
  boot_results_unnest %&amp;gt;% 
  select(id, term, estimate) %&amp;gt;% 
  pivot_wider(names_from = term, 
              values_from = estimate
              ) %&amp;gt;% 
  clean_names() %&amp;gt;% 
  mutate(est_effect = exp(member_enrolled + intercept) - exp(intercept))


boot_results_summary &amp;lt;- boot_results_est %&amp;gt;%
  summarise(lb = quantile(est_effect, 0.025),
            mdn = quantile(est_effect, 0.5),
            ub = quantile(est_effect, 0.975)
  ) %&amp;gt;%
  pivot_longer(everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lb_est &amp;lt;- boot_results_summary %&amp;gt;% filter(name==&amp;#39;lb&amp;#39;) %&amp;gt;% pull(value)
ub_est &amp;lt;- boot_results_summary %&amp;gt;% filter(name==&amp;#39;ub&amp;#39;) %&amp;gt;% pull(value)
effect_est &amp;lt;- boot_results_summary %&amp;gt;% filter(name==&amp;#39;mdn&amp;#39;) %&amp;gt;% pull(value)
boot_results_est %&amp;gt;% 
  ggplot(aes(x=est_effect)) + 
  geom_histogram(fill=&amp;#39;grey90&amp;#39;, color = &amp;#39;black&amp;#39;) + 
  theme_bw() + 
  geom_segment(aes(x=lb_est, xend = lb_est), y = 0, yend = 10, lty = 3, size = 2) + 
  geom_segment(aes(x=effect_est, xend = effect_est), y = 0, yend = 10, lty = 3, size = 2) + 
  geom_segment(aes(x=ub_est, xend = ub_est), y = 0, yend = 10, lty = 3, size = 2) + 
  annotate(geom=&amp;#39;text&amp;#39;, x=lb_est, y = 11, label = round(lb_est, 2), size = 8) + 
  annotate(geom=&amp;#39;text&amp;#39;, x=effect_est, y = 11, label = round(effect_est, 2), size = 8) + 
  annotate(geom=&amp;#39;text&amp;#39;, x=ub_est, y = 11, label = round(ub_est, 2), size = 8) + 
  labs(x = &amp;#39;Estimated Treatment Effect&amp;#39;) + 
  theme(
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2021-05-01-propensity-scores/causal_inference_propensity_scores_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our final estimate of the causal effect of membership on purchase frequency is between 0.06 and 0.14. Note that these confidence intervals aren‚Äôt much different from those in the original, non-bootstrapped approach. As mentioned previously, the primary reason is that our sample size for both groups is fairly large and there is not a lot of variance in the determinants of membership, given that we generated the data. However, in many instances, confidence intervals following bootstrapping will be wider ‚Äì and more accurate ‚Äì than those provided by OLS.&lt;/p&gt;
&lt;p&gt;Hopefully, this provided a solid end-to-end walkthrough of how to generate causal inferences from observational data with propensity scores. In the next post, we‚Äôll discuss an equally important topic ‚Äì variance reduction methods ‚Äì that come in handy when you can run a true A/B test. Until then, happy experimenting!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Choosing a Fantasy Football Kicker with Emperical Bayes Estimation</title>
      <link>http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers/</link>
      <pubDate>Mon, 26 Aug 2019 21:13:14 -0500</pubDate>
      <guid>http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;http://example.org/post/2019-08-29-fantasy-football-kickers/images/header_image.jpg&#34; width=&#34;700&#34; height=&#34;400&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;In less than two weeks, Fantasy Football will once again resume for the 2019 NFL season! While I‚Äôm looking forward to the impending draft, the start of the season brings back memories of a not-so-distant loss that left me one game shy of the championship. The loss stemmed from a missed field goal, leaving my team two points shy of victory. Of course, a myriad of factors beyond that missed field goal contributed to my fantasy demise, but those two points reinvigorated a question I‚Äôve wondered about for the past few years: Why are kickers drafted in the last round?&lt;/p&gt;
&lt;p&gt;Prevailing wisdom suggests that your kicker doesn‚Äôt matter. Some Fantasy Football leagues don‚Äôt even have kickers on the roster, which I think does a disservice to a player who probably doesn‚Äôt get invited to the cool team parties yet can decide the fate of a season in a single moment (like mine). As long as they suit up to take the field, the rest is out of your control. However, is it a suboptimal strategy to relegate your choice of kicker to the final round of the draft? Let‚Äôs find out!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting Started&lt;/h3&gt;
&lt;p&gt;Before loading any data or discussing techniques, we‚Äôll begin by defining our analytical objective. An easy way to get started is by posing a simple question: ‚ÄúHow many more points can I expect over a 16-game regular season if I draft the best kicker relative to the worst kicker?‚Äù We‚Äôll answer this question in two steps. First, we‚Äôll estimate the &lt;em&gt;True&lt;/em&gt; field goal percentage for each kicker currently active in the NFL (as of 2016), which is analogous to a batting average in baseball or free-throw percentage in basketball. This parameter estimate will be used to compare the skill of one kicker to another. Second, we‚Äôll translate our estimate into actual Fantasy Football points by simulating the outcomes 1000 football seasons for each kicker. Simulation enables us to quantify a realistic point differential between kickers, which is what we (the Fantasy Football team owners) will use to determine if we should try to select the best kicker by drafting in an earlier round.&lt;/p&gt;
&lt;p&gt;With that question in mind, let‚Äôs load all pertinent libraries. The data can be downloaded directly from the üéã &lt;a href=&#34;https://github.com/thecodeforest&#34;&gt;the codeforest data repo&lt;/a&gt; üéÑ. Note the original data comes from Kaggle and can found &lt;a href=&#34;https://www.kaggle.com/kendallgillies/nflstatistics/version/1?select=Career_Stats_Field_Goal_Kickers.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Modeling 
library(gamlss)

# Core packages
library(tidyverse)
library(janitor)

# Visualization 
library(ggplot2)
library(scales)
library(viridis)
library(ggridges)

# Tables
library(gt)

# Global plot theme
theme_set(theme_minimal())

# Code Forest repo
data_url &amp;lt;- &amp;quot;https://raw.githubusercontent.com/thecodeforest/codeforest_datasets/main/fantasy_football_kickers_data/Career_Stats_Field_Goal_Kickers.csv&amp;quot;

# Helper function for visualization
my_plot_theme = function(){
  font_family = &amp;quot;Helvetica&amp;quot;
  font_face = &amp;quot;bold&amp;quot;
  return(theme(
    axis.text.x = element_text(size = 16, face = font_face, family = font_family),
    axis.text.y = element_text(size = 16, face = font_face, family = font_family),
    axis.title.x = element_text(size = 16, face = font_face, family = font_family),
    axis.title.y = element_text(size = 16, face = font_face, family = font_family),
    strip.text.y = element_text(size = 22, face = font_face, family = font_family),
    plot.title = element_text(size = 22, face = font_face, family = font_family),
    
    legend.position = &amp;quot;top&amp;quot;,
    legend.title = element_text(size = 16,
                                face = font_face,
                                family = font_family),
    legend.text = element_text(size = 16,
                               face = font_face,
                               family = font_family),
    legend.key = element_rect(size = 5),
    legend.key.size = unit(1.5, &amp;#39;lines&amp;#39;)
  ))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are several columns we won‚Äôt be using so we‚Äôll select only the relevant ones.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats_raw &amp;lt;- read_csv(data_url) %&amp;gt;% 
  clean_names() %&amp;gt;% 
  select(player_id, 
         name, 
         year, 
         games_played, 
         contains(&amp;#39;made&amp;#39;), 
         contains(&amp;#39;attempted&amp;#39;),
         contains(&amp;#39;percentage&amp;#39;),
         -contains(&amp;#39;extra&amp;#39;), 
         -longest_fg_made
         )

glimpse(stats_raw)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1,994
## Columns: 19
## $ player_id                  &amp;lt;chr&amp;gt; &amp;quot;jeffhall/2500970&amp;quot;, &amp;quot;benagajanian/2508255&amp;quot;‚Ä¶
## $ name                       &amp;lt;chr&amp;gt; &amp;quot;Hall, Jeff&amp;quot;, &amp;quot;Agajanian, Ben&amp;quot;, &amp;quot;Agajanian‚Ä¶
## $ year                       &amp;lt;dbl&amp;gt; 2000, 1964, 1962, 1961, 1961, 1960, 1957, ‚Ä¶
## $ games_played               &amp;lt;dbl&amp;gt; 3, 3, 6, 3, 3, 14, 12, 10, 12, 12, 10, 12,‚Ä¶
## $ f_gs_made                  &amp;lt;chr&amp;gt; &amp;quot;4&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;13&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;5&amp;quot;, ‚Ä¶
## $ f_gs_made_20_29_yards      &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;‚Ä¶
## $ f_gs_made_30_39_yards      &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;‚Ä¶
## $ f_gs_made_40_49_yards      &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;‚Ä¶
## $ f_gs_made_50_yards         &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;‚Ä¶
## $ f_gs_attempted             &amp;lt;chr&amp;gt; &amp;quot;5&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;14&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;24&amp;quot;, &amp;quot;18&amp;quot;, &amp;quot;13&amp;quot;‚Ä¶
## $ f_gs_attempted_20_29_yards &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;‚Ä¶
## $ f_gs_attempted_30_39_yards &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;‚Ä¶
## $ f_gs_attempted_40_49_yards &amp;lt;chr&amp;gt; &amp;quot;2&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;‚Ä¶
## $ f_gs_attempted_50_yards    &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;‚Ä¶
## $ fg_percentage              &amp;lt;chr&amp;gt; &amp;quot;80.0&amp;quot;, &amp;quot;50.0&amp;quot;, &amp;quot;35.7&amp;quot;, &amp;quot;50.0&amp;quot;, &amp;quot;33.3&amp;quot;, &amp;quot;5‚Ä¶
## $ fg_percentage_20_29_yards  &amp;lt;chr&amp;gt; &amp;quot;100.0&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--‚Ä¶
## $ fg_percentage_30_39_yards  &amp;lt;chr&amp;gt; &amp;quot;100.0&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--‚Ä¶
## $ fg_percentage_40_49_yards  &amp;lt;chr&amp;gt; &amp;quot;50.0&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;‚Ä¶
## $ fg_percentage_50_yards     &amp;lt;chr&amp;gt; &amp;quot;100.0&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--&amp;quot;, &amp;quot;--‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like most real-world datasets, this one is a bit messy (e.g., non-values are coded as ‚Äú‚Äì‚Äù). I find it helps at the outset of data cleaning to envision what a perfect, pristine dataset should look like once data munging steps are complete. Below is an example of a basic starting point.
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#zpvivrqdou .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#zpvivrqdou .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#zpvivrqdou .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#zpvivrqdou .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#zpvivrqdou .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#zpvivrqdou .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#zpvivrqdou .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#zpvivrqdou .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#zpvivrqdou .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#zpvivrqdou .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#zpvivrqdou .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#zpvivrqdou .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#zpvivrqdou .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#zpvivrqdou .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#zpvivrqdou .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#zpvivrqdou .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#zpvivrqdou .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#zpvivrqdou .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#zpvivrqdou .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#zpvivrqdou .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#zpvivrqdou .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#zpvivrqdou .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#zpvivrqdou .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#zpvivrqdou .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#zpvivrqdou .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#zpvivrqdou .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#zpvivrqdou .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#zpvivrqdou .gt_left {
  text-align: left;
}

#zpvivrqdou .gt_center {
  text-align: center;
}

#zpvivrqdou .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#zpvivrqdou .gt_font_normal {
  font-weight: normal;
}

#zpvivrqdou .gt_font_bold {
  font-weight: bold;
}

#zpvivrqdou .gt_font_italic {
  font-style: italic;
}

#zpvivrqdou .gt_super {
  font-size: 65%;
}

#zpvivrqdou .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;zpvivrqdou&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34; style=&#34;table-layout: fixed;; width: 0px&#34;&gt;
  &lt;colgroup&gt;
    &lt;col style=&#34;width:155px;&#34;/&gt;
    &lt;col style=&#34;width:155px;&#34;/&gt;
    &lt;col style=&#34;width:155px;&#34;/&gt;
  &lt;/colgroup&gt;
  &lt;thead class=&#34;gt_header&#34;&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_title gt_font_normal&#34; style&gt;&lt;strong&gt;Desired Data Format&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_subtitle gt_font_normal gt_bottom_border&#34; style&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;id&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;n_success&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;n_trials&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;1&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;5&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;2&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;3&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;1&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;4&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;10&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;5&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;20&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;6&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;30&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;60&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;200&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;8&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;2&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;9&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;11&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;10&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;24&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;61&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;I used generic column names if you‚Äôre interested in adopting the techniques described herein to solve a separate problem. At a basic level, each row represents an individual observation, a count of the number of successes (i.e., count how many field goals are made), and finally the number of trials (i.e., count how many field goals are attempted). If you have this setup, the building blocks are in place to get started.&lt;/p&gt;
&lt;p&gt;However, before going any further, we need to ensure the relationships in the data align with our understanding of the world. One approach is to generate some simple hypotheses that you know to be true. For example, water is wet, the sky is blue, and, in our case, the field goal percentage should decrease as the distance to the goal increases. That is, field goals taken from 50+ yards should be made at a lower rate those taken from 30-35 yards. Let‚Äôs verify our hypothesis below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_by_dist &amp;lt;-
  stats_raw %&amp;gt;%
  select(starts_with(&amp;quot;fg_percentage_&amp;quot;)) %&amp;gt;%
  mutate_all(as.numeric) %&amp;gt;%
  gather(key = &amp;quot;dist&amp;quot;, value = &amp;quot;fg_pct&amp;quot;) %&amp;gt;%
  mutate(
    dist = str_extract(dist,
                       pattern = &amp;quot;\\d{2}&amp;quot;
                       ),
    dist = if_else(dist == &amp;quot;50&amp;quot;,
                   paste0(dist, &amp;quot;+&amp;quot;),
                   paste0(dist,&amp;quot;-&amp;quot;,as.numeric(dist) + 9)
                   ),
    fg_pct = fg_pct / 100
  ) %&amp;gt;%
  na.omit()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_by_dist %&amp;gt;% 
  ggplot(aes(fg_pct, dist, fill = dist)) + 
  geom_density_ridges(
    aes(point_color = dist, 
        point_fill = dist, 
        point_shape = dist),
        alpha = .2, 
        point_alpha = 1, 
        jittered_points = TRUE
    ) + 
  scale_point_color_hue(l = 40) +
  scale_discrete_manual(aesthetics = &amp;quot;point_shape&amp;quot;, 
                        values = c(21, 22, 23, 24)) + 
  scale_x_continuous(labels = scales::percent,
                     breaks = c(0,0.2, 0.4, 0.6, 0.8, 1)
                     ) + 
  scale_fill_viridis_d() + 
  my_plot_theme() + 
  labs(x = &amp;#39;Field Goal Percentage&amp;#39;,
       y = &amp;#39;Distance (Yards)&amp;#39;
       ) + 
    theme(legend.position = &amp;#39;none&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks good! Each point represents the field goal percentage for a player-season-distance combination. As the distance increases, the make rate gradually shifts to left, which is exactly what we‚Äôd expect. We‚Äôll do a bit more cleaning below before proceeding.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats_processed &amp;lt;- 
  stats_raw %&amp;gt;%
  mutate(
    name = str_remove(name, &amp;quot;,&amp;quot;),
    first_name = map(name, function(x) str_split(x, &amp;quot; &amp;quot;)[[1]][2]),
    last_name = map(name, function(x) str_split(x, &amp;quot; &amp;quot;)[[1]][1]),
    player_id = str_extract(player_id, &amp;quot;\\d+&amp;quot;)
  ) %&amp;gt;%
  unite(&amp;quot;name&amp;quot;, c(&amp;quot;first_name&amp;quot;, &amp;quot;last_name&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;%
  mutate_at(vars(matches(&amp;quot;attempted|made&amp;quot;)), as.numeric) %&amp;gt;% 
  replace(., is.na(.), 0) %&amp;gt;% 
  select(player_id, name, year, games_played, contains(&amp;quot;made&amp;quot;), contains(&amp;quot;attempted&amp;quot;)) %&amp;gt;%
  rename(
    fg_made = f_gs_made,
    fg_attempted = f_gs_attempted
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let‚Äôs view the resulting data for one of the best kickers in modern NFL to familiarize ourselves with the format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats_processed %&amp;gt;%
  filter(name == &amp;quot;Justin Tucker&amp;quot;) %&amp;gt;%
  mutate(fg_pct = fg_made / fg_attempted) %&amp;gt;% 
  select(name, year, fg_made, fg_attempted)&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#pkjidbziro .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#pkjidbziro .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#pkjidbziro .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#pkjidbziro .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#pkjidbziro .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#pkjidbziro .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#pkjidbziro .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#pkjidbziro .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#pkjidbziro .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#pkjidbziro .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#pkjidbziro .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#pkjidbziro .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#pkjidbziro .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#pkjidbziro .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#pkjidbziro .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#pkjidbziro .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#pkjidbziro .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#pkjidbziro .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#pkjidbziro .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#pkjidbziro .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#pkjidbziro .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#pkjidbziro .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#pkjidbziro .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#pkjidbziro .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#pkjidbziro .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#pkjidbziro .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#pkjidbziro .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#pkjidbziro .gt_left {
  text-align: left;
}

#pkjidbziro .gt_center {
  text-align: center;
}

#pkjidbziro .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#pkjidbziro .gt_font_normal {
  font-weight: normal;
}

#pkjidbziro .gt_font_bold {
  font-weight: bold;
}

#pkjidbziro .gt_font_italic {
  font-style: italic;
}

#pkjidbziro .gt_super {
  font-size: 65%;
}

#pkjidbziro .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;pkjidbziro&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34; style=&#34;table-layout: fixed;; width: 0px&#34;&gt;
  &lt;colgroup&gt;
    &lt;col style=&#34;width:155px;&#34;/&gt;
    &lt;col style=&#34;width:155px;&#34;/&gt;
    &lt;col style=&#34;width:155px;&#34;/&gt;
    &lt;col style=&#34;width:155px;&#34;/&gt;
  &lt;/colgroup&gt;
  &lt;thead class=&#34;gt_header&#34;&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;4&#34; class=&#34;gt_heading gt_title gt_font_normal&#34; style&gt;&lt;strong&gt;Justin Tucker Stats&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;4&#34; class=&#34;gt_heading gt_subtitle gt_font_normal gt_bottom_border&#34; style&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;name&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;year&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;fg_made&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;fg_attempted&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Justin Tucker&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;2016&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;38&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Justin Tucker&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;2015&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;33&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Justin Tucker&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;2014&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;29&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;34&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Justin Tucker&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;2013&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;38&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;41&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Justin Tucker&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;2012&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;30&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;33&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;Just like what we had above! Next, we‚Äôll add a few filters to reduce some of the noise in our data. Any player who has less than 30 field goal attempts and/or has kicked field goals in only one season across their career will be excluded from the analysis. Additionally, we‚Äôll ignore any players with a rookie year before the 1970s. The rationale here is that the NFL made several changes to the location and positioning of the goal during the early 70s, so we want to keep the dynamics of the kicking environment consistent for all players.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min_attempts &amp;lt;- 30
min_seasons &amp;lt;- 2
min_decade &amp;lt;- 1970

filter_df &amp;lt;- 
  stats_processed %&amp;gt;% 
  group_by(player_id) %&amp;gt;% 
  summarise(n_seasons = n(),
            n_attempts = sum(fg_attempted),
            rookie_decade = min(year) %/% 10 * 10
            ) %&amp;gt;% 
  filter(n_seasons &amp;gt;= min_seasons,
         n_attempts &amp;gt;= min_attempts,
         rookie_decade &amp;gt;= min_decade
         ) %&amp;gt;% 
  select(player_id)

stats_processed &amp;lt;- inner_join(stats_processed, filter_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we‚Äôll add a few features before aggregating the kicking data from a season level to a career level for each player. I‚Äôll cover the rationale of the features shortly in the estimation and inference section below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_time_stats &amp;lt;- 
  stats_processed %&amp;gt;% 
  group_by(player_id) %&amp;gt;% 
  summarise(
    rookie_decade  = min(year) %/% 10 * 10,
    last_yr_active = max(year)
    ) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(rookie_decade &amp;gt;= min_decade) %&amp;gt;% 
  mutate(status = ifelse(last_yr_active == 2016, &amp;#39;active&amp;#39;, &amp;#39;inactive&amp;#39;))

stats_processed &amp;lt;- inner_join(stats_processed,df_time_stats)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have our time-based features and the last step is to calculate our three key metrics ‚Äì successes, attempts, and our rate metric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats_agg &amp;lt;- 
  stats_processed %&amp;gt;% 
  group_by(player_id, name, rookie_decade, status) %&amp;gt;% 
  summarise(fg_made = sum(fg_made),
            fg_attempted = sum(fg_attempted),
            fg_pct = fg_made / fg_attempted
            ) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time to move on to the key focus of this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-and-inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimation and Inference&lt;/h3&gt;
&lt;p&gt;Let‚Äôs now discuss the logic underlying our estimation method as well as the role of the additional features (Note that some of the code below was inspired by the excellent book &lt;a href=&#34;http://varianceexplained.org/r/empirical-bayes-book/&#34;&gt;Introduction to Empirical Bayes: Examples from Baseball Statistics&lt;/a&gt; by David Robinson). To recap, we are estimating a proportion that captures the relationship between successes and attempts. We can model this outcome with the &lt;code&gt;beta distribution&lt;/code&gt;, which is simply a distribution of probabilities ranging from 0 - 1. In our case, it represents the likelihood of a particular field goal percentage for each player, which will fall somewhere between 0.5 and 0.9 depending on the decade(s) the player was active (more on that in second).&lt;/p&gt;
&lt;p&gt;Below we‚Äôll fit an &lt;em&gt;null model&lt;/em&gt; with no additional parameters when estimating each player‚Äôs &lt;code&gt;beta&lt;/code&gt; value. The absence of any inputs means that all players have the same &lt;code&gt;prior&lt;/code&gt;, independent of what decade they played in, whether they‚Äôre still active, or how many chances they‚Äôve had to kick a field goal. We‚Äôll then take our prior and update it based on how much information we have about each player, namely the number of field goals they‚Äôve taken and how often they‚Äôve succeeded.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_null &amp;lt;- gamlss(cbind(fg_made, fg_attempted - fg_made) ~ 1,
  family = BB(mu.link = &amp;quot;identity&amp;quot;),
  data = stats_agg
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## GAMLSS-RS iteration 1: Global Deviance = 936.6917 
## GAMLSS-RS iteration 2: Global Deviance = 836.9846 
## GAMLSS-RS iteration 3: Global Deviance = 828.0258 
## GAMLSS-RS iteration 4: Global Deviance = 827.9528 
## GAMLSS-RS iteration 5: Global Deviance = 827.9526&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats_agg_est &amp;lt;- 
  stats_agg %&amp;gt;% 
   mutate(
    mu = fitted(fit_null, &amp;quot;mu&amp;quot;), 
    sigma = fitted(fit_null, &amp;quot;sigma&amp;quot;), 
    alpha0 = mu / sigma, 
    beta0 = (1 - mu) / sigma,
    alpha1 = alpha0 + fg_made,
    beta1 = beta0 + fg_attempted - fg_made,
    estimate = alpha1 / (alpha1 + beta1),
    raw = fg_made / fg_attempted,
    low = qbeta(.025, alpha1, beta1),
    high = qbeta(.975, alpha1, beta1)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let‚Äôs plot out the estimate for all active players.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats_agg_est %&amp;gt;%
  mutate(name = paste0(name, &amp;quot;: &amp;quot;, fg_made, &amp;quot;|&amp;quot;, fg_attempted),
         name = fct_reorder(name, estimate)
         ) %&amp;gt;%
  filter(status == &amp;quot;active&amp;quot;) %&amp;gt;%
  ggplot(aes(name, estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = low, ymax = high)) +
  coord_flip() +
  geom_point(aes(name, raw), color = &amp;quot;red&amp;quot;, size = 3, alpha = 0.6) +
  scale_y_continuous(labels = scales::percent_format()) + 
  my_plot_theme() + 
  labs(x = NULL,
       y = &amp;#39;Field Goal Percentage&amp;#39;,
       title = &amp;#39;Estimated field goal percentage amongst active NFL kickers&amp;#39;,
       subtitle = &amp;#39;Black dot represents estimate while red dot is actual. Note the bias in our estimates.&amp;#39;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let‚Äôs talk through this figure by comparing the field goal percentage estimates for Adam Vinatieri, who has made 530 of 629 fields goals throughout his career, to Chris Boswell, who has made 50 of 57 field goals. While Vinatieri has a lower actual make rate than Boswell (84.2% vs.¬†87.7%), we consider him to be a better field goal kicker. The seemingly incongruent finding is based on the fact that we have more evidence for Vinatieri (629 FG attempts vs.¬†57 FG attempts) than Boswell. It‚Äôs like saying, ‚ÄúChris Boswell is good kicker, maybe better than Vinatieri, but we don‚Äôt have enough evidence (yet) to believe he is that much better than an average kicker, a number represented by our prior‚Äù. Indeed, if we also consider the width of the credible intervals surrounding these two players, Adam Vinatieri‚Äôs interval is considerably smaller than Chris Boswell‚Äôs interval.&lt;/p&gt;
&lt;p&gt;While this is a good way to gain an intuition for what‚Äôs happening under the hood, we see an immediate problem ‚Äì all of our estimates are biased! The actual field goal percentage is above every single estimate. Luckily, there is a solution: we can create conditional estimates of our prior. One way to do this is to create features that explain variability between our players. For example, &lt;a href=&#34;https://fivethirtyeight.com/features/kickers-are-forever/&#34;&gt;field goal percentages have improved dramatically over the past 50 years&lt;/a&gt;. Let‚Äôs consider our own data and map out this pattern from the 1970s to the 2010s.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats_agg %&amp;gt;%
  mutate(rookie_decade = as.factor(rookie_decade)) %&amp;gt;%
  ggplot(aes(rookie_decade, fg_pct, color = rookie_decade)) +
  geom_boxplot() +
  geom_jitter() +
  scale_y_continuous(labels = scales::percent_format()) +
  my_plot_theme() +
  scale_color_viridis_d() + 
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(
    x = &amp;quot;Decade&amp;quot;,
    y = &amp;quot;Field Goal Percentage&amp;quot;,
    title = &amp;#39;Kicker performance has improved over time&amp;#39;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1152&#34; /&gt;
The best kicker in 1970s has a lower field goal percentage than the worst kicker in the 2010s. Including the decade of a kicker‚Äôs rookie season allows us to create a more informed prior. Thus, if we use the median field goal percentage of all kickers who debuted as rookies in 2010+, our best guess would be about 84%, whereas a kicker who debuted in the 1970s would be somewhere around 64%. This explains why the estimates from our null model were biased.&lt;/p&gt;
&lt;p&gt;The second factor to consider is the number of field goal attempts per player, because better players have more opportunities to kick field goals. This makes intuitive sense and is captured in the following plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats_agg %&amp;gt;%
  ggplot(aes(log2(fg_attempted), fg_pct)) +
  geom_point(size = 3) +
  geom_smooth(span = 1) +
  scale_y_continuous(labels = scales::percent_format()) +
  my_plot_theme() +
  labs(
    x = &amp;quot;Log2(Total Attempts)&amp;quot;,
    y = &amp;quot;Field Goal percentage&amp;quot;,
    title = &amp;quot;Better kickers have more opportunities&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Below we‚Äôll use the same model except this time we‚Äôll account for the number of field goal attempts and a player‚Äôs rookie decade.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_complete &amp;lt;- 
  gamlss(cbind(fg_made, fg_attempted - fg_made) ~ log2(fg_attempted) + rookie_decade,
  family = BB(mu.link = &amp;quot;identity&amp;quot;),
  data = stats_agg
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## GAMLSS-RS iteration 1: Global Deviance = 918.3071 
## GAMLSS-RS iteration 2: Global Deviance = 714.8471 
## GAMLSS-RS iteration 3: Global Deviance = 668.3708 
## GAMLSS-RS iteration 4: Global Deviance = 668.1839 
## GAMLSS-RS iteration 5: Global Deviance = 668.1838&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1152&#34; /&gt;
Much better! Our estimates do not exhibit the same degree of bias as before. Moreover, the width of our credible intervals shrank across all players. This makes sense, given that we can now condition our prior estimates on inputs that explain variability in the field goal percentage. While there are other factors that might improve our model (e.g., did a player‚Äôs team have their home games in a dome?), this is a good starting point for answering our original question.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-parameters-to-points&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;From Parameters to Points&lt;/h3&gt;
&lt;p&gt;We have a model that does a reasonable job of estimating a kicker‚Äôs field goal percentage. Now we need to translate that into an estimate of fantasy points. This will take a few steps, but I‚Äôll outline each in turn. First, we need to estimate the average worth (in fantasy points) of each successful field goal. Typically, field goals less-than 40 yards are worth 3 points, 40 - 49 yards are worth 4 points, and 50 or more yards are worth 5 points. We‚Äôll use the 2016 season to come up with a global average. While we could technically account for distances of each player (e.g., some kickers are excellent at a longer distances, others not so much), this approach will give us a ‚Äúgood-enough‚Äù answer. Second, we‚Äôll estimate the average number of field goal attempts per season. This can vary widely from one season to the next for a given kicker, as it is contingent upon the offense getting within kicking range. Again, we‚Äôll keep it simple and just average the number of attempts across all players from the 2016 season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Average points per FG
pts_per_fg &amp;lt;- 
  stats_processed %&amp;gt;% 
  filter(year == 2016) %&amp;gt;% 
  mutate(pt_3_fgs = (f_gs_made_20_29_yards + f_gs_made_30_39_yards) * 3,
         pt_4_fgs = f_gs_made_40_49_yards * 4,
         pt_5_fgs = f_gs_made_50_yards * 5,
         tot_pts = pt_3_fgs + pt_4_fgs + pt_5_fgs
  )

pts_per_fg &amp;lt;- round(sum(pts_per_fg$tot_pts) / sum(pts_per_fg$fg_made), 1)

# Average number of attempts
attempts_per_season &amp;lt;- 
  stats_processed %&amp;gt;% 
  filter(year == 2016) %&amp;gt;% 
  pull(fg_attempted) %&amp;gt;% 
  mean() %&amp;gt;% 
  round()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here comes the fun part. Below we‚Äôll simulate 1000 seasons for each player by randomly generating 1000 values of &lt;code&gt;beta&lt;/code&gt;. This value is based on the posterior estimates, &lt;code&gt;alpha1&lt;/code&gt; and &lt;code&gt;beta1&lt;/code&gt;, produced by our model. The estimates will vary from one simulation to next, though most values will fall somewhere between 0.75 and 0.9. Better players like Justin Tucker will be near the high end of that range while player like Graham Gano will be near the lower end. We‚Äôll then take each estimate and plug it into the &lt;code&gt;binomial distribution&lt;/code&gt; below. Recall that the &lt;code&gt;binomial distribution&lt;/code&gt; is defined by a single parameter, which represents the probability of success. This is exactly what our estimate of &lt;code&gt;beta&lt;/code&gt; represents! Given that all active players had an average of 27 FG attempts in 2016, each of the 1000 simulations will consist of 27 trials (or &lt;code&gt;attempts_per_season&lt;/code&gt;) each with a slightly different probability of success (how likely they are to make a field goal on a given attempt). We‚Äôll lean on the &lt;code&gt;purrr&lt;/code&gt; package to vectorize these operations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2018)
n_seasons &amp;lt;- 1000

est_active &amp;lt;- 
  stats_agg_est %&amp;gt;% 
  filter(status == &amp;#39;active&amp;#39;)

est_make_pct &amp;lt;- map2(est_active %&amp;gt;% pull(alpha1),
                     est_active %&amp;gt;% pull(beta1), 
                     function(x, y) rbeta(n_seasons, x, y)
                     )
est_outcomes &amp;lt;- map(est_make_pct, 
                    function(x) rbinom(n = n_seasons, 
                                       size = attempts_per_season,
                                       prob = x
                                       )
                    )
names(est_outcomes) &amp;lt;- est_active$name&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So much data! Below we‚Äôll plot the distribution of total points accumulated for each player across the 1000 simulated seasons. We‚Äôll create quantiles as a way to see how much overlap there is between players.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pt_simulation &amp;lt;- 
  est_outcomes %&amp;gt;% 
  tbl_df() %&amp;gt;% 
  gather() %&amp;gt;% 
  transmute(name = key,
            season_pts = value * pts_per_fg
            ) %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  mutate(avg_pts = mean(season_pts)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(name = fct_reorder(name, avg_pts))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `tbl_df()` is deprecated as of dplyr 1.0.0.
## Please use `tibble::as_tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pt_simulation %&amp;gt;% 
  ggplot(aes(season_pts, y = name, fill = factor(..quantile..))) + 
  stat_density_ridges(
    geom = &amp;quot;density_ridges_gradient&amp;quot;,
    calc_ecdf = TRUE,
    quantiles = 4,
    quantile_lines = TRUE,
    bandwidth = 2
  ) +
  scale_fill_viridis(discrete = TRUE, name = &amp;quot;Point Quartile&amp;quot;, alpha = 0.5) +
  my_plot_theme() + 
  scale_x_continuous(breaks = pretty_breaks(n = 7)) + 
  labs(x = &amp;#39;Total Points Per Simulated Season&amp;#39;,
       y = NULL,
       title = &amp;quot;The best kicker is not much better than the worst kicker&amp;quot;,
       subtitle = &amp;#39;Drafing any kicker is fine&amp;#39;
       ) + 
  theme(legend.position = &amp;#39;none&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wait! We went all this way for you to tell me that the status quo is probably right? Yes, I did. But we still haven‚Äôt quantified how much better or worse drafting the best or worst kicker is in terms of fantasy points. A simple way is to count the number of seasons where Justin Tucker (the best kicker) scored more points than Andrew Franks (the worst kicker).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;jt_pts &amp;lt;- 
  pt_simulation %&amp;gt;% 
  filter(name == &amp;#39;Justin Tucker&amp;#39;) %&amp;gt;% 
  pull(season_pts) 

af_pts &amp;lt;- 
pt_simulation %&amp;gt;% 
  filter(name == &amp;#39;Andrew Franks&amp;#39;) %&amp;gt;% 
  pull(season_pts)

pct_greater &amp;lt;- sum(jt_pts &amp;gt; af_pts) / n_seasons
print(str_glue(&amp;#39;PCT greater: {pct_greater * 100}%&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PCT greater: 77.5%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turns out that approximately 77 of every 100 seasons Justin Tucker outscores Andrew Franks. Let‚Äôs go one step further and quantify the actual difference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data.frame(pt_diff = jt_pts - af_pts), aes(pt_diff)) + 
  geom_histogram(fill = &amp;#39;gray&amp;#39;, color = &amp;#39;black&amp;#39;, bins = 10) + 
  scale_x_continuous(breaks = pretty_breaks(n = 15)) + 
  labs(x = &amp;#39;Point Difference over Entire Season&amp;#39;) + 
  theme_minimal() + 
  geom_vline(xintercept = quantile(jt_pts - af_pts, .05), lty = 2) + 
  geom_vline(xintercept = quantile(jt_pts - af_pts, .5), lty = 2, color = &amp;#39;red&amp;#39;, size = 2) + 
  geom_vline(xintercept = quantile(jt_pts - af_pts, .95), lty = 2) + 
  my_plot_theme() + 
  labs(x = &amp;#39;Point Difference&amp;#39;,
       y = &amp;#39;Count&amp;#39;,
       title = &amp;#39;The best kicker should score about 10 more points per season compared to the worst&amp;#39;,
       subtitle = &amp;#39;Estimate based on 27 FG attempts per season with each FG worth 3.5 points&amp;#39;
       )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1152&#34; /&gt;
If we spread this estimate out across 16 regular-season games, it comes out to less than a single point per game.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Needless to say, pick your kicker last in Fantasy Football! All kickers in modern-day NFL are really good, so save those late-round picks for positions other than a kicker. Cheers!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The State of Names in America</title>
      <link>http://example.org/post/2019-06-12-state-of-names/state_of_names/</link>
      <pubDate>Wed, 12 Jun 2019 21:13:14 -0500</pubDate>
      <guid>http://example.org/post/2019-06-12-state-of-names/state_of_names/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;http://example.org/post/2019-06-12-state-of-names/images/header_img.jpg&#34; width=&#34;700&#34; height=&#34;600&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Phil Karlton, a famous Netscape Developer (i.e., OG Google Chrome) once said, ‚ÄòThere are two hard things in computer science: cache invalidation and naming things‚Äô. I haven‚Äôt done much cache invalidation, but I have named a few things ‚Äì and naming a person is by far the hardest of them all! Indeed, having waited two days after my own son‚Äôs birth to finally settle on a name, I wondered to what extent other new parents encountered the same struggles. Are there shortcuts or heuristics that others use to simplify the decision-making process, specifically cues from their immediate surroundings to help guide their choices when choosing a baby name? This question motivated me to look into the nuances of naming conventions over the past century in America.&lt;/p&gt;
&lt;p&gt;Accordingly, in this post, we‚Äôll investigate the influence of one‚Äôs state of residence on the frequency with which certain names occur. We‚Äôll also explore possible reasons for why some states have more variety in their names than others. Finally, we‚Äôll finish up in my home state of Oregon to identify the trendiest names over the past 20 years and predict whether those names will remain trendy in the future. From a technical standpoint, we‚Äôll cover some central, bread-and-butter topics in data science, including trend detection, false discovery rates, web scraping, time-series forecasting, and geovisualization. Let‚Äôs get started!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;people-born-in-oregon-are-named-after-trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;People Born in Oregon are Named after Trees&lt;/h3&gt;
&lt;p&gt;We‚Äôll begin by downloading more than 110 years of US name data from üéÑ &lt;a href=&#34;https://github.com/thecodeforest&#34;&gt;the codeforest github repo&lt;/a&gt; üå¥. Our dataset is published yearly by the &lt;a href=&#34;https://www.ssa.gov/OACT/babynames/limits.html&#34;&gt;Social Security Administration&lt;/a&gt;, and it contains a count of all names that occur more than five times by year within each US state. Let‚Äôs get started by loading relevant libraries and pulling our data into R.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Choosing a Fantasy Football Quarterback</title>
      <link>http://example.org/post/2017-09-10-choosing-ff-qb/choosing_fantasy_qb/</link>
      <pubDate>Sun, 10 Sep 2017 21:13:14 -0500</pubDate>
      <guid>http://example.org/post/2017-09-10-choosing-ff-qb/choosing_fantasy_qb/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;http://example.org/post/2017-09-10-choosing-ff-qb/choosing_fantasy_qb_files/mariota.jpg&#34; width=&#34;700&#34; height=&#34;400&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Understanding a new concept is all about connecting it with something you already know. I don‚Äôt know much, but I do know Fantasy Football. Thus, when I come across new concepts, I often think to myself, ‚ÄúHow can I use this information to beat my friend Steve in Fantasy Football‚Äù? This very question was the impetus for putting these words and figures together in a post, which will introduce the idea of using the Beta Distribution to determine your weekly starter. I‚Äôll explain this approach in the context of my 2015 Fantasy Football season.&lt;/p&gt;
&lt;p&gt;At the outset of that season, I drafted two quarterbacks: Joe Flacco and Marcus Mariota (it was a rough draft). Flacco had been in the NFL for a few years, while Mariota was still a rookie yet to play a game. I was also considering a separate rookie, Jameis Winston, who was available to pick up anytime during the season off the waiver wire. Throughout the season, I was faced with the following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Who do I make the starting QB?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If one QB is performing poorly, when is the right time to make the switch (e.g., Flacco -&amp;gt; Mariota; Flacco -&amp;gt; Winston; Mariota -&amp;gt; Winston)?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This question is faced by NFL coaches and fantasy owners alike. If your QB has a few bad weeks, should you continue with them into the next week, replace them with the 2nd string QB, or sign a free agent to your team mid-season?&lt;/p&gt;
&lt;p&gt;Before getting into the technical details, let‚Äôs first define what ‚ÄúSuccess‚Äù looks like for a Fantasy Football QB. Success can be defined in one word: Consistency. A QB that throws three touchdowns (TDs) every game for the first six games of the season (18 total) is better than a QB who throws five TDs for the first three games and then one TD during the next three games, despite having thrown the same number of TDs. Simply put - you want consistent, reliable performance every week. It doesn‚Äôt matter if you win by one point or 50 points ‚Äì a win is a win. Thus, I evaluate my QB‚Äôs performance on the following criteria: A ‚ÄúSuccessful‚Äù performance is defined as &lt;strong&gt;3 or more touchdowns AND/OR 300 or more yards&lt;/strong&gt; for a given week. Touchdowns and passing yards are the two primary sources of QB fantasy points, and a +3TD|300yard weekly statline should cement a QB amongst that week‚Äôs top performers. Failing to meet either of these criteria was defined as an ‚ÄúUnsuccessful‚Äù performance. Note that this label could also factor in interceptions, pass completions, and fumble, but we‚Äôll keep it simple and just focus on passing yards and passing touchdowns.&lt;/p&gt;
&lt;p&gt;Having defined the evaluation criteria, the data generating process was modeled via the beta distribution. Recall that the beta distribution defines a distribution of probabilities, and we‚Äôre interested in the probability of our QB having a Successful week. There are several years of performance history on Joe Flacco, so we can provide a reasonably informed estimate of his weekly probabilty for achieving success (i.e., our prior). In contrast, there is no NFL game history on Mariota or Winston, so we‚Äôll assign each a uniform or uninformative prior. Our estimate of the Success parameter for Winston and Mariota will change rapidly as we acquire in-season data because our posterior is determined entirely from the data. We could create a more informed-‚Äìand stronger-‚Äìprior by assigning Mariota and Winston the historic first-year league average for all rookie QBs entering the NFL but we‚Äôll keep it simple. A uniform prior means that all probabilities from 0-1 are equally likely.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collecting-qb-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collecting QB Data&lt;/h3&gt;
&lt;p&gt;We‚Äôll use the &lt;code&gt;nflgame&lt;/code&gt; python package to gather QB data. We‚Äôll pull 2013-2014 weekly performance data for Joe Flacco to calculate our prior, as well as the 2015 data for all three players. During the season we‚Äôll update our priors to determine which QB we should play for a given week. That is, as we acquire results over the season, updates will be made to obtain a better, more reliable estimate of the ‚Äúsuccess‚Äù parameter for each QB.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import nflgame
import pandas as pd

game_years = range(2013, 2016)
game_weeks = range(1, 17)
qbs = (&amp;quot;Joe Flacco&amp;quot;, 
       &amp;quot;Marcus Mariota&amp;quot;,
       &amp;quot;Jameis Winston&amp;quot;)
       
def get_passing_data(year, week, players, qbs):
    qb_list = list()
    for p in players.passing():
        player = &amp;quot; &amp;quot;.join(str(p.player).split(&amp;quot; &amp;quot;)[:2]) 
        if player in qbs:
            qb_list.append([year, week, player, p.passing_tds, p.passing_yds])
    return qb_list
    
quarterback_data = pd.DataFrame()
for year in game_years:
    print &amp;quot;Retrieving Player Data for {year}&amp;quot;.format(year = year)
    for week in game_weeks:
        games = nflgame.games(year, week)
        players = nflgame.combine_game_stats(games)
        temp_qb_stats = get_passing_data(year, week, players, qbs)
        quarterback_data = quarterback_data.append(pd.DataFrame(temp_qb_stats))
        
quarterback_data.columns = [&amp;quot;year&amp;quot;, &amp;quot;week&amp;quot;, &amp;quot;player&amp;quot;, &amp;quot;touchdowns&amp;quot;, &amp;quot;passing_yds&amp;quot;]
quarterback_data.to_csv(&amp;quot;quarterback_data.csv&amp;quot;, index = False)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
