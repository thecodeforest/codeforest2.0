[{"authors":null,"categories":null,"content":"I love asking big questions and answering them with data. My background is in Psychology and Statistics, and it‚Äôs been a blast seeing how these fields have co-evolved over the years. I currently work as a data scientist, and I started this blog as a way to document a few of the more interesting topics I‚Äôve encountered throughout my journeys. The goal is to abstract away some of the technical bits in an attempt to give you (the reader) the ability to implement while learning. I‚Äôm always interested in connecting with fellow data-scientists/whisperers/magicians/analysts, so feel free to PM me on linkedin!\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://example.org/author/mark-leboeuf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mark-leboeuf/","section":"authors","summary":"I love asking big questions and answering them with data. My background is in Psychology and Statistics, and it‚Äôs been a blast seeing how these fields have co-evolved over the years.","tags":null,"title":"Mark LeBoeuf","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Mark LeBoeuf FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"http://example.org/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"http://example.org/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"http://example.org/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"http://example.org/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"http://example.org/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":["AWS","Lambda","Python","Running"],"content":" Overview As a runner, I‚Äôve always enjoyed the pursuit of pacing faster today relative to yesterday. With the advent of apps like Strava that track your performance (among many other metrics), it‚Äôs easy to measure if you are running faster over time and, perhaps more importantly, which factors affect your run pace. Indeed, based on my historical running data, I‚Äôve noticed two factors that moderate my run times: time of day and weather. My fastest runs usually happen between 12 PM - 7 PM, and slower runs occurred with high winds, cold weather (less than 30¬∞F ), hot weather (greater than 90¬∞F), or rain (being wet makes me miserable). On these ‚Äúbad weather‚Äù days, I‚Äôd prefer to run inside on the treadmill and wait until more optimal running conditions.\nWith these criteria in mind, I would begin most mornings by deciding if it was an ‚Äúinside‚Äù or ‚Äúoutside‚Äù running day by executing the following mental steps:\n‚òÅÔ∏è Log on to weather.com at 7AM\n‚òÅÔ∏è Check the hourly forecast between 12PM and 5PM\n‚òÅÔ∏è Check the temperature, wind-speed, and chance of precipitation\n‚òÅÔ∏è Make a ‚ÄúYes‚Äù or ‚ÄúNo‚Äù decision to run outside based on the forecast\nWhile it isn‚Äôt a huge inconvenience to repeat these steps each day, it required a few minutes of the morning. Perhaps more importantly, though, it was yet another decision that needed attention. I make lots of decisions in a day, and each decision requires thought and energy. Thus, if I could automate one of those decisions by creating a ‚Äúrules engine,‚Äù it would save me the time and cognition required to plan my daily run.\nThe journey of automating this process is what inspired the following post, which will cover a few key concepts, including:\n Scheduling a workflow using AWS Event Bridge Building Lambda functions Sending emails via AWS SES  These concepts can be generalized to any reoccurring process. Perhaps it‚Äôs a daily forecast that planners use to manage a store‚Äôs inventory. Or maybe it‚Äôs a marketing email sent to new customers after making their first purchase. Extracting data from a database/API, applying some business logic, and then socializing the results through an Email is a standard analytics workflow. Read on to learn more about how I leveraged this approach to identify the best times to run each day, save myself a few minutes, and remove one more decision from my day.\n Be The Algorithm Before diving into the solution, I wanted to quickly discuss a technique I‚Äôve found helpful when automating a decision-making process. Before writing code or building queries, it‚Äôs good to step through the process manually that you are trying to automate. That is, you want to assume the role of the computer or algorithm. Repeating the steps above each day made it clear how I would automate the decision-making process by identifying:\nThe information I needed to make a decision\n The timing and frequency of the decision The values of the criteria that would lead to an ‚Äúinside‚Äù or ‚Äúoutside‚Äù run decision  You could easily sub in a machine learning model to discover the rules, but the overall process flow will be essentially unchanged. Keep this in mind next time you go to create a process that automates a decision.\nIn the next section, we‚Äôll cover the technical specifics.\n Architecture Overview No post on AWS would be complete without a diagram outlining how data flows through our system. Accordingly, the figure above depicts the order of operations and each service‚Äôs role in our decision workflow. Each service is described in greater detail below.\n Event Bridge - this is our scheduler. Each day at Noon PST, Amazon Event Bridge initiates the first Lambda function (TimeToRun).\n Lambda (TimeToRun) - TimeToRun connects to the OpenWeather API, extracts weather forecasts for my latitude and longitude, and formats the resulting data. The forecasts are then saved to an S3 bucket.\n Lambda (SendRunningEmail) - SendRunningEmail is triggered by any action in the S3 bucket containing the hourly weather forecasts. In this case, when a new object lands in the bucket, the Lambda function automatically starts and retrieves the data from the S3 bucket.\n Amazon SES - While this service is part of the SendRunningEmail Lambda, I separated it since it‚Äôs such a helpful service. Sending emails through Python can be tricky, and I‚Äôve found the approach using AWS SES to be much easier. You import the service, define the message contents, add a bit of HTML (to make it look pretty, of course), and send the message to a set of desired email addresses. It‚Äôs that simple.\n Personal Gmail - this is where the resulting message lands, alerting me if it is an ‚Äúinside‚Äù or ‚Äúoutside‚Äù running day.\n  In the following sections, we‚Äôll cover the two Lambda functions that comprise this workflow. We‚Äôll also cover a few ‚Äúgotchas‚Äù that come up frequently when working with Lambda for the first time.\n Part 1: TimeToRun The first part will cover the data collection process, which briefly entails:\n1. Scheduling\n2. Extracting hourly forecasts from OpenWeather API\n3. Saving the forecasts to an S3 bucket\nWe‚Äôll use EventBridge for scheduling, which you can see in the diagram on the left below. To connect Lambda with EventBridge, you add a trigger and then indicate how frequently you want it to execute. The desired cadence for the hourly weather forecasts was every weekday at 7 PM GMT (or noon PST), expressed via Cron below. Now that we‚Äôve scheduled our Lambda function, the next step is to add logic that collects the forecasts and saves them to S3.\nimport os import sys from typing import List import json from datetime import datetime import logging import pytz import requests import boto3 S3_BUCKET = \u0026quot;running-weather-data\u0026quot; logger = logging.getLogger() logger.setLevel(logging.INFO) def retrieve_weather_data(units_of_measure: str) -\u0026gt; dict: api_key = os.environ[\u0026quot;WEATHER_API_KEY\u0026quot;] lat = os.environ[\u0026quot;LOCATION_LATITUDE\u0026quot;] lon = os.environ[\u0026quot;LOCATION_LONGITUDE\u0026quot;] base_url = \u0026quot;https://api.openweathermap.org/data/2.5/onecall?\u0026quot; url = f\u0026quot;{base_url}lat={lat}\u0026amp;lon={lon}\u0026amp;appid={api_key}\u0026amp;units={units_of_measure}\u0026quot; response = requests.get(url) weather_data = json.loads(response.text) return weather_data def parse_weather_data(weather_hour: dict) -\u0026gt; dict: hour = datetime.fromtimestamp( weather_hour[\u0026quot;dt\u0026quot;], pytz.timezone(\u0026quot;America/Los_Angeles\u0026quot;) ).hour temp = weather_hour[\u0026quot;temp\u0026quot;] wind_speed = weather_hour[\u0026quot;wind_speed\u0026quot;] weather_status = weather_hour[\u0026quot;weather\u0026quot;][0] status = weather_status[\u0026quot;main\u0026quot;] return {\u0026quot;hour\u0026quot;: hour, \u0026quot;temp\u0026quot;: temp, \u0026quot;wind_speed\u0026quot;: wind_speed, \u0026quot;status\u0026quot;: status} def is_today_weather(weather_hour: dict, timezone: str = \u0026quot;America/Los_Angeles\u0026quot;) -\u0026gt; bool: weather_fmt = \u0026quot;%Y-%m-%d\u0026quot; today_dt = datetime.now().strftime(weather_fmt) weather_dt = datetime.fromtimestamp(weather_hour[\u0026quot;dt\u0026quot;], pytz.timezone(timezone)) if weather_dt.strftime(weather_fmt) == today_dt: return True else: return False def _generate_s3_path() -\u0026gt; str: year, month, day = datetime.now().strftime(\u0026quot;%Y-%m-%d\u0026quot;).split(\u0026quot;-\u0026quot;) s3_path = f\u0026quot;data/{year}-{month}-{day}-running-times.json\u0026quot; return s3_path def save_json_to_s3(json_data: dict, s3_bucket: str) -\u0026gt; None: s3 = boto3.resource(\u0026quot;s3\u0026quot;) response = s3.Object(s3_bucket, _generate_s3_path()).put( Body=(bytes(json.dumps(json_data).encode(\u0026quot;UTF-8\u0026quot;))) ) if response.get(\u0026quot;HTTPStatusCode\u0026quot;) == 200: print(f\u0026quot;Data successfully landed\u0026quot;) def lambda_handler(event, context): try: # retrieve weather forecast from OpenWeatherAPI weather_data = retrieve_weather_data(units_of_measure=\u0026quot;imperial\u0026quot;) # extract hourly forecast hourly_data = weather_data[\u0026quot;hourly\u0026quot;] # filter to only today\u0026#39;s forecast today_weather_bool = [is_today_weather(x) for x in hourly_data if x] # extract fields relevant to deciding if run outside hourly_data = [parse_weather_data(x) for x in hourly_data] # filter to today\u0026#39;s hourly data today_hourly_data = [ today_weather for (today_weather, is_today) in zip(hourly_data, today_weather_bool) if is_today ] # convert all data to dictionary hourly_data_dict = {\u0026quot;weather_data\u0026quot;: today_hourly_data} # save hourly weather data to S3 Bucket as .json save_json_to_s3(json_data=json.dumps(hourly_data_dict), s3_bucket=S3_BUCKET) return {\u0026quot;statusCode\u0026quot;: 200, \u0026quot;body\u0026quot;: json.dumps(hourly_data_dict)} except Exception as exp: exception_type, exception_value, exception_traceback = sys.exc_info() err_msg = json.dumps( {\u0026quot;errorType\u0026quot;: exception_type.__name__, \u0026quot;errorMessage\u0026quot;: str(exception_value)} ) logger.error(err_msg) This entire block of code is triggered daily, landing a single .json file in the desired S3 Bucket. While this looks straightforward, it‚Äôs not as simple as copy-pasting your code and hitting play. Like most things in the AWS ecosystem, getting everything to work takes a few tries. The subsections below highlight areas that are potential sources of confusion when starting with Lambda.\nConfiguring Environment Variables Environment variables store sensitive information, such as API keys, passwords, or other private values. In this case, I‚Äôve stored the OpenWeather API key and Latitude/Longitude of where I want the daily forecasts. The image below depicts how to add these variables via the console.\nAnd this is where these variables are accessed in the Lambda code.\ndef retrieve_weather_data(units_of_measure: str) -\u0026gt; dict: api_key = os.environ[\u0026quot;WEATHER_API_KEY\u0026quot;] lat = os.environ[\u0026quot;LOCATION_LATITUDE\u0026quot;] lon = os.environ[\u0026quot;LOCATION_LONGITUDE\u0026quot;] base_url = \u0026quot;https://api.openweathermap.org/data/2.5/onecall?\u0026quot; url = f\u0026quot;{base_url}lat={lat}\u0026amp;lon={lon}\u0026amp;appid={api_key}\u0026amp;units={units_of_measure}\u0026quot; response = requests.get(url) weather_data = json.loads(response.text) return weather_data Note that this approach to managing keys and constants is sufficient for smaller projects and prototypes. However, for larger projects where you are collaborating with other developers and stakeholders, configuration data will likely be stored in a way that allows for versioning and tracking.\n Adding Layers A layer is a .zip file that includes additional code or data. If you noticed in the retrieve_weather_data function, we use the requests package to access the OpenWeather API. Requests is not part of the Python Standard Library, so we must include it as part of a layer (there is no way to pip install requests or any other third-party libraries). While a full explanation of adding a layer is beyond the scope of this post, the following article nicely summarizes how to incorporate third-party libraries on Lambda.\n Adding Permissions Any time you set up a service through AWS, the default is to have minimal permissions in place. Among other activities, permissions allow your lambda function to interact with other AWS services. For example, the TimeToRun Lambda function writes the weather forecasts to an S3 bucket. The ability to interact with S3 is not setup by default, so you‚Äôll have to attach a policy. Below I‚Äôve enabled AmazonS3FullAccess, which allows access to S3. You‚Äôll need to do the same for the second Lambda function as well.\nIf you ever receive an error message like ‚Äú‚Ä¶is not authorized to perform‚Ä¶‚Äù, it usually can be solved by updating the permissions for a given service.\n Run Time Limit A second default setting that might not be immediately obvious is the standard run-time limit. This setting indicates how long AWS will let a Lambda run before terminating. The default is set to three seconds. Depending on the processing time, I‚Äôll usually increase the limit to 30 seconds and then gradually go down or up from there. The image below indicates where you can adjust the run-time or memory for more compute-heavy tasks.\nIf you‚Äôve successfully implemented all of the steps above, you should receive something that looks like this when testing the function:\nThis response indicates that everything ran smoothly and you are ready for the next section!\n  Part 2: SendRunningEmail Lambda The second part of this post covers the data formatting and transmission process in four steps:\nExtract data from S3\n Determine if ‚Äúinside‚Äù or ‚Äúoutside‚Äù running day\n Format the decision text (so it looks nice, of course)\n Send the decision to the desired email address(es)  import sys import boto3 import json import logging from datetime import datetime from typing import List S3_BUCKET = \u0026quot;\u0026lt;weather-data-bucket-name\u0026gt;\u0026quot; SENDER = \u0026quot;\u0026lt;sender-email-address\u0026gt;\u0026quot; RECIPIENT = \u0026quot;\u0026lt;recipient-email-addresses\u0026gt;\u0026quot; AWS_REGION = \u0026quot;us-west-2\u0026quot; SUBJECT = \u0026quot;Best Times to Run Today\u0026quot; CHARSET = \u0026quot;UTF-8\u0026quot; RUNNING_CONDS = { \u0026quot;hour\u0026quot;: {\u0026quot;min_hour\u0026quot;: 13, \u0026quot;max_hour\u0026quot;: 19}, \u0026quot;status\u0026quot;: [\u0026quot;Rain\u0026quot;, \u0026quot;Snow\u0026quot;, \u0026quot;Smoke\u0026quot;], \u0026quot;wind_speed\u0026quot;: {\u0026quot;min_speed\u0026quot;: 0, \u0026quot;max_speed\u0026quot;: 30}, \u0026quot;temp\u0026quot;: {\u0026quot;min_temp\u0026quot;: 30, \u0026quot;max_temp\u0026quot;: 90}, } logger = logging.getLogger() logger.setLevel(logging.INFO) def find_most_recent_data_path(s3_bucket: str) -\u0026gt; str: today_dt = datetime.now().strftime(\u0026quot;%Y-%m-%d\u0026quot;) s3 = boto3.resource(\u0026quot;s3\u0026quot;) bucket = s3.Bucket(s3_bucket) existing_data = [ x.key for x in bucket.objects.all() if str(x.key).startswith(\u0026quot;data\u0026quot;) and str(x.key).endswith(\u0026quot;-running-times.json\u0026quot;) ] most_recent_dt = max( [x.split(\u0026quot;/\u0026quot;)[-1].replace(\u0026quot;-running-times.json\u0026quot;, \u0026quot;\u0026quot;) for x in existing_data] ) assert most_recent_dt == today_dt, \u0026quot;No Data Found for Today\u0026#39;s Date\u0026quot; s3_key = [x for x in existing_data if most_recent_dt in x][0] return s3_key def read_json_from_s3(s3_bucket: str, s3_key: str) -\u0026gt; str: s3 = boto3.resource(\u0026quot;s3\u0026quot;) obj = s3.Object(s3_bucket, s3_key) file_content = obj.get()[\u0026quot;Body\u0026quot;].read().decode(\u0026quot;utf-8\u0026quot;) json_content = json.loads(file_content) return json_content def _convert_to_12hr_format(hr: int) -\u0026gt; str: return datetime.strptime(str(hr), \u0026quot;%H\u0026quot;).strftime(\u0026quot;%I:%M %p\u0026quot;).strip(\u0026quot;0\u0026quot;) def format_run_times(run_times: List[dict]) -\u0026gt; str: if run_times: hour_fmt = [ f\u0026quot;\u0026lt;b\u0026gt;{_convert_to_12hr_format(x.get(\u0026#39;hour\u0026#39;))}:\u0026lt;/b\u0026gt;\u0026quot; for x in run_times ] temp_fmt = [f\u0026quot;{round(x.get(\u0026#39;temp\u0026#39;))}F with\u0026quot; for x in run_times] wind_speed_fmt = [ f\u0026quot;wind at {round(x.get(\u0026#39;wind_speed\u0026#39;))} mph\u0026quot; for x in run_times ] status_fmt = [f\u0026quot;and {x.get(\u0026#39;status\u0026#39;).lower()}\u0026quot; for x in run_times] fmt_msg = zip(hour_fmt, temp_fmt, wind_speed_fmt, status_fmt) fmt_msg_list = [\u0026quot; \u0026quot;.join(x) for x in fmt_msg] return fmt_msg_list else: return [\u0026quot;No Times to Run Today!\u0026quot;] def is_time_for_run(weather_hour: dict) -\u0026gt; bool: is_time = ( RUNNING_CONDS[\u0026quot;hour\u0026quot;][\u0026quot;min_hour\u0026quot;] \u0026lt;= weather_hour[\u0026quot;hour\u0026quot;] \u0026lt;= RUNNING_CONDS[\u0026quot;hour\u0026quot;][\u0026quot;max_hour\u0026quot;] ) is_temp = ( RUNNING_CONDS[\u0026quot;temp\u0026quot;][\u0026quot;min_temp\u0026quot;] \u0026lt;= weather_hour[\u0026quot;temp\u0026quot;] \u0026lt;= RUNNING_CONDS[\u0026quot;temp\u0026quot;][\u0026quot;max_temp\u0026quot;] ) is_wind = ( RUNNING_CONDS[\u0026quot;wind_speed\u0026quot;][\u0026quot;min_speed\u0026quot;] \u0026lt;= weather_hour[\u0026quot;wind_speed\u0026quot;] \u0026lt;= RUNNING_CONDS[\u0026quot;wind_speed\u0026quot;][\u0026quot;max_speed\u0026quot;] ) is_status = weather_hour[\u0026quot;status\u0026quot;] not in RUNNING_CONDS[\u0026quot;status\u0026quot;] if all([is_time, is_temp, is_wind, is_status]): return True else: return False def lambda_handler(event, context): # generate s3 key for most recent weather data running_data_path = find_most_recent_data_path(s3_bucket=S3_BUCKET) # read as str hourly_weather_data = read_json_from_s3( s3_bucket=S3_BUCKET, s3_key=running_data_path ) # convert to dict and extract weather data hourly_data_dict = eval(hourly_weather_data)[\u0026quot;weather_data\u0026quot;] # True or False filter based on hour, temperature, windspeed criteria run_time_bool = [is_time_for_run(x) for x in hourly_data_dict] # applies weather criteria and filters only to hours where critera are met run_times = [ time for (time, time_to_run) in zip(hourly_data_dict, run_time_bool) if time_to_run ] # beautify the message with a sprinkle of HTML running_msg_lst = format_run_times(run_times) running_msg_str = \u0026quot;\u0026lt;p\u0026gt;\u0026quot; + \u0026quot;\u0026lt;br/\u0026gt;\u0026quot;.join(running_msg_lst) + \u0026quot;\u0026lt;/p\u0026gt;\u0026quot; running_msg = f\u0026quot;\u0026quot;\u0026quot;\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Best Times to Run\u0026lt;/h1\u0026gt; {running_msg_str} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026quot;\u0026quot;\u0026quot; try: client = boto3.client(\u0026quot;ses\u0026quot;, region_name=AWS_REGION) response = client.send_email( Destination={ \u0026quot;ToAddresses\u0026quot;: [ RECIPIENT, ] }, Message={ \u0026quot;Body\u0026quot;: { \u0026quot;Html\u0026quot;: { \u0026quot;Charset\u0026quot;: CHARSET, \u0026quot;Data\u0026quot;: running_msg, }, }, \u0026quot;Subject\u0026quot;: { \u0026quot;Charset\u0026quot;: CHARSET, \u0026quot;Data\u0026quot;: SUBJECT, }, }, Source=SENDER, ) except Exception as exp: exception_type, exception_value, exception_traceback = sys.exc_info() err_msg = json.dumps( {\u0026quot;errorType\u0026quot;: exception_type.__name__, \u0026quot;errorMessage\u0026quot;: str(exception_value)} ) logger.error(err_msg) Most of the logic is concerned with accessing and formatting the data we collected in the first part. However, this is where we determine an inside or outside run. The two sections highlighted below are responsible for making this decision.\nRUNNING_CONDS = { \u0026quot;hour\u0026quot;: {\u0026quot;min_hour\u0026quot;: 13, \u0026quot;max_hour\u0026quot;: 19}, \u0026quot;status\u0026quot;: [\u0026quot;Rain\u0026quot;, \u0026quot;Snow\u0026quot;, \u0026quot;Smoke\u0026quot;], \u0026quot;wind_speed\u0026quot;: {\u0026quot;min_speed\u0026quot;: 0, \u0026quot;max_speed\u0026quot;: 30}, \u0026quot;temp\u0026quot;: {\u0026quot;min_temp\u0026quot;: 30, \u0026quot;max_temp\u0026quot;: 90}, } These are all of the criteria - time, status (I probably don‚Äôt want to run if there‚Äôs a üî•wildfire smoke üî•), wind speed, and temperature - and their limits used in making the running decision. The is_time_for_run function ensures that the forecast data satisfies all four conditions.\ndef is_time_for_run(weather_hour: dict) -\u0026gt; bool: is_time = ( RUNNING_CONDS[\u0026quot;hour\u0026quot;][\u0026quot;min_hour\u0026quot;] \u0026lt;= weather_hour[\u0026quot;hour\u0026quot;] \u0026lt;= RUNNING_CONDS[\u0026quot;hour\u0026quot;][\u0026quot;max_hour\u0026quot;] ) is_temp = ( RUNNING_CONDS[\u0026quot;temp\u0026quot;][\u0026quot;min_temp\u0026quot;] \u0026lt;= weather_hour[\u0026quot;temp\u0026quot;] \u0026lt;= RUNNING_CONDS[\u0026quot;temp\u0026quot;][\u0026quot;max_temp\u0026quot;] ) is_wind = ( RUNNING_CONDS[\u0026quot;wind_speed\u0026quot;][\u0026quot;min_speed\u0026quot;] \u0026lt;= weather_hour[\u0026quot;wind_speed\u0026quot;] \u0026lt;= RUNNING_CONDS[\u0026quot;wind_speed\u0026quot;][\u0026quot;max_speed\u0026quot;] ) is_status = weather_hour[\u0026quot;status\u0026quot;] not in RUNNING_CONDS[\u0026quot;status\u0026quot;] if all([is_time, is_temp, is_wind, is_status]): return True else: return False I‚Äôll receive a message (like the one below) in my inbox every weekday at Noon when these conditions are met. Otherwise, I‚Äôll receive the message below:\nOverall, it looks like a solid day for a run. The one thing to note is that 3 PM and 4 PM do not have any information. The absence of data at these times indicates that at least one of the criteria was not met. Indeed, the local weather forecast showed rain for those times, so they were automatically filtered out in the message, leaving only times that met all four criteria. Portland, Oregon (my home) is a rainy place, and this sort of granular information is beneficial for those days where you get a brief window of dryness to go run.\n Parting Thoughts I hope this was a helpful introduction to setting up and running a basic Lambda workflow. It is a useful service, and I‚Äôve found numerous applications in my day-to-day life beyond just helping me plan my daily run. Please feel free to comment below if you have any thoughts or questions. Until next time, happy coding!\n ","date":1621563194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621563194,"objectID":"e98c845c8feca2895baeb6d94199c3b4","permalink":"http://example.org/post/2021-05-21-time-to-run-with-aws/running_alert_aws_lambda/","publishdate":"2021-05-20T21:13:14-05:00","relpermalink":"/post/2021-05-21-time-to-run-with-aws/running_alert_aws_lambda/","section":"post","summary":"Learn how I built an AWS powered alert system that finds the perfect time to go running!","tags":["AWS","Lambda","Python","Running"],"title":"Time to Run with AWS","type":"post"},{"authors":null,"categories":["PySpark","Time-Series Forecasting","Prophet","Python"],"content":" Overview Whether predicting daily demand for thousands of products or the number of workers to staff across many distribution centers, generating operational forecasts in parallel is a common task for data scientists. Accordingly, the goal of this post is to outline an approach for creating many forecasts via PySpark. We‚Äôll cover some common data-cleaning steps that often precede forecasting, and then generate several thousand week-level demand predictions for a variety consumer products. Note that we will not cover how to implement this workflow in a cloud computing environment (which, in a real-world setting, would typically be the case). Nor will we delve into model tuning or selection. The goal is to provide a straightforward workflow for quickly generating many time-series forecasts in parallel.\n Getting Started We‚Äôll use data originally provided by Walmart that represents weekly demand for products at the store-department level. All code for this post is stored in the Codeforest Repository. Before diving into the details, let‚Äôs briefly review the key modules and files.\nconf.json - A configuration file that defines various parameters for our job. It‚Äôs a good practice to keep these parameters outside of your actual code, as it makes it easier for others (or future you!) to adapt and extend to other use cases. pyspark_fcast.py - Our main module, or where the forecasting gets done. We‚Äôll cover this in detail below.\nfcast_data_frame.py - A class responsible for common pre-forecasting data transformations. These include filling in missing values, filtering time-series with only a few observations, or log transforming our outcome variable. Visit here for access to all methods.\nYou‚Äôll also need to import the following packages to follow along with the tutorial.\nimport argparse import json import logging import os import re from datetime import datetime from pathlib import Path from typing import List import numpy as np import pandas as pd from fbprophet import Prophet # fbprophet==0.7.1 \u0026amp; pystan==2.18.0 from pyspark.sql import SparkSession # pyspark==3.0.1 from pyspark.sql.functions import lit from pyspark.sql.types import (DateType, FloatType, IntegerType, StructField, StructType) from pyspark_ts_fcast.fcast_data_frame import FcastDataFrame Assuming the imports were successful, we‚Äôll peak at a few rows in our data to get a feel for the format. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rqobitfmng .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rqobitfmng .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rqobitfmng .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rqobitfmng .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #rqobitfmng .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rqobitfmng .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rqobitfmng .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rqobitfmng .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rqobitfmng .gt_column_spanner_outer:first-child { padding-left: 0; } #rqobitfmng .gt_column_spanner_outer:last-child { padding-right: 0; } #rqobitfmng .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #rqobitfmng .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #rqobitfmng .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rqobitfmng .gt_from_md  :first-child { margin-top: 0; } #rqobitfmng .gt_from_md  :last-child { margin-bottom: 0; } #rqobitfmng .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rqobitfmng .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #rqobitfmng .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rqobitfmng .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #rqobitfmng .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rqobitfmng .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rqobitfmng .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rqobitfmng .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rqobitfmng .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rqobitfmng .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #rqobitfmng .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rqobitfmng .gt_sourcenote { font-size: 90%; padding: 4px; } #rqobitfmng .gt_left { text-align: left; } #rqobitfmng .gt_center { text-align: center; } #rqobitfmng .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rqobitfmng .gt_font_normal { font-weight: normal; } #rqobitfmng .gt_font_bold { font-weight: bold; } #rqobitfmng .gt_font_italic { font-style: italic; } #rqobitfmng .gt_super { font-size: 65%; } #rqobitfmng .gt_footnote_marks { font-style: italic; font-size: 65%; }     Sample Data       store dept date weekly_sales    1 1 2010-02-05 24924   1 1 2010-02-12 46039   1 1 2010-02-19 41596   1 1 2010-02-26 19404   1 1 2010-03-05 21828    Let‚Äôs now discuss the process of passing and documenting the forecasting parameters. We‚Äôll execute the following from the command line to generate our forecasts:\npython3 pyspark_fcast.py --forecast-config-file 'config/conf.json'\nHere we are passing in the location of our configuration file and extracting the parameters. Don‚Äôt worry if the individual parameters don‚Äôt make sense now. I‚Äôll explain each in greater detail below.\nargs = read_args() with open(args.forecast_config_file) as f: config = json.load(f) log_input_params(config=config) # forecasting parameters input_data_path = config[\u0026quot;input_data_path\u0026quot;] fcast_params = config[\u0026quot;fcast_parameters\u0026quot;] group_fields = fcast_params[\u0026quot;group_fields\u0026quot;] date_field = fcast_params[\u0026quot;date_field\u0026quot;] yvar_field = fcast_params[\u0026quot;yvar_field\u0026quot;] ts_frequency = fcast_params[\u0026quot;ts_frequency\u0026quot;] fcast_floor = fcast_params[\u0026quot;forecast_floor\u0026quot;] fcast_cap = fcast_params[\u0026quot;forecast_cap\u0026quot;] min_obs_threshold = fcast_params[\u0026#39;min_obs_count\u0026#39;] # spark parameters spark_n_threads = str(config[\u0026#39;spark_n_threads\u0026#39;]) java_home = config[\u0026quot;java_home\u0026quot;] Note the two helper functions: read_args and log_input_params.\ndef read_args() -\u0026gt; argparse.Namespace: \u0026quot;\u0026quot;\u0026quot;Read Forecasting arguments Returns: argparse.Namespace: argparse Namespace \u0026quot;\u0026quot;\u0026quot; parser = argparse.ArgumentParser() parser.add_argument(\u0026quot;--forecast-config-file\u0026quot;, type=str) return parser.parse_args() read_args takes arguments in our configuration file, then we document which parameters we‚Äôre using with log_input_params.\nlogging.basicConfig( format=\u0026quot;%(levelname)s - %(asctime)s - %(filename)s - %(message)s\u0026quot;, level=logging.INFO, filename=\u0026quot;run_{start_time}.log\u0026quot;.format( start_time=datetime.now().strftime(\u0026quot;%Y-%m-%d %H-%M-%S\u0026quot;) ), ) def log_input_params(config: dict) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Logs all parameters in configuration file Args: config (dict): Configuration parameters for forecast and data \u0026quot;\u0026quot;\u0026quot; params = pd.json_normalize(config).transpose() [ logging.info(\u0026quot;input params:\u0026quot; + x[0] + \u0026quot;-\u0026quot; + str(x[1])) for x in zip(params.index, params.iloc[:, 0]) ] return None There are several benefits to documenting our inputs. First, we can validate if the correct parameters have been passed to our forecasting process. Having a record of these values facilitates debugging. Second, it is useful for experimentation. We can try out different parameters to see which combination provides the best results. Logging does not receive a lot of attention in the data science world, but it is incredibly useful and will save you time as your project matures.\nWe have our parameters and have set up logging. Next, we‚Äôll read in the data stored here and execute some basic field formatting with clean_names.\ndef clean_names(df: pd.DataFrame) -\u0026gt; pd.DataFrame: \u0026quot;\u0026quot;\u0026quot;Applies the following transformations to column names: - Removes camel case - Replaces any double underscore with single underscore - Removes spaces in the middle of a string name - Replaces periods with underscores Args: df (pd.DataFrame): Dataframe with untransformed column names Returns: pd.DataFrame: Dataframe with transformed column names \u0026quot;\u0026quot;\u0026quot; cols = df.columns cols = [re.sub(r\u0026quot;(?\u0026lt;!^)(?=[A-Z])\u0026quot;, \u0026quot;_\u0026quot;, x).lower() for x in cols] cols = [re.sub(r\u0026quot;_{2,}\u0026quot;, \u0026quot;_\u0026quot;, x) for x in cols] cols = [re.sub(r\u0026quot;\\s\u0026quot;, \u0026quot;\u0026quot;, x) for x in cols] cols = [re.sub(r\u0026quot;\\.\u0026quot;, \u0026quot;_\u0026quot;, x) for x in cols] df.columns = cols return df sales_df = pd.read_csv(input_data_path) sales_df = clean_names(sales_df) If you don‚Äôt have a clean_names-type function as part of your codebase, I‚Äôd highly recommend creating one. It‚Äôs a function that I use frequently when reading data from various sources and encourages a standardized way of formatting field names.\nNow that we have our data, we‚Äôll do some pre-forecasting data cleaning. The main steps are outlined below:\nFilter groups with limited observations - It‚Äôs a good idea to put predictions against items where you have some historical data. While the space of cold-start forecasting is very interesting, it is outside the scope of this post. Thus, we are putting a minimum threshold on the number of data points per group. This is also a good idea because some forecasting algorithms will not fit a model against a few observations, causing your program to crash.\nReplace negative values with zero - I‚Äôm assuming a negative value represents a returned product. Our goal is to forecast demand not demand - returns. This is an assumption that would need to be validated with domain knowledge.\nPad missing values - Accounting for missing data is an easy step to overlook for one simple reason: Missing values in time-series data are not usually flagged as ‚Äúmissing‚Äù. For example, a store may shut down for six weeks of renovations. As a result, there will be a series of dates that have no sales data. Identifying these gaps is pivotal for generating reliable forecasts. I‚Äôve provided a brief example below to illustrate what this looks like from a data perspective.\nhtml { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nbpomebbll .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #nbpomebbll .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nbpomebbll .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #nbpomebbll .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #nbpomebbll .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nbpomebbll .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nbpomebbll .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #nbpomebbll .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #nbpomebbll .gt_column_spanner_outer:first-child { padding-left: 0; } #nbpomebbll .gt_column_spanner_outer:last-child { padding-right: 0; } #nbpomebbll .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #nbpomebbll .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #nbpomebbll .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #nbpomebbll .gt_from_md  :first-child { margin-top: 0; } #nbpomebbll .gt_from_md  :last-child { margin-bottom: 0; } #nbpomebbll .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nbpomebbll .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nbpomebbll .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nbpomebbll .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nbpomebbll .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nbpomebbll .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nbpomebbll .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #nbpomebbll .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nbpomebbll .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nbpomebbll .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #nbpomebbll .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nbpomebbll .gt_sourcenote { font-size: 90%; padding: 4px; } #nbpomebbll .gt_left { text-align: left; } #nbpomebbll .gt_center { text-align: center; } #nbpomebbll .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nbpomebbll .gt_font_normal { font-weight: normal; } #nbpomebbll .gt_font_bold { font-weight: bold; } #nbpomebbll .gt_font_italic { font-style: italic; } #nbpomebbll .gt_super { font-size: 65%; } #nbpomebbll .gt_footnote_marks { font-style: italic; font-size: 65%; }     Incomplete Data       store dept date weekly    1 1 2010-02-05 24924   1 1 2010-02-19 41596   1 1 2010-02-26 19404   1 1 2010-03-19 22137   1 1 2010-03-26 26229    html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rggjwgclsv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rggjwgclsv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rggjwgclsv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rggjwgclsv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #rggjwgclsv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rggjwgclsv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rggjwgclsv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rggjwgclsv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rggjwgclsv .gt_column_spanner_outer:first-child { padding-left: 0; } #rggjwgclsv .gt_column_spanner_outer:last-child { padding-right: 0; } #rggjwgclsv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #rggjwgclsv .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #rggjwgclsv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rggjwgclsv .gt_from_md  :first-child { margin-top: 0; } #rggjwgclsv .gt_from_md  :last-child { margin-bottom: 0; } #rggjwgclsv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rggjwgclsv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #rggjwgclsv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rggjwgclsv .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #rggjwgclsv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rggjwgclsv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rggjwgclsv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rggjwgclsv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rggjwgclsv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rggjwgclsv .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #rggjwgclsv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rggjwgclsv .gt_sourcenote { font-size: 90%; padding: 4px; } #rggjwgclsv .gt_left { text-align: left; } #rggjwgclsv .gt_center { text-align: center; } #rggjwgclsv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rggjwgclsv .gt_font_normal { font-weight: normal; } #rggjwgclsv .gt_font_bold { font-weight: bold; } #rggjwgclsv .gt_font_italic { font-style: italic; } #rggjwgclsv .gt_super { font-size: 65%; } #rggjwgclsv .gt_footnote_marks { font-style: italic; font-size: 65%; }     Padded Data       store dept date weekly    1 1 2010-02-05 24924   NA NA 2010-02-12 NA   1 1 2010-02-19 41596   1 1 2010-02-26 19404   NA NA 2010-03-05 NA   NA NA 2010-03-12 NA   1 1 2010-03-19 22137   1 1 2010-03-26 26229    We‚Äôll go back and fill or ‚Äúinterpolate‚Äù those missing values in the weekly_sales field in a later step.\nFilter groups with long ‚Äòstreaks‚Äô of missing observations - Building on the previous example, let‚Äôs say the store closes for six months instead of six weeks. Thus, half of the year will not have any sales information. We could fill it in with a reasonable value, such as the average, but this won‚Äôt capture the overall trend, seasonality, or potential holiday/event effects that help to explain variation in our outcome variable. I‚Äôll often initially exclude these time-series, and then try to understand why/how long streaks of values are missing. In this case, we‚Äôll set a limit of four weeks (i.e., if any time-series has more than four consecutive dates missing, exclude from the final forecasting step).\nInterpolate missing values - Fills in missing data with ‚Äúreasonable‚Äù values. We‚Äôll use the overall mean of each series, which is a very simple and easy to understand technique. There are better approaches that account for seasonality or local trends. However, the goal here isn‚Äôt to generate the best forecast but instead to create a good starting point from which to iterate.\nAdd forecasting bounds - This function is specific to the Prophet API and is not required to generate a forecast via PySpark. However, when you cannot inspect the quality of each forecast, adding in some ‚Äúguardrails‚Äù can prevent errant predictions that erode trust with your stakeholders. The floor and cap fields provide bounds that a forecast cannot go above or below. For example, if the minimum value in a time-series is 10 and the maximum is 100, a floor of 0.5 and a cap of 1.5 ensures all forecasted values are not above 150 (100 * 1.5) or less than 5 (10 * 0.5). Again, these decisions are often driven by domain knowledge of the forecaster. We‚Äôll go a bit deeper on this field below as well.\nLog transform outcome variable - Log transforming our outcome variable is an effective approach to reduce the influence of outliers and stabilize variance that increases over time. A separate approach is to use a box-cox transformation (see here for more details), which can yield better results than a log-transformation. However, I often start with a log-transformation because it does require us to keep track of the transformation parameters, which is something you‚Äôll need to do with a box-cox transformation. Are we seeing a theme here? Start simple.\nWhew - that was a lot of information, but we can finally implement all of these data-cleaning steps via the FcastDataFrame class. The format was inspired by the sklearn.pipeline class to prepare and clean grouped time-series data prior to generating forecasts.\nclass FcastDataFrame: \u0026quot;\u0026quot;\u0026quot;Use for pre-processing data prior to forecasting\u0026quot;\u0026quot;\u0026quot; def __init__( self, df: pd.DataFrame, group_fields: List[str], date_field: str, yvar_field: str, ts_frequency: str, ): \u0026quot;\u0026quot;\u0026quot; Args: df (pd.DataFrame): dataframe with to be forecasted data group_fields (List[str]): grouping fields. These are often re represented by attributes of each unit (e.g., store id, product id, etc.). date_field (str): date field yvar_field (str): outcome (\u0026quot;y\u0026quot;) field ts_frequency (str): granularity of the data. For example, data that is recorded on a weekly basis, every Friday will be \u0026quot;W-FRI\u0026quot;. Note that sub-day level (e.g, hourly, minute) data is not supported. \u0026quot;\u0026quot;\u0026quot; self.df = df self.group_fields = group_fields self.date_field = date_field self.yvar_field = yvar_field self.ts_frequency = ts_frequency fcast_df = FcastDataFrame( df=sales_df, group_fields=group_fields, date_field=date_field, yvar_field=yvar_field, ts_frequency=ts_frequency, )  While we won‚Äôt cover all methods in this class, I‚Äôll briefly review one of the methods ‚Äì filter_groups_min_obs ‚Äì to illustrate the structure of the class.\ndef filter_groups_min_obs(self, min_obs_threshold: int): \u0026quot;\u0026quot;\u0026quot;Filters groups based on some minimum number of observations required for forecasting Args: min_obs_threshold (int): removes all groups with less obsevations than this threshold \u0026quot;\u0026quot;\u0026quot; n_unique_groups = self.df[self.group_fields].drop_duplicates().shape[0] min_obs_filter_df = ( self.df.groupby(self.group_fields)[self.yvar_field] .count() .reset_index() .rename(columns={self.yvar_field: \u0026quot;obs_count\u0026quot;}) .query(f\u0026quot;obs_count \u0026gt; {str(min_obs_threshold)}\u0026quot;) .drop(columns=\u0026quot;obs_count\u0026quot;) ) n_remaining_groups = min_obs_filter_df.shape[0] df = pd.merge(self.df, min_obs_filter_df, how=\u0026quot;inner\u0026quot;, on=self.group_fields) self.df = df logger.info(\u0026quot;N groups dropped: {}\u0026quot;.format(n_unique_groups - n_remaining_groups)) Each data transformation takes in our data, applies some filtering, cleaning, or formatting, logs the changes, and then replaces the existing DataFrame with the updated DataFrame. This pattern is applied at each step until we are satisfied with the changes. Let‚Äôs apply these filtering and cleaning steps below.\n# filter out groups with less than min number of observations fcast_df.filter_groups_min_obs(min_obs_threshold=min_obs_threshold) # replace any negative value with a zero fcast_df.replace_negative_value_with_zero() # replace missing dates between start and end of time-series by group fcast_df.pad_missing_values() # filter groups with consecutive missing streak longer than 4 fcast_df.filter_groups_max_missing_streak(max_streak=4) # impute missing values fcast_df.fill_missing_values() # add upper and lower bounds for forecasting fcast_df.add_forecast_bounds( floor_multiplier=fcast_floor, cap_multiplier=fcast_cap ) # log transform outcome, floor, and cap values fcast_df.log_transform_values(yvar_field, \u0026quot;floor_value\u0026quot;, \u0026quot;cap_value\u0026quot;) # return transformed data fcast_df_trans = fcast_df.return_transformed_df() Now we are ready to do some forecasting. In the next section, we‚Äôll produce our forecasts from the cleaned and prepared data.\n  Pyspark Forecasting Let‚Äôs start by translating the field names to those that Prophet understands. For example, our date variable should be named ds and our outcome variable y. We‚Äôll use the prep_for_prophet function to make the transition.\ndef prep_for_prophet( df: pd.DataFrame, yvar_field: str, date_field: str, group_fields: List[str] ) -\u0026gt; pd.DataFrame: \u0026quot;\u0026quot;\u0026quot;Renames key field names to be compatible with Prophet Forecasting API Args: df (pd.DataFrame): Contains data that will be used to generate forecasting yvar_field (str): outcome (\u0026quot;y\u0026quot;) field name date_field (str): date field name group_fields (List[str]): grouping fields. These are often represented by attributes of each unit (e.g., store id, product id, etc.). Returns: pd.DataFrame: Data with compatible field names \u0026quot;\u0026quot;\u0026quot; fields = df.columns.tolist() cap_value_index = [ index for index, value in enumerate([\u0026quot;cap_value\u0026quot; in x for x in fields]) if value ] floor_value_index = [ index for index, value in enumerate([\u0026quot;floor_value\u0026quot; in x for x in fields]) if value ] if cap_value_index and floor_value_index: df = df.rename( columns={ fields[cap_value_index[0]]: \u0026quot;cap\u0026quot;, fields[floor_value_index[0]]: \u0026quot;floor\u0026quot;, } ) group_fields = group_fields + [\u0026quot;cap\u0026quot;, \u0026quot;floor\u0026quot;] df = df[group_fields + [date_field] + [yvar_field]] df = df.rename(columns={date_field: \u0026quot;ds\u0026quot;, yvar_field: \u0026quot;y\u0026quot;}) df[\u0026quot;ds\u0026quot;] = pd.to_datetime(df[\u0026quot;ds\u0026quot;]) return df fcast_df_prophet_input = prep_for_prophet( df=fcast_df_trans, yvar_field=\u0026quot;weekly_sales_prep_log1p\u0026quot;, date_field=date_field, group_fields=group_fields, )  With our data prepared, we‚Äôll shift over to creating a Spark Session and indicate where our Java version is located. Note this step will vary depending on your computing environment.\nos.environ[\u0026quot;JAVA_HOME\u0026quot;] = java_home SPARK = ( SparkSession.builder.master(f\u0026quot;local[{spark_n_threads}]\u0026quot;) .appName(config[\u0026quot;app_name\u0026quot;]) .config(\u0026quot;spark.sql.execution.arrow.pyspark.enabled\u0026quot;, \u0026quot;true\u0026quot;) .getOrCreate() ) Next, we‚Äôll define the schema (or format) of our input and output data.\nINPUT_SCHEMA = StructType( [ StructField(\u0026quot;store\u0026quot;, IntegerType(), True), StructField(\u0026quot;dept\u0026quot;, IntegerType(), True), StructField(\u0026quot;cap\u0026quot;, FloatType(), True), StructField(\u0026quot;floor\u0026quot;, FloatType(), True), StructField(\u0026quot;ds\u0026quot;, DateType(), True), StructField(\u0026quot;y\u0026quot;, FloatType(), True), ] ) OUTPUT_SCHEMA = StructType( [ StructField(\u0026quot;ds\u0026quot;, DateType(), True), StructField(\u0026quot;store\u0026quot;, IntegerType(), True), StructField(\u0026quot;dept\u0026quot;, IntegerType(), True), StructField(\u0026quot;yhat_lower\u0026quot;, FloatType(), True), StructField(\u0026quot;yhat_upper\u0026quot;, FloatType(), True), StructField(\u0026quot;yhat\u0026quot;, FloatType(), True), ] )  We‚Äôll now translate our Pandas DataFrame to a Spark DataFrame and pass in the schema we defined above.\nfcast_spark_prophet_input = SPARK.createDataFrame( fcast_df_prophet_input, schema=INPUT_SCHEMA ) The function below does the actual forecasting and we‚Äôll spend some time unpacking what‚Äôs happening here.\ndef run_forecast(keys, df): \u0026quot;\u0026quot;\u0026quot;Generate time-series forecast Args: keys: Grouping keys df: Spark Dataframe \u0026quot;\u0026quot;\u0026quot; fields = [\u0026quot;ds\u0026quot;, \u0026quot;store\u0026quot;, \u0026quot;dept\u0026quot;, \u0026quot;yhat_lower\u0026quot;, \u0026quot;yhat_upper\u0026quot;,\u0026quot;yhat\u0026quot;] store, dept = keys cap = df[\u0026quot;cap\u0026quot;][0] floor = df[\u0026quot;floor\u0026quot;][0] model = Prophet( interval_width=0.95, growth=\u0026quot;logistic\u0026quot;, yearly_seasonality=True, seasonality_mode=\u0026quot;additive\u0026quot;, ) model.add_country_holidays(country_name=\u0026quot;US\u0026quot;) model.fit(df) future_df = model.make_future_dataframe( periods=13, freq=\u0026quot;W-FRI\u0026quot;, include_history=False ) future_df[\u0026quot;cap\u0026quot;] = cap future_df[\u0026quot;floor\u0026quot;] = floor results_df = model.predict(future_df) results_df[\u0026quot;store\u0026quot;] = store results_df[\u0026quot;dept\u0026quot;] = dept results_df = results_df[fields] return results_df Let‚Äôs start by discussing the Prophet model, which automates the selection of many forecasting settings, like seasonality, determined during the model fitting process. Below is a brief summary of some of the key settings:\ninterval_width - Interval width quantifies uncertainty in our forecast. Wider intervals indicate greater uncertainty. Here, we are indicating that the actual values should fall outside of the interval ~5% of the time. By default, Prophet is set to 80%, which is less conservative than our setting here. Providing a measure of uncertainty is perhaps even more important than the forecast itself, as it allows a business to hedge against the risk of being wrong. For example, imagine a product has a high margin and a low inventory holding cost. In this instance, you would want to plan to a high percentile, as you rarely want to stock out of this product.\nyearly_seasonality - Setting this to True indicates my belief that there is week-over-week variation that repeats itself over the course of a year. For example, sales for items like sandals or sunscreen are likely higher in Summer weeks and lower in the Winter weeks. There are two other seasonality options not included above - daily and hourly. Daily captures hourly changes within a day, while hourly captures minute-by-minute changes within an hour. Our data is at the week level, so we can ignore these two settings.\ngrowth - Growth is a way to encode our beliefs regarding if a forecast should reach a ‚Äúsaturation‚Äù point across your prediction horizon (see here for official documentation). For example, customer acquisition slows as a market matures and will eventually reach a saturation point (i.e., the total addressable market has been acquired). This is typically used for ‚Äúlong-range‚Äù forecasting on the scale of several years. Our forecasting horizon is much shorter at only 13 weeks. However, I like to codify what I consider to be reasonable amount of growth, via the ‚Äúcap‚Äù parameter, as well as contraction, via the ‚Äúfloor‚Äù parameter, in my forecasts, especially when I cannot inspect each result.\nseasonality_mode - I‚Äôve selected ‚Äúadditive‚Äù for this parameter based on my belief that the magnitude of seasonal changes do not vary across time. Recall that our outcome variable has already been log-transformed, thus we are actually using an additive decomposition of the log-transformed values.\nadd_country_holidays - Holidays tend to drive increases in consumption of particular products. And some holidays, like Easter, are not consistent year-over-year. Thus, you can improve forecasting accuracy if you anticipate how demand shifts when generating forecasts based on when holidays occur. One thing to note that is not included in the current post (but is incredibly useful) is the ability to apply a lower_window and upper_window to each holiday date. Continuing with our Easter example, you can imagine egg sales increase in the days leading up to Easter. Sales on the holiday date may not be that high, unless you are doing some last minute shopping. By extending the lower_window parameter for this holiday to something like -5, you can capture the elevated demand during the five days that precede Easter.\nNow that we are familiar with how the model is being tuned, let‚Äôs generate the forecasts. This may take a few minutes depending on how many threads you are using. I am using four, and it took about 20 minutes to complete.\nfcast_df_prophet_output = ( fcast_spark_prophet_input.groupBy(group_fields) .applyInPandas(func=run_forecast, schema=OUTPUT_SCHEMA) .withColumn(\u0026quot;part\u0026quot;, lit(\u0026quot;forecast\u0026quot;)) .withColumn(\u0026quot;fcast_date\u0026quot;, lit(datetime.now().strftime(\u0026quot;%Y-%m-%d\u0026quot;))) .toPandas() .rename( columns={ \u0026quot;yhat\u0026quot;: yvar_field, \u0026quot;yhat_lower\u0026quot;: f\u0026quot;{yvar_field}_lb\u0026quot;, \u0026quot;yhat_upper\u0026quot;: f\u0026quot;{yvar_field}_ub\u0026quot;, \u0026quot;ds\u0026quot;: date_field, } ) ) We should have 13-week forecasts for all store-department combinations. Our next steps are to combine the forecasts with the historical data and invert our log-transformation of the outcome variable to get back to our original scale. Note that np.log1p and np.expm1 are inverses of one another, and elegantly deal with zero values by adding/subtracting a value of ‚Äú1‚Äù to avoid taking the log of zero, which is undefined and will make your code go üí•. Lastly, we‚Äôll write the results out to our root directory.\nfcast_df_prophet_input[\u0026quot;part\u0026quot;] = \u0026quot;actuals\u0026quot; fcast_df_prophet_input = fcast_df_prophet_input.rename( columns={\u0026quot;y\u0026quot;: yvar_field, \u0026quot;ds\u0026quot;: date_field} ) del fcast_df_prophet_input[\u0026quot;cap\u0026quot;] del fcast_df_prophet_input[\u0026quot;floor\u0026quot;] ret_df = pd.concat([fcast_df_prophet_input, fcast_df_prophet_output]) ret_df = ret_df.apply(lambda x: round(np.expm1(x)) if yvar_field in x.name else x) ret_df.to_csv(Path.cwd() / \u0026quot;sales_data_forecast.csv\u0026quot;, index=False)  Quality Assurance We‚Äôll transition back to the world of R for some quick quality-assurance work. Let‚Äôs read in our forecasts and examine a few store-department combinations. Note there are much more formal ways to validate the performance of our models, but our objective is to do a quick sanity check (i.e., ‚Äúdo the forecasts look reasonable for a few randomly sampled grouped?‚Äù). The raw output is stored in Github. Let‚Äôs start by examining the first and last five rows for a single Store-Dept combination.\nlibrary(tidyverse) library(timetk) library(lubridate) fcast_df_url = \u0026quot;https://raw.githubusercontent.com/thecodeforest/codeforest_datasets/main/pyspark_forecasting_data/sales_data_forecast.csv\u0026quot; fcast_df = read_csv(fcast_df_url) df_store_dept_sample \u0026lt;- fcast_df %\u0026gt;% filter(store == 1, dept == 1) %\u0026gt;% mutate(date = as_date(date)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #vajqssudrv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 9px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #vajqssudrv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vajqssudrv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #vajqssudrv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #vajqssudrv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vajqssudrv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vajqssudrv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #vajqssudrv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #vajqssudrv .gt_column_spanner_outer:first-child { padding-left: 0; } #vajqssudrv .gt_column_spanner_outer:last-child { padding-right: 0; } #vajqssudrv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #vajqssudrv .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #vajqssudrv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #vajqssudrv .gt_from_md  :first-child { margin-top: 0; } #vajqssudrv .gt_from_md  :last-child { margin-bottom: 0; } #vajqssudrv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #vajqssudrv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #vajqssudrv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vajqssudrv .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #vajqssudrv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vajqssudrv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #vajqssudrv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #vajqssudrv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vajqssudrv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vajqssudrv .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #vajqssudrv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vajqssudrv .gt_sourcenote { font-size: 90%; padding: 4px; } #vajqssudrv .gt_left { text-align: left; } #vajqssudrv .gt_center { text-align: center; } #vajqssudrv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #vajqssudrv .gt_font_normal { font-weight: normal; } #vajqssudrv .gt_font_bold { font-weight: bold; } #vajqssudrv .gt_font_italic { font-style: italic; } #vajqssudrv .gt_super { font-size: 65%; } #vajqssudrv .gt_footnote_marks { font-style: italic; font-size: 65%; }     Top 5 Rows of Forecasting Data       store dept date weekly_sales part weekly_sales_lb weekly_sales_ub fcast_date    1 1 2010-02-05 24924 actuals NA NA NA   1 1 2010-02-12 46039 actuals NA NA NA   1 1 2010-02-19 41596 actuals NA NA NA   1 1 2010-02-26 19404 actuals NA NA NA   1 1 2010-03-05 21828 actuals NA NA NA    html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #uwmuwyxmsv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 9px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #uwmuwyxmsv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uwmuwyxmsv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #uwmuwyxmsv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #uwmuwyxmsv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uwmuwyxmsv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uwmuwyxmsv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #uwmuwyxmsv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #uwmuwyxmsv .gt_column_spanner_outer:first-child { padding-left: 0; } #uwmuwyxmsv .gt_column_spanner_outer:last-child { padding-right: 0; } #uwmuwyxmsv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #uwmuwyxmsv .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #uwmuwyxmsv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #uwmuwyxmsv .gt_from_md  :first-child { margin-top: 0; } #uwmuwyxmsv .gt_from_md  :last-child { margin-bottom: 0; } #uwmuwyxmsv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #uwmuwyxmsv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #uwmuwyxmsv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uwmuwyxmsv .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #uwmuwyxmsv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uwmuwyxmsv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #uwmuwyxmsv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #uwmuwyxmsv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uwmuwyxmsv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uwmuwyxmsv .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #uwmuwyxmsv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uwmuwyxmsv .gt_sourcenote { font-size: 90%; padding: 4px; } #uwmuwyxmsv .gt_left { text-align: left; } #uwmuwyxmsv .gt_center { text-align: center; } #uwmuwyxmsv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #uwmuwyxmsv .gt_font_normal { font-weight: normal; } #uwmuwyxmsv .gt_font_bold { font-weight: bold; } #uwmuwyxmsv .gt_font_italic { font-style: italic; } #uwmuwyxmsv .gt_super { font-size: 65%; } #uwmuwyxmsv .gt_footnote_marks { font-style: italic; font-size: 65%; }     Bottom 5 Rows of Forecasting Data       store dept date weekly_sales part weekly_sales_lb weekly_sales_ub fcast_date    1 1 2012-12-28 30948 forecast 21883 42839 2021-04-05   1 1 2013-01-04 21138 forecast 14793 30024 2021-04-05   1 1 2013-01-11 16149 forecast 11384 22832 2021-04-05   1 1 2013-01-18 15553 forecast 10712 21662 2021-04-05   1 1 2013-01-25 18954 forecast 13475 27282 2021-04-05    Let‚Äôs sample a few forecasts and plot them out.\nset.seed(2021) fcast_df %\u0026gt;% filter(store \u0026lt; 3, dept %in% c(df %\u0026gt;% distinct(dept) %\u0026gt;% sample_n(2) %\u0026gt;% pull()) ) %\u0026gt;% mutate(store = paste0(\u0026#39;Store: \u0026#39;, store), dept = paste0(\u0026#39;Dept: \u0026#39;, dept), store_id = paste(store, dept, sep=\u0026#39; \u0026#39;)) %\u0026gt;% select(date, store_id, contains(\u0026#39;weekly\u0026#39;)) %\u0026gt;% pivot_longer(contains(\u0026#39;weekly\u0026#39;)) %\u0026gt;% mutate(name = str_to_title(str_replace_all(name, \u0026#39;_\u0026#39;, \u0026#39; \u0026#39;))) %\u0026gt;% ggplot(aes(date, value, color = name)) + geom_line(size = 1.5, alpha = 0.8) + facet_grid(store_id ~ ., scales = \u0026#39;free\u0026#39;) + theme_bw() + scale_y_continuous(labels = scales::comma_format()) + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;Weekly Sales\u0026#39;, color = NULL, title = \u0026#39;Sample Forecasts\u0026#39; ) + theme(legend.position = \u0026quot;top\u0026quot;, legend.text = element_text(size = 12), strip.text.y = element_text(size = 12), plot.title = element_text(size = 14) ) Overall, the forecasts appear to capture changes in the trend and seasonal variation. A more formal approach to this problem is to do back-testing by holding out some historical data and generating forecasts against it. However, this is a great starting point from which to build more advanced models and incorporate external variables to further improve our forecasts. Hopefully this is enough to get you started on your way to forecasting at an enterprise scale. Until next time, happy forecasting!\n ","date":1617070394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617070394,"objectID":"b80014d95722605d7f4bfae86202f384","permalink":"http://example.org/post/2021-03-28-pyspark-forecasting/pyspark_time_series_forecasting/","publishdate":"2021-03-29T21:13:14-05:00","relpermalink":"/post/2021-03-28-pyspark-forecasting/pyspark_time_series_forecasting/","section":"post","summary":"Whether predicting daily demand for thousands of products or the number of workers to staff across many distribution centers, generating operational forecasts in parallel is a common task for data scientists. Accordingly, the goal of this post is to outline an approach for creating many forecasts via PySpark.","tags":["PySpark","Time-Series Forecasting","Prophet","Python"],"title":"Scalable Time-Series Forecasting in Python","type":"post"},{"authors":null,"categories":["Causal Inference","Propensity Scores","R"],"content":" Overview Causal inference attempts to answer ‚Äúwhat-if‚Äù questions. For example, if the minimum wage were increased, what effect would it have on unemployment rates? Or if an entertainment company launched a marketing campaign for a new movie, what effect would it have on box-office sales? The objective in each of these examples is to quantify the impact of an intervention ‚Äì a change in wages or a targeted marketing campaign ‚Äì on an outcome ‚Äì increasing employment or bolstering revenue. Estimating how a particular action can affect an end-state falls within the realm of prescriptive analytics and can inform decision-making in the face of multiple possible actions.\nHowever, most analytics efforts are applied to either describing or predicting an outcome rather than understanding what drives it. For example, imagine you work for a cheese shop. You might be asked to describe how sales of cheese have changed over the past year. Or perhaps you want to predict how much cheese will sell over the next 12 months. Descriptive analytics can reveal if existing operational or strategic decisions are impacting the business (i.e., cheese sales) as anticipated. Predictive analytics can inform operational planning (e.g., how much cheese to manufacture), improve consumer experiences (e.g., an online cheese recommendation system), or automate repetitive tasks (e.g., automatically detecting defective cheese wheels during production with computer vision). While all of the applications can provide valuable answers to different questions, none can provide insight into the source of variation or root cause(s) of change in an outcome. Without this knowledge, it can be difficult to know where resources should be focused or how to grow and improve the business.\nAccordingly, the goal of this post is to highlight one approach to conducting prescriptive analytics and generating causal inferences with observational data. We‚Äôll first walk through some of the basics of causal inference and propensity scores, followed by a practical example that brings these concepts together. At the end of this post, you should have a solid understanding of how propensity scores can be used in the real world to guide decision-making.\n Causal Inference \u0026amp; Propensity Scores When people hear the words ‚Äúcausal inference‚Äù, they often think ‚ÄúA/B Test‚Äù. Indeed, the traditional way of answering causal questions is to randomly assign individuals to a treatment or control condition. The treatment is exposed to the intervention, while the control is not. The average difference is then calculated between the two conditions on some measure of interest to understand if the intervention had the desired effect.\nWhile A/B testing is considered the most rigorous way of inferring causation, it is not practical or possible in many situations. For example, if you were interested in the effect of a membership program on future purchasing behavior, you cannot assign customers to be a member or non-member; customers would enroll in the program under their own volition. Further, customers who enrolled as members are probably more interested in the product than those who did not enroll. This fact ‚Äúconfounds‚Äù the relationship between the effect of our member program on purchasing behavior.\nPropensity score matching attempts to address this issue, known as selection bias, by adjusting for factors that relate both to the treatment and outcome (i.e., confounding variables). A propensity score is scaled from 0 - 1 and indicates the probability of receiving treatment. Continuing with our previous membership example, a propensity score indicates the probability that a customer joins our membership program after seeing a banner on our website or receiving a promotional email. It does not indicate their probability of making a future purchase. Formalizing the roles of individual variables that increase/decrease membership enrollment and their interrelations is the topic of the next section.\n Causal Graphs We can codify our beliefs and assumptions about observational data through a causal graph. This is normally the first step on our journey of causal inference, as it allows us to translate domain knowledge into a formal structure. By creating a diagram about potential confounding variables as well as the direction of causal influence, we make our assumptions about the data generating process explicit.\nIn the context of the current example, we assume that a customer in enrolling as a member influences future purchase behavior, not that future purchase behavior influences enrollment in membership. We can then encode this assumption in our causal graph. The exclusion of certain variables from our graph (e.g., age, gender, what types of products someone has previously purchased, etc.) is also an assumption, such that we assume these variables do not directly or indirectly affect purchase frequency or membership.\nThese assumptions can and should be verified. If we believe a customer‚Äôs age affects purchase frequency and membership enrollment, we can stratify our customers by age (i.e,., 20-29, 30-39) and test both hypotheses. If there were significant differences between groups, we would include an age variable in our graph and adjust for its influence on the treatment and outcome.\nThis is a contrived example, so we‚Äôll keep things simple and formalize the main components of our analysis as follows:\nPurchase Frequency - the total number of purchases six months following the launch of our membership program. This is our outcome variable.\nMembership - if a customer enrolled as a member since the launch of the membership program. This is our treatment variable.\nEngagement - this is an example of a latent variable. We would use several variables in practice but, to keep things simple, we‚Äôll only use prior purchase history, defined as the total number of purchases in the six months before the launch of the membership program. This variable will serve as a proxy for Engagement. We assume that customers who have made more purchases in the past six months will be inclined to make more purchases in the future ‚Äì that is, more engaged in the past translates into more engaged in the future. We also assume that this (partially) motivates membership enrollment. Engaged customers will not only purchase more frequently but also be more interested in exclusive offers and discounts ‚Äì a few benefits provided to members ‚Äì relative to customers that have historically purchased infrequently.\nThe image above was created via the daggity website, which makes it easy to create Causal DAGs or Directed Acyclic Graphs. Note the goal of creating a propensity score is to block the arrow from Engagement to Purchase Frequency. This addresses the issue of selection bias, in that our customers can ‚Äúselect into‚Äù the member condition. By adjusting for this pre-existing difference, we are attempting to make this bias strongly ignorable, similar to a randomized experiment.\nAnother aspect to consider is when an individual joined our membership program. We want to allow enough time for differences to emerge, so ideally a few months have elapsed so we can see what happens. Second, membership offers and the quality may change over time, just as the consumer‚Äôs relationship with our brand changes. By narrowing the time frame of analysis, we can further control for time-related factors.\nLast, we want to time-bound prior purchase history. Some customers may have frequently purchased in the past but have not been active for several years (or churned completely). We want to ensure that all customers in our sample have a chance of being exposed to the treatment. Thus, we could apply simple logic to narrow our consideration set, such as ‚Äúall customers that have engaged with the brand in some capacity (e.g., made a purchase, browsed the website, or opened a marketing communication) since the start of our member program‚Äù. This is not a hard-and-fast rule but something to consider when deciding which individuals to include in your analysis.\n Estimating the Effect of Membership on Purchase Frequency Now that we have a solid conceptual foundation, let‚Äôs continue to work through our membership example by generating some contrived data.\nlibrary(tidyverse) library(broom) library(rsample) library(janitor) # set base theme as black \u0026amp; white theme_set(theme_bw()) set.seed(2021) # sample size of members and non-members n = 5000 # expected purchase frequency base_lambda = .75 # purchase frequency effect size for \u0026quot;more engaged\u0026quot; customers engagement_effect_size = .25 less_engaged = rpois(n=n, lambda = base_lambda) more_engaged = rpois(n=n, lambda = base_lambda + engagement_effect_size) # create tibble with number of previous purchases for each customer purchase_df \u0026lt;- tibble(n_purchase_pre = c(less_engaged, more_engaged)) In the code block above, we expect ‚Äúless engaged‚Äù customers to make 0.75 purchases (on average) over six months, and ‚Äúmore engaged‚Äù customers to make one purchase over the same period. The difference in purchase frequency between our customer types is ascribed to our latent variable of Engagement. We assume the data generating process for historical purchase frequency can be represented by the Poisson distribution. Recall that the Poisson distribution models the number of events expected to occur within a given period. It also approximates consumer purchase frequency patterns in the real world, such that most customers make a small number of purchases, while a few customers make a large number of purchases.\nWe established our expected purchase frequency and engagement effect size above,so let‚Äôs simulate the effect of Engagement on Membership. We‚Äôll create three bins and assign a probability of enrolling as a member within each bin, such that higher bins (i.e., the top 33% of customers) have a higher probability of enrolling relative to lower bins (i.e., the bottom 33% of customers).\nmembership_sim \u0026lt;- function(bin){ if(bin == 1){ return(rbinom(1, 1, prob = 0.2)) } else if (bin == 2){ return(rbinom(1, 1, prob = 0.3)) } else { return(rbinom(1, 1, prob = 0.4)) } } purchase_df \u0026lt;- purchase_df %\u0026gt;% mutate(bin = ntile(n_purchase_pre, 3), member_enrolled = map_int(bin, membership_sim) ) %\u0026gt;% select(-bin) While we know the true effect size (given that we generated the numbers), let‚Äôs verify the assumption that our proxy for engagement (Prior Purchases) exhibits the hypothesized effect on membership.\npurchase_df %\u0026gt;% mutate(n_purchase_pre = fct_lump(as.factor(n_purchase_pre), n=2, other_level = \u0026#39;2 or More\u0026#39; )) %\u0026gt;% group_by(n_purchase_pre) %\u0026gt;% summarise(pct_member = mean(member_enrolled)) %\u0026gt;% ggplot(aes(n_purchase_pre, pct_member)) + theme_bw() + geom_col() + coord_flip() + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + labs(x = \u0026#39;N Prior Purchases\u0026#39;, y = \u0026#39;Percent Enrolled as Members\u0026#39; ) + theme(axis.text.x = element_text(size = 12, angle = 90), axis.text.y = element_text(size = 14), axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), legend.text = element_text(size = 14), legend.title = element_text(size = 16), strip.text.y = element_text(size = 14) ) Now that we have validated the relationship between these variables, we‚Äôll create the joint effect of treatment (Membership) and our single covariate (Engagement) on our outcome (Purchase Frequency).\npurchase_sim \u0026lt;- function(n_purchase_pre, member_enrolled){ membership_effect_size = .1 if(member_enrolled == 1){ post_purchase_freq = rpois(n=1, lambda=n_purchase_pre + membership_effect_size) return(post_purchase_freq) } else { post_purchase_freq = rpois(n=1, lambda=n_purchase_pre) return(post_purchase_freq) } } purchase_df \u0026lt;- purchase_df %\u0026gt;% mutate(n_purchase_post = map2_int(n_purchase_pre, member_enrolled, purchase_sim) ) The purchase_sim function accounts for the number of prior purchases as well as if the customer has enrolled as a member. If they have enrolled, the true effect size is .1, in that enrolling a customer as a member leads to .1 additional purchases (on average) during the six months.\nBelow, we‚Äôll confirm that members have made more purchases relative to non-members.\navg_purchase_freq \u0026lt;- purchase_df %\u0026gt;% group_by(member_enrolled) %\u0026gt;% summarise(avg_post_purchase = mean(n_purchase_post), se = sqrt(mean(n_purchase_post) / n()) ) %\u0026gt;% mutate(is_member = str_to_title(ifelse(member_enrolled == 1, \u0026#39;member\u0026#39;, \u0026#39;non-member\u0026#39;)), is_member = fct_reorder(is_member, avg_post_purchase), lb = avg_post_purchase - 1.96 * se, ub = avg_post_purchase + 1.96 * se ) avg_purchase_freq %\u0026gt;% ggplot(aes(is_member, avg_post_purchase, fill = is_member)) + geom_col(color = \u0026#39;black\u0026#39;) + geom_errorbar(aes(ymin = lb, ymax=ub), width=0.4) + coord_flip() + labs(x = NULL, y = \u0026#39;Average Number of Purchases\u0026#39;, fill = \u0026#39;Membership Status\u0026#39; ) + theme(axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12), axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), legend.text = element_text(size = 14), legend.title = element_text(size = 16), legend.position = \u0026#39;top\u0026#39; ) Indeed, this confirms that the data aligns with our expectations. We could reach the same conclusion by fitting a regression model and then considering the magnitude of the coefficient for membership.\nincorrect_fit \u0026lt;- glm(n_purchase_post ~ member_enrolled, data = purchase_df, family = \u0026quot;poisson\u0026quot; ) incorrect_fit_coef \u0026lt;- tidy(incorrect_fit) intercept \u0026lt;- incorrect_fit_coef %\u0026gt;% filter(term == \u0026#39;(Intercept)\u0026#39;) %\u0026gt;% pull(estimate) member_est \u0026lt;- incorrect_fit_coef %\u0026gt;% filter(term == \u0026#39;member_enrolled\u0026#39;) %\u0026gt;% pull(estimate) effect_size_est \u0026lt;- round(exp(intercept + member_est) - exp(intercept), 3) print(glue::glue(\u0026#39;Estimated Difference: {effect_size_est}\u0026#39;)) ## Estimated Difference: 0.402 Note that if we did not adjust for any confounding variables, we would mistakenly conclude that membership leads to ~.3 extra purchases per customer. That is, we would overestimate the effect size of membership on customer purchase behaviors because we know that Engagement affects both Membership and Purchase Frequency. In the following section, we‚Äôll generate propensity scores to create more balance between our control and treatment groups on our confounding variables.\n Propensity Scores Two common approaches for creating comparison groups with observational data are (1) propensity score matching and (2) inverse probability of treatment weighting (IPTW). Both methods use a propensity score but create groups differently. Matching looks for individuals in the non-treated condition who have similar propensity scores to those in the treated condition. If groups are different sizes, the number of non-treated observations is reduced to the size of the treated condition, as each treated observation is matched with a non-treated observation. In contrast, weighting includes all observations but places more weight on observations with high propensity scores and less weight on observations with low propensity scores. While both approaches can yield similar results, I prefer weighting because you are not discarding any data.\nBelow, we‚Äôll specify our model for generating propensity scores.\n# specify model for estimating P(treatment | Previous Purchases) model_spec \u0026lt;- as.formula(member_enrolled ~ n_purchase_pre) member_model \u0026lt;- glm(model_spec, data = purchase_df, family = binomial()) member_prop_df \u0026lt;- member_model %\u0026gt;% augment(type.predict = \u0026#39;response\u0026#39;, data = purchase_df) %\u0026gt;% select(member_enrolled, n_purchase_pre, n_purchase_post, member_prob = .fitted) %\u0026gt;% mutate(iptw = 1 / ifelse(member_enrolled == 0, 1 - member_prob, member_prob)) A few things to note since we‚Äôve created our propensity scores. First, we used logistic regression to estimate membership probability. However, any classification model can generate a propensity score. If there are non-linearities between your covariates and treatment variable, using a model that can better capture these relationships, such as a tree-based model, may yield better estimates.\nSecond, we‚Äôll need to be cognizant of the resulting weights. If certain observations receive very large weights, they will have an outsized influence on our coefficient estimates. It is a common practice to truncate large weights at 10 (why 10 I‚Äôm not sure). I‚Äôd prefer to use a point from our actual distribution, so we‚Äôll assign any value above the 99th percentile to the value at the 99th percentile.\niptw_pct_99 \u0026lt;- quantile(member_prop_df %\u0026gt;% pull(iptw), 0.99)[[1]] member_prop_df \u0026lt;- member_prop_df %\u0026gt;% mutate(iptw = ifelse(iptw \u0026gt; iptw_pct_99, iptw_pct_99, iptw)) Now that we‚Äôve addressed some common pre-modeling issues, let‚Äôs generate an initial estimate of the effect of Membership on Purchase Frequency.\nmembership_fit \u0026lt;- glm(n_purchase_post ~ member_enrolled, data = member_prop_df, family = \u0026#39;poisson\u0026#39;, weights = iptw ) membership_fit_coef \u0026lt;- tidy(membership_fit) intercept \u0026lt;- membership_fit_coef %\u0026gt;% filter(term == \u0026#39;(Intercept)\u0026#39;) %\u0026gt;% pull(estimate) member_est \u0026lt;- membership_fit_coef %\u0026gt;% filter(term == \u0026#39;member_enrolled\u0026#39;) %\u0026gt;% pull(estimate) effect_size_est_adj \u0026lt;- round(exp(intercept + member_est) - exp(intercept), 3) print(glue::glue(\u0026#39;Estimated Difference: {effect_size_est_adj}\u0026#39;)) ## Estimated Difference: 0.097 Our initial estimate indicates that membership leads to ~.1 extra purchases, which matches perfectly with the true effect size! This is a great start, but we also need to consider certainty in the estimate. In our example, we have a fairly large sample size, there is only one covariate, and the distribution of treatment (members vs.¬†non-members) is relatively even between groups. Real world data sets, on the other hand, are often small, present a high degree of skew between groups, or exhibit intricate causal structures. The presence of these factors affects how accurately we can estimate a true effect, and using the method above to create a confidence interval can lead to incorrect estimates of our standard error (too small). To address this issue, we‚Äôll bootstrap the entire process - from generating our propensity score weights to estimating our causal effect - and then use the resulting distribution to better quantify uncertainty in our estimate.\nmember_fit_bootstrap \u0026lt;- function(split){ temp_df \u0026lt;- analysis(split) temp_model \u0026lt;- glm(member_enrolled ~ n_purchase_pre, family = binomial(), data = temp_df ) temp_df \u0026lt;- temp_model %\u0026gt;% augment(type.predict = \u0026#39;response\u0026#39;, data = temp_df) %\u0026gt;% select(member_enrolled, n_purchase_pre, n_purchase_post, member_prob = .fitted) %\u0026gt;% mutate(iptw = 1 / ifelse(member_enrolled == 0, 1 - member_prob, member_prob)) temp_iptw_pct_99 \u0026lt;- quantile(temp_df %\u0026gt;% pull(iptw), 0.99)[[1]] temp_df \u0026lt;- temp_df %\u0026gt;% mutate(iptw = ifelse(iptw \u0026gt; temp_iptw_pct_99, temp_iptw_pct_99, iptw)) temp_ret_df \u0026lt;- glm(n_purchase_post ~ member_enrolled, data = temp_df, family = \u0026#39;poisson\u0026#39;, weights = iptw) %\u0026gt;% tidy() return(temp_ret_df) } n_boot = 500 boot_results \u0026lt;- bootstraps(purchase_df, n_boot, apparent = TRUE) %\u0026gt;% mutate(results = map(splits, member_fit_bootstrap)) In the above code snippet, we created 500 boot-strapped replicates and then fit a model to each. Our next step is to look at the distribution of the resulting estimates.\nboot_results_unnest \u0026lt;- boot_results %\u0026gt;% select(-splits) %\u0026gt;% unnest(cols=results) boot_results_est \u0026lt;- boot_results_unnest %\u0026gt;% select(id, term, estimate) %\u0026gt;% pivot_wider(names_from = term, values_from = estimate ) %\u0026gt;% clean_names() %\u0026gt;% mutate(est_effect = exp(member_enrolled + intercept) - exp(intercept)) boot_results_summary \u0026lt;- boot_results_est %\u0026gt;% summarise(lb = quantile(est_effect, 0.025), mdn = quantile(est_effect, 0.5), ub = quantile(est_effect, 0.975) ) %\u0026gt;% pivot_longer(everything()) lb_est \u0026lt;- boot_results_summary %\u0026gt;% filter(name==\u0026#39;lb\u0026#39;) %\u0026gt;% pull(value) ub_est \u0026lt;- boot_results_summary %\u0026gt;% filter(name==\u0026#39;ub\u0026#39;) %\u0026gt;% pull(value) effect_est \u0026lt;- boot_results_summary %\u0026gt;% filter(name==\u0026#39;mdn\u0026#39;) %\u0026gt;% pull(value) boot_results_est %\u0026gt;% ggplot(aes(x=est_effect)) + geom_histogram(fill=\u0026#39;grey90\u0026#39;, color = \u0026#39;black\u0026#39;) + theme_bw() + geom_segment(aes(x=lb_est, xend = lb_est), y = 0, yend = 10, lty = 3, size = 2) + geom_segment(aes(x=effect_est, xend = effect_est), y = 0, yend = 10, lty = 3, size = 2) + geom_segment(aes(x=ub_est, xend = ub_est), y = 0, yend = 10, lty = 3, size = 2) + annotate(geom=\u0026#39;text\u0026#39;, x=lb_est, y = 11, label = round(lb_est, 2), size = 8) + annotate(geom=\u0026#39;text\u0026#39;, x=effect_est, y = 11, label = round(effect_est, 2), size = 8) + annotate(geom=\u0026#39;text\u0026#39;, x=ub_est, y = 11, label = round(ub_est, 2), size = 8) + labs(x = \u0026#39;Estimated Treatment Effect\u0026#39;) + theme( axis.text.x = element_text(size = 14), axis.text.y = element_text(size = 14), axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14) ) Our final estimate of the causal effect of membership on purchase frequency is between 0.06 and 0.14. Note that these confidence intervals aren‚Äôt much different from those in the original, non-bootstrapped approach. As mentioned previously, the primary reason is that our sample size for both groups is fairly large and there is not a lot of variance in the determinants of membership, given that we generated the data. However, in many instances, confidence intervals following bootstrapping will be wider ‚Äì and more accurate ‚Äì than those provided by OLS.\nHopefully, this provided a solid end-to-end walkthrough of how to generate causal inferences from observational data with propensity scores. In the next post, we‚Äôll discuss an equally important topic ‚Äì variance reduction methods ‚Äì that come in handy when you can run a true A/B test. Until then, happy experimenting!\n ","date":1615860794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615860794,"objectID":"28912b7c365a0ae6dbcae7e8e96f7613","permalink":"http://example.org/post/2021-05-01-propensity-scores/causal_inference_propensity_scores/","publishdate":"2021-03-15T21:13:14-05:00","relpermalink":"/post/2021-05-01-propensity-scores/causal_inference_propensity_scores/","section":"post","summary":"This post highlights one approach to generating causal inferences with observational data. We‚Äôll first walk through some of the basics of causal inference and propensity scores, followed by a practical example that brings these concepts together. At the end of this post, you should have a solid understanding of how propensity scores can be used in the real world to guide decision-making","tags":["Causal Inference","Propensity Scores","R"],"title":"Causal Inference with Propensity Scores","type":"post"},{"authors":null,"categories":["R","Emperical Bayes","Fantasy Football"],"content":" Overview In less than two weeks, Fantasy Football will once again resume for the 2019 NFL season! While I‚Äôm looking forward to the impending draft, the start of the season brings back memories of a not-so-distant loss that left me one game shy of the championship. The loss stemmed from a missed field goal, leaving my team two points shy of victory. Of course, a myriad of factors beyond that missed field goal contributed to my fantasy demise, but those two points reinvigorated a question I‚Äôve wondered about for the past few years: Why are kickers drafted in the last round?\nPrevailing wisdom suggests that your kicker doesn‚Äôt matter. Some Fantasy Football leagues don‚Äôt even have kickers on the roster, which I think does a disservice to a player who probably doesn‚Äôt get invited to the cool team parties yet can decide the fate of a season in a single moment (like mine). As long as they suit up to take the field, the rest is out of your control. However, is it a suboptimal strategy to relegate your choice of kicker to the final round of the draft? Let‚Äôs find out!\n Getting Started Before loading any data or discussing techniques, we‚Äôll begin by defining our analytical objective. An easy way to get started is by posing a simple question: ‚ÄúHow many more points can I expect over a 16-game regular season if I draft the best kicker relative to the worst kicker?‚Äù We‚Äôll answer this question in two steps. First, we‚Äôll estimate the True field goal percentage for each kicker currently active in the NFL (as of 2016), which is analogous to a batting average in baseball or free-throw percentage in basketball. This parameter estimate will be used to compare the skill of one kicker to another. Second, we‚Äôll translate our estimate into actual Fantasy Football points by simulating the outcomes 1000 football seasons for each kicker. Simulation enables us to quantify a realistic point differential between kickers, which is what we (the Fantasy Football team owners) will use to determine if we should try to select the best kicker by drafting in an earlier round.\nWith that question in mind, let‚Äôs load all pertinent libraries. The data can be downloaded directly from the üéã the codeforest data repo üéÑ. Note the original data comes from Kaggle and can found here.\n# Modeling library(gamlss) # Core packages library(tidyverse) library(janitor) # Visualization library(ggplot2) library(scales) library(viridis) library(ggridges) # Tables library(gt) # Global plot theme theme_set(theme_minimal()) # Code Forest repo data_url \u0026lt;- \u0026quot;https://raw.githubusercontent.com/thecodeforest/codeforest_datasets/main/fantasy_football_kickers_data/Career_Stats_Field_Goal_Kickers.csv\u0026quot; # Helper function for visualization my_plot_theme = function(){ font_family = \u0026quot;Helvetica\u0026quot; font_face = \u0026quot;bold\u0026quot; return(theme( axis.text.x = element_text(size = 16, face = font_face, family = font_family), axis.text.y = element_text(size = 16, face = font_face, family = font_family), axis.title.x = element_text(size = 16, face = font_face, family = font_family), axis.title.y = element_text(size = 16, face = font_face, family = font_family), strip.text.y = element_text(size = 22, face = font_face, family = font_family), plot.title = element_text(size = 22, face = font_face, family = font_family), legend.position = \u0026quot;top\u0026quot;, legend.title = element_text(size = 16, face = font_face, family = font_family), legend.text = element_text(size = 16, face = font_face, family = font_family), legend.key = element_rect(size = 5), legend.key.size = unit(1.5, \u0026#39;lines\u0026#39;) )) } There are several columns we won‚Äôt be using so we‚Äôll select only the relevant ones.\nstats_raw \u0026lt;- read_csv(data_url) %\u0026gt;% clean_names() %\u0026gt;% select(player_id, name, year, games_played, contains(\u0026#39;made\u0026#39;), contains(\u0026#39;attempted\u0026#39;), contains(\u0026#39;percentage\u0026#39;), -contains(\u0026#39;extra\u0026#39;), -longest_fg_made ) glimpse(stats_raw) ## Rows: 1,994 ## Columns: 19 ## $ player_id \u0026lt;chr\u0026gt; \u0026quot;jeffhall/2500970\u0026quot;, \u0026quot;benagajanian/2508255\u0026quot;‚Ä¶ ## $ name \u0026lt;chr\u0026gt; \u0026quot;Hall, Jeff\u0026quot;, \u0026quot;Agajanian, Ben\u0026quot;, \u0026quot;Agajanian‚Ä¶ ## $ year \u0026lt;dbl\u0026gt; 2000, 1964, 1962, 1961, 1961, 1960, 1957, ‚Ä¶ ## $ games_played \u0026lt;dbl\u0026gt; 3, 3, 6, 3, 3, 14, 12, 10, 12, 12, 10, 12,‚Ä¶ ## $ f_gs_made \u0026lt;chr\u0026gt; \u0026quot;4\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;13\u0026quot;, \u0026quot;10\u0026quot;, \u0026quot;5\u0026quot;, ‚Ä¶ ## $ f_gs_made_20_29_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;‚Ä¶ ## $ f_gs_made_30_39_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;‚Ä¶ ## $ f_gs_made_40_49_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;‚Ä¶ ## $ f_gs_made_50_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;‚Ä¶ ## $ f_gs_attempted \u0026lt;chr\u0026gt; \u0026quot;5\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;14\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;9\u0026quot;, \u0026quot;24\u0026quot;, \u0026quot;18\u0026quot;, \u0026quot;13\u0026quot;‚Ä¶ ## $ f_gs_attempted_20_29_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;‚Ä¶ ## $ f_gs_attempted_30_39_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;‚Ä¶ ## $ f_gs_attempted_40_49_yards \u0026lt;chr\u0026gt; \u0026quot;2\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;‚Ä¶ ## $ f_gs_attempted_50_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;‚Ä¶ ## $ fg_percentage \u0026lt;chr\u0026gt; \u0026quot;80.0\u0026quot;, \u0026quot;50.0\u0026quot;, \u0026quot;35.7\u0026quot;, \u0026quot;50.0\u0026quot;, \u0026quot;33.3\u0026quot;, \u0026quot;5‚Ä¶ ## $ fg_percentage_20_29_yards \u0026lt;chr\u0026gt; \u0026quot;100.0\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--‚Ä¶ ## $ fg_percentage_30_39_yards \u0026lt;chr\u0026gt; \u0026quot;100.0\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--‚Ä¶ ## $ fg_percentage_40_49_yards \u0026lt;chr\u0026gt; \u0026quot;50.0\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;‚Ä¶ ## $ fg_percentage_50_yards \u0026lt;chr\u0026gt; \u0026quot;100.0\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--‚Ä¶ Like most real-world datasets, this one is a bit messy (e.g., non-values are coded as ‚Äú‚Äì‚Äù). I find it helps at the outset of data cleaning to envision what a perfect, pristine dataset should look like once data munging steps are complete. Below is an example of a basic starting point. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #zpvivrqdou .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #zpvivrqdou .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zpvivrqdou .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #zpvivrqdou .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #zpvivrqdou .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zpvivrqdou .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zpvivrqdou .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #zpvivrqdou .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #zpvivrqdou .gt_column_spanner_outer:first-child { padding-left: 0; } #zpvivrqdou .gt_column_spanner_outer:last-child { padding-right: 0; } #zpvivrqdou .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #zpvivrqdou .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #zpvivrqdou .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #zpvivrqdou .gt_from_md  :first-child { margin-top: 0; } #zpvivrqdou .gt_from_md  :last-child { margin-bottom: 0; } #zpvivrqdou .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #zpvivrqdou .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #zpvivrqdou .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zpvivrqdou .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #zpvivrqdou .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zpvivrqdou .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #zpvivrqdou .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #zpvivrqdou .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zpvivrqdou .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zpvivrqdou .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #zpvivrqdou .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zpvivrqdou .gt_sourcenote { font-size: 90%; padding: 4px; } #zpvivrqdou .gt_left { text-align: left; } #zpvivrqdou .gt_center { text-align: center; } #zpvivrqdou .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #zpvivrqdou .gt_font_normal { font-weight: normal; } #zpvivrqdou .gt_font_bold { font-weight: bold; } #zpvivrqdou .gt_font_italic { font-style: italic; } #zpvivrqdou .gt_super { font-size: 65%; } #zpvivrqdou .gt_footnote_marks { font-style: italic; font-size: 65%; }     Desired Data Format       id n_success n_trials    1 5 10   2 7 30   3 1 8   4 10 12   5 20 24   6 30 50   7 60 200   8 2 4   9 11 14   10 24 61   \nI used generic column names if you‚Äôre interested in adopting the techniques described herein to solve a separate problem. At a basic level, each row represents an individual observation, a count of the number of successes (i.e., count how many field goals are made), and finally the number of trials (i.e., count how many field goals are attempted). If you have this setup, the building blocks are in place to get started.\nHowever, before going any further, we need to ensure the relationships in the data align with our understanding of the world. One approach is to generate some simple hypotheses that you know to be true. For example, water is wet, the sky is blue, and, in our case, the field goal percentage should decrease as the distance to the goal increases. That is, field goals taken from 50+ yards should be made at a lower rate those taken from 30-35 yards. Let‚Äôs verify our hypothesis below.\nmake_by_dist \u0026lt;- stats_raw %\u0026gt;% select(starts_with(\u0026quot;fg_percentage_\u0026quot;)) %\u0026gt;% mutate_all(as.numeric) %\u0026gt;% gather(key = \u0026quot;dist\u0026quot;, value = \u0026quot;fg_pct\u0026quot;) %\u0026gt;% mutate( dist = str_extract(dist, pattern = \u0026quot;\\\\d{2}\u0026quot; ), dist = if_else(dist == \u0026quot;50\u0026quot;, paste0(dist, \u0026quot;+\u0026quot;), paste0(dist,\u0026quot;-\u0026quot;,as.numeric(dist) + 9) ), fg_pct = fg_pct / 100 ) %\u0026gt;% na.omit() make_by_dist %\u0026gt;% ggplot(aes(fg_pct, dist, fill = dist)) + geom_density_ridges( aes(point_color = dist, point_fill = dist, point_shape = dist), alpha = .2, point_alpha = 1, jittered_points = TRUE ) + scale_point_color_hue(l = 40) + scale_discrete_manual(aesthetics = \u0026quot;point_shape\u0026quot;, values = c(21, 22, 23, 24)) + scale_x_continuous(labels = scales::percent, breaks = c(0,0.2, 0.4, 0.6, 0.8, 1) ) + scale_fill_viridis_d() + my_plot_theme() + labs(x = \u0026#39;Field Goal Percentage\u0026#39;, y = \u0026#39;Distance (Yards)\u0026#39; ) + theme(legend.position = \u0026#39;none\u0026#39;) Looks good! Each point represents the field goal percentage for a player-season-distance combination. As the distance increases, the make rate gradually shifts to left, which is exactly what we‚Äôd expect. We‚Äôll do a bit more cleaning below before proceeding.\nstats_processed \u0026lt;- stats_raw %\u0026gt;% mutate( name = str_remove(name, \u0026quot;,\u0026quot;), first_name = map(name, function(x) str_split(x, \u0026quot; \u0026quot;)[[1]][2]), last_name = map(name, function(x) str_split(x, \u0026quot; \u0026quot;)[[1]][1]), player_id = str_extract(player_id, \u0026quot;\\\\d+\u0026quot;) ) %\u0026gt;% unite(\u0026quot;name\u0026quot;, c(\u0026quot;first_name\u0026quot;, \u0026quot;last_name\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% mutate_at(vars(matches(\u0026quot;attempted|made\u0026quot;)), as.numeric) %\u0026gt;% replace(., is.na(.), 0) %\u0026gt;% select(player_id, name, year, games_played, contains(\u0026quot;made\u0026quot;), contains(\u0026quot;attempted\u0026quot;)) %\u0026gt;% rename( fg_made = f_gs_made, fg_attempted = f_gs_attempted ) Let‚Äôs view the resulting data for one of the best kickers in modern NFL to familiarize ourselves with the format.\nstats_processed %\u0026gt;% filter(name == \u0026quot;Justin Tucker\u0026quot;) %\u0026gt;% mutate(fg_pct = fg_made / fg_attempted) %\u0026gt;% select(name, year, fg_made, fg_attempted) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #pkjidbziro .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #pkjidbziro .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pkjidbziro .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #pkjidbziro .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #pkjidbziro .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pkjidbziro .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pkjidbziro .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #pkjidbziro .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #pkjidbziro .gt_column_spanner_outer:first-child { padding-left: 0; } #pkjidbziro .gt_column_spanner_outer:last-child { padding-right: 0; } #pkjidbziro .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #pkjidbziro .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #pkjidbziro .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #pkjidbziro .gt_from_md  :first-child { margin-top: 0; } #pkjidbziro .gt_from_md  :last-child { margin-bottom: 0; } #pkjidbziro .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #pkjidbziro .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #pkjidbziro .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pkjidbziro .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #pkjidbziro .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pkjidbziro .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #pkjidbziro .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #pkjidbziro .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pkjidbziro .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pkjidbziro .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #pkjidbziro .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pkjidbziro .gt_sourcenote { font-size: 90%; padding: 4px; } #pkjidbziro .gt_left { text-align: left; } #pkjidbziro .gt_center { text-align: center; } #pkjidbziro .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #pkjidbziro .gt_font_normal { font-weight: normal; } #pkjidbziro .gt_font_bold { font-weight: bold; } #pkjidbziro .gt_font_italic { font-style: italic; } #pkjidbziro .gt_super { font-size: 65%; } #pkjidbziro .gt_footnote_marks { font-style: italic; font-size: 65%; }     Justin Tucker Stats       name year fg_made fg_attempted    Justin Tucker 2016 38 39   Justin Tucker 2015 33 40   Justin Tucker 2014 29 34   Justin Tucker 2013 38 41   Justin Tucker 2012 30 33    Just like what we had above! Next, we‚Äôll add a few filters to reduce some of the noise in our data. Any player who has less than 30 field goal attempts and/or has kicked field goals in only one season across their career will be excluded from the analysis. Additionally, we‚Äôll ignore any players with a rookie year before the 1970s. The rationale here is that the NFL made several changes to the location and positioning of the goal during the early 70s, so we want to keep the dynamics of the kicking environment consistent for all players.\nmin_attempts \u0026lt;- 30 min_seasons \u0026lt;- 2 min_decade \u0026lt;- 1970 filter_df \u0026lt;- stats_processed %\u0026gt;% group_by(player_id) %\u0026gt;% summarise(n_seasons = n(), n_attempts = sum(fg_attempted), rookie_decade = min(year) %/% 10 * 10 ) %\u0026gt;% filter(n_seasons \u0026gt;= min_seasons, n_attempts \u0026gt;= min_attempts, rookie_decade \u0026gt;= min_decade ) %\u0026gt;% select(player_id) stats_processed \u0026lt;- inner_join(stats_processed, filter_df) Next, we‚Äôll add a few features before aggregating the kicking data from a season level to a career level for each player. I‚Äôll cover the rationale of the features shortly in the estimation and inference section below.\ndf_time_stats \u0026lt;- stats_processed %\u0026gt;% group_by(player_id) %\u0026gt;% summarise( rookie_decade = min(year) %/% 10 * 10, last_yr_active = max(year) ) %\u0026gt;% ungroup() %\u0026gt;% filter(rookie_decade \u0026gt;= min_decade) %\u0026gt;% mutate(status = ifelse(last_yr_active == 2016, \u0026#39;active\u0026#39;, \u0026#39;inactive\u0026#39;)) stats_processed \u0026lt;- inner_join(stats_processed,df_time_stats) We have our time-based features and the last step is to calculate our three key metrics ‚Äì successes, attempts, and our rate metric.\nstats_agg \u0026lt;- stats_processed %\u0026gt;% group_by(player_id, name, rookie_decade, status) %\u0026gt;% summarise(fg_made = sum(fg_made), fg_attempted = sum(fg_attempted), fg_pct = fg_made / fg_attempted ) %\u0026gt;% ungroup() Time to move on to the key focus of this post.\n Estimation and Inference Let‚Äôs now discuss the logic underlying our estimation method as well as the role of the additional features (Note that some of the code below was inspired by the excellent book Introduction to Empirical Bayes: Examples from Baseball Statistics by David Robinson). To recap, we are estimating a proportion that captures the relationship between successes and attempts. We can model this outcome with the beta distribution, which is simply a distribution of probabilities ranging from 0 - 1. In our case, it represents the likelihood of a particular field goal percentage for each player, which will fall somewhere between 0.5 and 0.9 depending on the decade(s) the player was active (more on that in second).\nBelow we‚Äôll fit an null model with no additional parameters when estimating each player‚Äôs beta value. The absence of any inputs means that all players have the same prior, independent of what decade they played in, whether they‚Äôre still active, or how many chances they‚Äôve had to kick a field goal. We‚Äôll then take our prior and update it based on how much information we have about each player, namely the number of field goals they‚Äôve taken and how often they‚Äôve succeeded.\nfit_null \u0026lt;- gamlss(cbind(fg_made, fg_attempted - fg_made) ~ 1, family = BB(mu.link = \u0026quot;identity\u0026quot;), data = stats_agg ) ## GAMLSS-RS iteration 1: Global Deviance = 936.6917 ## GAMLSS-RS iteration 2: Global Deviance = 836.9846 ## GAMLSS-RS iteration 3: Global Deviance = 828.0258 ## GAMLSS-RS iteration 4: Global Deviance = 827.9528 ## GAMLSS-RS iteration 5: Global Deviance = 827.9526 stats_agg_est \u0026lt;- stats_agg %\u0026gt;% mutate( mu = fitted(fit_null, \u0026quot;mu\u0026quot;), sigma = fitted(fit_null, \u0026quot;sigma\u0026quot;), alpha0 = mu / sigma, beta0 = (1 - mu) / sigma, alpha1 = alpha0 + fg_made, beta1 = beta0 + fg_attempted - fg_made, estimate = alpha1 / (alpha1 + beta1), raw = fg_made / fg_attempted, low = qbeta(.025, alpha1, beta1), high = qbeta(.975, alpha1, beta1) ) Let‚Äôs plot out the estimate for all active players.\nstats_agg_est %\u0026gt;% mutate(name = paste0(name, \u0026quot;: \u0026quot;, fg_made, \u0026quot;|\u0026quot;, fg_attempted), name = fct_reorder(name, estimate) ) %\u0026gt;% filter(status == \u0026quot;active\u0026quot;) %\u0026gt;% ggplot(aes(name, estimate)) + geom_point(size = 3) + geom_errorbar(aes(ymin = low, ymax = high)) + coord_flip() + geom_point(aes(name, raw), color = \u0026quot;red\u0026quot;, size = 3, alpha = 0.6) + scale_y_continuous(labels = scales::percent_format()) + my_plot_theme() + labs(x = NULL, y = \u0026#39;Field Goal Percentage\u0026#39;, title = \u0026#39;Estimated field goal percentage amongst active NFL kickers\u0026#39;, subtitle = \u0026#39;Black dot represents estimate while red dot is actual. Note the bias in our estimates.\u0026#39; ) Let‚Äôs talk through this figure by comparing the field goal percentage estimates for Adam Vinatieri, who has made 530 of 629 fields goals throughout his career, to Chris Boswell, who has made 50 of 57 field goals. While Vinatieri has a lower actual make rate than Boswell (84.2% vs.¬†87.7%), we consider him to be a better field goal kicker. The seemingly incongruent finding is based on the fact that we have more evidence for Vinatieri (629 FG attempts vs.¬†57 FG attempts) than Boswell. It‚Äôs like saying, ‚ÄúChris Boswell is good kicker, maybe better than Vinatieri, but we don‚Äôt have enough evidence (yet) to believe he is that much better than an average kicker, a number represented by our prior‚Äù. Indeed, if we also consider the width of the credible intervals surrounding these two players, Adam Vinatieri‚Äôs interval is considerably smaller than Chris Boswell‚Äôs interval.\nWhile this is a good way to gain an intuition for what‚Äôs happening under the hood, we see an immediate problem ‚Äì all of our estimates are biased! The actual field goal percentage is above every single estimate. Luckily, there is a solution: we can create conditional estimates of our prior. One way to do this is to create features that explain variability between our players. For example, field goal percentages have improved dramatically over the past 50 years. Let‚Äôs consider our own data and map out this pattern from the 1970s to the 2010s.\nstats_agg %\u0026gt;% mutate(rookie_decade = as.factor(rookie_decade)) %\u0026gt;% ggplot(aes(rookie_decade, fg_pct, color = rookie_decade)) + geom_boxplot() + geom_jitter() + scale_y_continuous(labels = scales::percent_format()) + my_plot_theme() + scale_color_viridis_d() + theme(legend.position = \u0026quot;none\u0026quot;) + labs( x = \u0026quot;Decade\u0026quot;, y = \u0026quot;Field Goal Percentage\u0026quot;, title = \u0026#39;Kicker performance has improved over time\u0026#39; ) The best kicker in 1970s has a lower field goal percentage than the worst kicker in the 2010s. Including the decade of a kicker‚Äôs rookie season allows us to create a more informed prior. Thus, if we use the median field goal percentage of all kickers who debuted as rookies in 2010+, our best guess would be about 84%, whereas a kicker who debuted in the 1970s would be somewhere around 64%. This explains why the estimates from our null model were biased.\nThe second factor to consider is the number of field goal attempts per player, because better players have more opportunities to kick field goals. This makes intuitive sense and is captured in the following plot.\nstats_agg %\u0026gt;% ggplot(aes(log2(fg_attempted), fg_pct)) + geom_point(size = 3) + geom_smooth(span = 1) + scale_y_continuous(labels = scales::percent_format()) + my_plot_theme() + labs( x = \u0026quot;Log2(Total Attempts)\u0026quot;, y = \u0026quot;Field Goal percentage\u0026quot;, title = \u0026quot;Better kickers have more opportunities\u0026quot; ) Below we‚Äôll use the same model except this time we‚Äôll account for the number of field goal attempts and a player‚Äôs rookie decade.\nfit_complete \u0026lt;- gamlss(cbind(fg_made, fg_attempted - fg_made) ~ log2(fg_attempted) + rookie_decade, family = BB(mu.link = \u0026quot;identity\u0026quot;), data = stats_agg ) ## GAMLSS-RS iteration 1: Global Deviance = 918.3071 ## GAMLSS-RS iteration 2: Global Deviance = 714.8471 ## GAMLSS-RS iteration 3: Global Deviance = 668.3708 ## GAMLSS-RS iteration 4: Global Deviance = 668.1839 ## GAMLSS-RS iteration 5: Global Deviance = 668.1838 Much better! Our estimates do not exhibit the same degree of bias as before. Moreover, the width of our credible intervals shrank across all players. This makes sense, given that we can now condition our prior estimates on inputs that explain variability in the field goal percentage. While there are other factors that might improve our model (e.g., did a player‚Äôs team have their home games in a dome?), this is a good starting point for answering our original question.\n From Parameters to Points We have a model that does a reasonable job of estimating a kicker‚Äôs field goal percentage. Now we need to translate that into an estimate of fantasy points. This will take a few steps, but I‚Äôll outline each in turn. First, we need to estimate the average worth (in fantasy points) of each successful field goal. Typically, field goals less-than 40 yards are worth 3 points, 40 - 49 yards are worth 4 points, and 50 or more yards are worth 5 points. We‚Äôll use the 2016 season to come up with a global average. While we could technically account for distances of each player (e.g., some kickers are excellent at a longer distances, others not so much), this approach will give us a ‚Äúgood-enough‚Äù answer. Second, we‚Äôll estimate the average number of field goal attempts per season. This can vary widely from one season to the next for a given kicker, as it is contingent upon the offense getting within kicking range. Again, we‚Äôll keep it simple and just average the number of attempts across all players from the 2016 season.\n# Average points per FG pts_per_fg \u0026lt;- stats_processed %\u0026gt;% filter(year == 2016) %\u0026gt;% mutate(pt_3_fgs = (f_gs_made_20_29_yards + f_gs_made_30_39_yards) * 3, pt_4_fgs = f_gs_made_40_49_yards * 4, pt_5_fgs = f_gs_made_50_yards * 5, tot_pts = pt_3_fgs + pt_4_fgs + pt_5_fgs ) pts_per_fg \u0026lt;- round(sum(pts_per_fg$tot_pts) / sum(pts_per_fg$fg_made), 1) # Average number of attempts attempts_per_season \u0026lt;- stats_processed %\u0026gt;% filter(year == 2016) %\u0026gt;% pull(fg_attempted) %\u0026gt;% mean() %\u0026gt;% round() Here comes the fun part. Below we‚Äôll simulate 1000 seasons for each player by randomly generating 1000 values of beta. This value is based on the posterior estimates, alpha1 and beta1, produced by our model. The estimates will vary from one simulation to next, though most values will fall somewhere between 0.75 and 0.9. Better players like Justin Tucker will be near the high end of that range while player like Graham Gano will be near the lower end. We‚Äôll then take each estimate and plug it into the binomial distribution below. Recall that the binomial distribution is defined by a single parameter, which represents the probability of success. This is exactly what our estimate of beta represents! Given that all active players had an average of 27 FG attempts in 2016, each of the 1000 simulations will consist of 27 trials (or attempts_per_season) each with a slightly different probability of success (how likely they are to make a field goal on a given attempt). We‚Äôll lean on the purrr package to vectorize these operations.\nset.seed(2018) n_seasons \u0026lt;- 1000 est_active \u0026lt;- stats_agg_est %\u0026gt;% filter(status == \u0026#39;active\u0026#39;) est_make_pct \u0026lt;- map2(est_active %\u0026gt;% pull(alpha1), est_active %\u0026gt;% pull(beta1), function(x, y) rbeta(n_seasons, x, y) ) est_outcomes \u0026lt;- map(est_make_pct, function(x) rbinom(n = n_seasons, size = attempts_per_season, prob = x ) ) names(est_outcomes) \u0026lt;- est_active$name So much data! Below we‚Äôll plot the distribution of total points accumulated for each player across the 1000 simulated seasons. We‚Äôll create quantiles as a way to see how much overlap there is between players.\npt_simulation \u0026lt;- est_outcomes %\u0026gt;% tbl_df() %\u0026gt;% gather() %\u0026gt;% transmute(name = key, season_pts = value * pts_per_fg ) %\u0026gt;% group_by(name) %\u0026gt;% mutate(avg_pts = mean(season_pts)) %\u0026gt;% ungroup() %\u0026gt;% mutate(name = fct_reorder(name, avg_pts)) ## Warning: `tbl_df()` is deprecated as of dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. pt_simulation %\u0026gt;% ggplot(aes(season_pts, y = name, fill = factor(..quantile..))) + stat_density_ridges( geom = \u0026quot;density_ridges_gradient\u0026quot;, calc_ecdf = TRUE, quantiles = 4, quantile_lines = TRUE, bandwidth = 2 ) + scale_fill_viridis(discrete = TRUE, name = \u0026quot;Point Quartile\u0026quot;, alpha = 0.5) + my_plot_theme() + scale_x_continuous(breaks = pretty_breaks(n = 7)) + labs(x = \u0026#39;Total Points Per Simulated Season\u0026#39;, y = NULL, title = \u0026quot;The best kicker is not much better than the worst kicker\u0026quot;, subtitle = \u0026#39;Drafing any kicker is fine\u0026#39; ) + theme(legend.position = \u0026#39;none\u0026#39;) Wait! We went all this way for you to tell me that the status quo is probably right? Yes, I did. But we still haven‚Äôt quantified how much better or worse drafting the best or worst kicker is in terms of fantasy points. A simple way is to count the number of seasons where Justin Tucker (the best kicker) scored more points than Andrew Franks (the worst kicker).\njt_pts \u0026lt;- pt_simulation %\u0026gt;% filter(name == \u0026#39;Justin Tucker\u0026#39;) %\u0026gt;% pull(season_pts) af_pts \u0026lt;- pt_simulation %\u0026gt;% filter(name == \u0026#39;Andrew Franks\u0026#39;) %\u0026gt;% pull(season_pts) pct_greater \u0026lt;- sum(jt_pts \u0026gt; af_pts) / n_seasons print(str_glue(\u0026#39;PCT greater: {pct_greater * 100}%\u0026#39;)) ## PCT greater: 77.5% Turns out that approximately 77 of every 100 seasons Justin Tucker outscores Andrew Franks. Let‚Äôs go one step further and quantify the actual difference.\nggplot(data.frame(pt_diff = jt_pts - af_pts), aes(pt_diff)) + geom_histogram(fill = \u0026#39;gray\u0026#39;, color = \u0026#39;black\u0026#39;, bins = 10) + scale_x_continuous(breaks = pretty_breaks(n = 15)) + labs(x = \u0026#39;Point Difference over Entire Season\u0026#39;) + theme_minimal() + geom_vline(xintercept = quantile(jt_pts - af_pts, .05), lty = 2) + geom_vline(xintercept = quantile(jt_pts - af_pts, .5), lty = 2, color = \u0026#39;red\u0026#39;, size = 2) + geom_vline(xintercept = quantile(jt_pts - af_pts, .95), lty = 2) + my_plot_theme() + labs(x = \u0026#39;Point Difference\u0026#39;, y = \u0026#39;Count\u0026#39;, title = \u0026#39;The best kicker should score about 10 more points per season compared to the worst\u0026#39;, subtitle = \u0026#39;Estimate based on 27 FG attempts per season with each FG worth 3.5 points\u0026#39; ) If we spread this estimate out across 16 regular-season games, it comes out to less than a single point per game.\n Conclusion Needless to say, pick your kicker last in Fantasy Football! All kickers in modern-day NFL are really good, so save those late-round picks for positions other than a kicker. Cheers!\n ","date":1566871994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566871994,"objectID":"bba60f55dd5430d350a9195332bc051c","permalink":"http://example.org/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers/","publishdate":"2019-08-26T21:13:14-05:00","relpermalink":"/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers/","section":"post","summary":"We'll use 50 years of NFL kicking data to inform the least -- or most -- important decision of your fantasy season: Drafting a kicker.","tags":["R","Emperical Bayes","Fantasy Football"],"title":"Choosing a Fantasy Football Kicker with Emperical Bayes Estimation","type":"post"},{"authors":null,"categories":["R","Hypothesis Testing","Forecasting","Webscraping","Names"],"content":" Overview Phil Karlton, a famous Netscape Developer (i.e., OG Google Chrome) once said, ‚ÄòThere are two hard things in computer science: cache invalidation and naming things‚Äô. I haven‚Äôt done much cache invalidation, but I have named a few things ‚Äì and naming a person is by far the hardest of them all! Indeed, having waited two days after my own son‚Äôs birth to finally settle on a name, I wondered to what extent other new parents encountered the same struggles. Are there shortcuts or heuristics that others use to simplify the decision-making process, specifically cues from their immediate surroundings to help guide their choices when choosing a baby name? This question motivated me to look into the nuances of naming conventions over the past century in America.\nAccordingly, in this post, we‚Äôll investigate the influence of one‚Äôs state of residence on the frequency with which certain names occur. We‚Äôll also explore possible reasons for why some states have more variety in their names than others. Finally, we‚Äôll finish up in my home state of Oregon to identify the trendiest names over the past 20 years and predict whether those names will remain trendy in the future. From a technical standpoint, we‚Äôll cover some central, bread-and-butter topics in data science, including trend detection, false discovery rates, web scraping, time-series forecasting, and geovisualization. Let‚Äôs get started!\n People Born in Oregon are Named after Trees We‚Äôll begin by downloading more than 110 years of US name data from üéÑ the codeforest github repo üéÑ. Our dataset is published yearly by the Social Security Administration, and it contains a count of all names that occur more than five times by year within each US state. Let‚Äôs get started by loading relevant libraries and pulling our data into R.\n# Core Packages library(tidyverse) library(purrr) library(skimr) library(janitor) library(drlib) library(broom) library(openintro) library(sweep) library(tidytext) library(usdata) # Date Manipulation library(lubridate) # Forecasting Packages library(forecast) library(timetk) # Webscraping library(rvest) # Visualization Packages library(ggplot2) library(ggmap) library(ggthemes) library(ggrepel) library(artyfarty) library(gt) library(maps) # Trend Detection Packages library(trend) # # Set visualization themes theme_set(theme_bw()) data_repo \u0026lt;- \u0026quot;https://raw.githubusercontent.com/thecodeforest/codeforest_datasets/main/state_of_names\u0026quot; # Create unique path for each state data_paths \u0026lt;- paste0(file.path(data_repo, datasets::state.abb), \u0026#39;.TXT\u0026#39;) # Append data from each state into single table names_raw_df \u0026lt;- data_paths %\u0026gt;% purrr::map(read_csv, col_names = FALSE) %\u0026gt;% reduce(rbind) Let‚Äôs have a quick peek at our data.\nhtml { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #uylsmmxvft .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #uylsmmxvft .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uylsmmxvft .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #uylsmmxvft .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #uylsmmxvft .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uylsmmxvft .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uylsmmxvft .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #uylsmmxvft .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #uylsmmxvft .gt_column_spanner_outer:first-child { padding-left: 0; } #uylsmmxvft .gt_column_spanner_outer:last-child { padding-right: 0; } #uylsmmxvft .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #uylsmmxvft .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #uylsmmxvft .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #uylsmmxvft .gt_from_md  :first-child { margin-top: 0; } #uylsmmxvft .gt_from_md  :last-child { margin-bottom: 0; } #uylsmmxvft .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #uylsmmxvft .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #uylsmmxvft .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uylsmmxvft .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #uylsmmxvft .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uylsmmxvft .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #uylsmmxvft .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #uylsmmxvft .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uylsmmxvft .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uylsmmxvft .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #uylsmmxvft .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uylsmmxvft .gt_sourcenote { font-size: 90%; padding: 4px; } #uylsmmxvft .gt_left { text-align: left; } #uylsmmxvft .gt_center { text-align: center; } #uylsmmxvft .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #uylsmmxvft .gt_font_normal { font-weight: normal; } #uylsmmxvft .gt_font_bold { font-weight: bold; } #uylsmmxvft .gt_font_italic { font-style: italic; } #uylsmmxvft .gt_super { font-size: 65%; } #uylsmmxvft .gt_footnote_marks { font-style: italic; font-size: 65%; }     Sample Data       X1 X2 X3 X4 X5    AL FALSE 1910 Mary 875   AL FALSE 1910 Annie 482   AL FALSE 1910 Willie 257   AL FALSE 1910 Mattie 232   AL FALSE 1910 Ruby 204   \nA little cleaning is in order. We‚Äôll name our fields, create a gender feature, and remove spurious names.\nnames(names_raw_df) \u0026lt;- c(\u0026quot;state\u0026quot;, \u0026quot;gender\u0026quot;, \u0026quot;year\u0026quot;, \u0026quot;name\u0026quot;, \u0026quot;frequency\u0026quot;) names_processed_df \u0026lt;- names_raw_df %\u0026gt;% mutate(gender = ifelse(is.na(gender), \u0026quot;Male\u0026quot;, \u0026quot;Female\u0026quot;)) %\u0026gt;% filter(!str_to_lower(name) %in% c(\u0026quot;unknown\u0026quot;, \u0026quot;noname\u0026quot;, \u0026quot;female\u0026quot;, \u0026quot;male\u0026quot;)) Let‚Äôs do some quick exploratory data analysis before addressing our original questions. Any time we are working with categorical variables (e.g., name, state, gender, etc.), I like to start by counting and visualizing their distributions. Below we‚Äôll create two separate data views for quality assurance purposes: (1) The most popular names since 1910, and (2) the total number of births (based on name counts) across time. The goal is to ensure the data aligns with our expectations (e.g., the most popular boy names over the past 100 years are not ‚ÄòFlorp‚Äô or ‚ÄòSpaghetti Joe‚Äô).\n# calculate the top 20 most popular names name_popularity \u0026lt;- names_processed_df %\u0026gt;% group_by(name, gender) %\u0026gt;% summarise(total = sum(frequency)) %\u0026gt;% group_by(gender) %\u0026gt;% top_n(20, total) %\u0026gt;% ungroup() %\u0026gt;% mutate(name = reorder_within(name, total, gender)) name_popularity %\u0026gt;% ggplot(aes(name, total, fill = gender)) + geom_col(alpha = 0.8, color = \u0026#39;black\u0026#39;) + coord_flip() + scale_x_reordered() + facet_wrap(~ gender, scales = \u0026#39;free\u0026#39;, ncol = 1) + scale_y_continuous(labels = scales::comma_format()) + scale_fill_manual(values = pal(\u0026quot;monokai\u0026quot;)) + my_plot_theme() + labs(x = NULL, y = \u0026#39;Total Names\u0026#39;, title = \u0026#39;US Top 20 names by gender since 1910\u0026#39; ) + theme(legend.position = \u0026quot;none\u0026quot;) These frequencies seem reasonable! Next, let‚Äôs examine how the total count of names has changed across time between 1910 and 2018 to determine if there are any missing or incomplete years.\nnames_processed_df %\u0026gt;% mutate(year = as.Date(paste(as.character(year), \u0026#39;01\u0026#39;, \u0026#39;01\u0026#39;, sep = \u0026#39;-\u0026#39;))) %\u0026gt;% group_by(year) %\u0026gt;% summarise(total = sum(frequency)) %\u0026gt;% ggplot(aes(year, total)) + geom_line(size = 2) + scale_y_continuous(labels = scales::comma_format()) + scale_x_date(date_breaks = \u0026quot;10 year\u0026quot;, date_labels = \u0026#39;%Y\u0026#39;) + my_plot_theme() + labs(x = \u0026#39;Year\u0026#39;, y = \u0026#39;Total Births\u0026#39;, title = \u0026#39;Total US Births by Year\u0026#39;, subtitle = \u0026#39;Total based only on names appearing \u0026gt; 5 times per state per year\u0026#39; )  The overall trend here also checks out as well, with the baby-boom occurring between 1946 to 1964 and a steady decline in births rates since the early 1990s.\nNow that we‚Äôve done some quick validation, let‚Äôs tackle our first question: Which names over-index within each state? To address this question, we‚Äôll compare the proportion of names occupied by a single name within a state relative to how frequently the name occurs across all 50 states. We‚Äôll also focus only on the past 10 years to capture recent name trends. Note that the technique implemented below was adapted from the excellent Tidy Tuesday Screen cast series found here.\n# count of names by state since 2008 name_state_counts \u0026lt;- names_processed_df %\u0026gt;% filter(year \u0026gt;= 2008) %\u0026gt;% group_by(name, state) %\u0026gt;% summarise(n = sum(frequency)) %\u0026gt;% ungroup() %\u0026gt;% complete(state, name, fill = list(n = 0)) # total births in US total_names \u0026lt;- sum(name_state_counts$n) # name count across all states name_counts \u0026lt;- name_state_counts %\u0026gt;% group_by(name) %\u0026gt;% summarise(name_total = sum(n)) # birth count by state state_counts \u0026lt;- name_state_counts %\u0026gt;% group_by(state) %\u0026gt;% summarise(state_total = sum(n)) Next, we‚Äôll create a ratio that summarizes how much more likely a name is to appear within a state relative to the US as a whole. We‚Äôll put some filters on as well to prevent rare names from overwhelming our analysis.\n# Minimum occurrences within a state cnt_in_state \u0026lt;- 100 # Minimum occurrences across all US cnt_in_US \u0026lt;- 200 # Calculate name ratio within state relative to within US all_name_counts \u0026lt;- name_state_counts %\u0026gt;% inner_join(name_counts) %\u0026gt;% inner_join(state_counts) %\u0026gt;% mutate(state_name_full = abbr2state(state)) %\u0026gt;% filter( n \u0026gt;= cnt_in_state, name_total \u0026gt;= cnt_in_US ) %\u0026gt;% mutate( percent_of_state = n / state_total, percent_of_names = name_total / total_names ) %\u0026gt;% mutate(overrepresented_ratio = percent_of_state / percent_of_names) %\u0026gt;% arrange(desc(overrepresented_ratio)) Below we‚Äôll plot the top 10 names by state from a geographically representative sample.\ntop_n_names \u0026lt;- 10 all_name_counts %\u0026gt;% group_by(state_name_full) %\u0026gt;% top_n(top_n_names, overrepresented_ratio) %\u0026gt;% ungroup() %\u0026gt;% filter(state_name_full %in% c( \u0026quot;Alabama\u0026quot;, \u0026quot;New Jersey\u0026quot;, \u0026quot;Arkansas\u0026quot;, \u0026quot;Oregon\u0026quot;, \u0026quot;Colorado\u0026quot;, \u0026quot;New Mexico\u0026quot;, \u0026quot;West Virginia\u0026quot;, \u0026quot;Hawaii\u0026quot; )) %\u0026gt;% mutate(name = reorder_within(name, overrepresented_ratio, state_name_full)) %\u0026gt;% ggplot(aes(name, overrepresented_ratio, fill = state_name_full)) + geom_col(color = \u0026quot;black\u0026quot;, alpha = 0.8) + coord_flip() + scale_x_reordered() + facet_wrap(~state_name_full, scales = \u0026quot;free\u0026quot;, ncol = 2) + scale_fill_manual(values = pal(\u0026quot;monokai\u0026quot;)) + my_plot_theme() + labs( x = NULL, y = \u0026quot;Overrepresentation Ratio\u0026quot; ) + theme(legend.position = \u0026quot;none\u0026quot;) There‚Äôs a lot to unpack here, but that fact that ‚ÄòCrimson‚Äô over-indexes in Alabama tells me we‚Äôre on to something. Let‚Äôs briefly summarise our findings for each state separately:\nAlabama - Roll Tide.\nArkansas - Future country music stars.\nColorado - Mountain towns (Aspen, Breckenridge) and famous skiers (Bode Miller)\nHawaii - Native Hawaiian names. Note the large magnitude of this ratio, indicating that these names are found exclusively in Hawaii.\nNew Jersey - Large Jewish population.\nNew Mexico - Large Hispanic population.\nOregon - Nature.\nWest Virginia - Preferred gun brands (Remington, Kolton).\nIt‚Äôs interesting to see how cultures unique to each state come through in people‚Äôs names. Are you a big fan of the University of Alabama‚Äôs Football team? Name your kid Crimson. Are you a firearm‚Äôs enthusiast? Remington has a nice ring to it. Do you enjoy long hikes in the woods? Forrest is a great name. This finding indicates that (unsurprisingly) geography plays a significant role in determining naming conventions within a state, and that people leverage the cultural norms from within their state when deciding on a name.\n Diversity of Names In the previous section, we established that one‚Äôs state of birth influences naming conventions (still trying to figure out if this is a good or bad thing‚Ä¶). Let‚Äôs continue with this theme and initially consider how ‚ÄòName Diversity‚Äô varies between states, which we‚Äôll define by comparing the proportion of all names represented by the top 100 most popular names in each state. For example, the figure below shows the cumulative percentage of all names captured by the top 5 names in Oregon relative to Vermont.\nnames_diversity_sample \u0026lt;- name_state_counts %\u0026gt;% filter(state %in% c(\u0026#39;OR\u0026#39;, \u0026#39;VT\u0026#39;)) %\u0026gt;% group_by(state) %\u0026gt;% arrange(desc(n)) %\u0026gt;% mutate(total = sum(n), cum_sum = cumsum(n), cum_pct = round(cum_sum / total, 2), name_rnk = 1:n() ) %\u0026gt;% slice(1:5) %\u0026gt;% ungroup() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xcqwiyijsn .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xcqwiyijsn .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xcqwiyijsn .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xcqwiyijsn .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #xcqwiyijsn .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xcqwiyijsn .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xcqwiyijsn .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xcqwiyijsn .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xcqwiyijsn .gt_column_spanner_outer:first-child { padding-left: 0; } #xcqwiyijsn .gt_column_spanner_outer:last-child { padding-right: 0; } #xcqwiyijsn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #xcqwiyijsn .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #xcqwiyijsn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xcqwiyijsn .gt_from_md  :first-child { margin-top: 0; } #xcqwiyijsn .gt_from_md  :last-child { margin-bottom: 0; } #xcqwiyijsn .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xcqwiyijsn .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #xcqwiyijsn .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xcqwiyijsn .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #xcqwiyijsn .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xcqwiyijsn .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xcqwiyijsn .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xcqwiyijsn .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xcqwiyijsn .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xcqwiyijsn .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #xcqwiyijsn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xcqwiyijsn .gt_sourcenote { font-size: 90%; padding: 4px; } #xcqwiyijsn .gt_left { text-align: left; } #xcqwiyijsn .gt_center { text-align: center; } #xcqwiyijsn .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xcqwiyijsn .gt_font_normal { font-weight: normal; } #xcqwiyijsn .gt_font_bold { font-weight: bold; } #xcqwiyijsn .gt_font_italic { font-style: italic; } #xcqwiyijsn .gt_super { font-size: 65%; } #xcqwiyijsn .gt_footnote_marks { font-style: italic; font-size: 65%; }     state name n total cum_sum cum_pct name_rnk    OR Emma 2549 366114 2549 0.01 1   OR Olivia 2452 366114 5001 0.01 2   OR Sophia 2210 366114 7211 0.02 3   OR Liam 2155 366114 9366 0.03 4   OR Benjamin 1999 366114 11365 0.03 5   VT Emma 378 32623 378 0.01 1   VT Liam 374 32623 752 0.02 2   VT Owen 372 32623 1124 0.03 3   VT Mason 357 32623 1481 0.05 4   VT Olivia 354 32623 1835 0.06 5    When comparing the cum_pct between states, we see that approximately 3% of all names are represented by the top 10 in Oregon while 6% of all names are represented in Vermont. This means that fewer names occupy a greater proportion of names in Vermont relative to Oregon. Therefore, Vermont has less Name Diversity than Oregon. What does this relationship look like when expanding our search to the top 100 names across all lower 48 states?\ntop_n_names \u0026lt;- 100 # Create Name Diversity metric names_diversity_lower_48 \u0026lt;- name_state_counts %\u0026gt;% group_by(state) %\u0026gt;% arrange(state, desc(n)) %\u0026gt;% mutate( name_index = row_number(), cum_sum = cumsum(n), cum_pct = cum_sum / sum(n) ) %\u0026gt;% ungroup() %\u0026gt;% filter(name_index == top_n_names) %\u0026gt;% select(state, cum_pct) %\u0026gt;% mutate(state_name_full = abbr2state(state)) # Join % of names accounted for by top 100 to map data us_map \u0026lt;- map_data(\u0026quot;state\u0026quot;) %\u0026gt;% as_tibble() %\u0026gt;% mutate(state_name_full = str_to_title(region)) %\u0026gt;% inner_join(names_diversity_lower_48, by = \u0026quot;state_name_full\u0026quot;) # Plot relationship by state us_map %\u0026gt;% ggplot(aes(long, lat)) + geom_polygon(aes(group = group, fill = cum_pct), color = \u0026quot;white\u0026quot;) + theme_map() + coord_map() + my_plot_theme() + scale_fill_viridis_c(labels = scales::percent) + labs(fill = \u0026quot;Percent of names in Top 100\u0026quot;, title = \u0026#39;Name Diversity by State\u0026#39;, subtitle = \u0026#39;Higher percentages indicate less diversity in names\u0026#39; ) + theme(legend.text=element_text(size=14), legend.title = element_blank(), legend.position = \u0026#39;top\u0026#39;, axis.text.y = element_blank(), axis.title.y = element_blank(), axis.text.x = element_blank(), axis.title.x = element_blank() ) West Coast and Southeastern states tend to have greater name diversity (i.e., a lower % of names are represented in the top 100) while the North East has less diversity. This begs the question: What type of diversity correlates with our Name Diversity index? A recent study ranked states along six dimensions of diversity, such as Cultural, Economic, Household, Religious and Political. Let‚Äôs bring these rankings in and join them with our newly created diversity index.\nurl \u0026lt;- \u0026quot;https://wallethub.com/edu/most-least-diverse-states-in-america/38262/\u0026quot; diversity_rank \u0026lt;- read_html(url) %\u0026gt;% html_nodes(\u0026quot;table\u0026quot;) %\u0026gt;% .[1] %\u0026gt;% html_table(fill = TRUE) %\u0026gt;% data.frame() %\u0026gt;% clean_names() names(diversity_rank) \u0026lt;- purrr::map_chr(names(diversity_rank), function(x) str_replace(x, \u0026quot;x_\u0026quot;, \u0026quot;\u0026quot;) ) diversity_tidy \u0026lt;- diversity_rank %\u0026gt;% select(state, ends_with(\u0026quot;_rank\u0026quot;)) %\u0026gt;% gather(diversity_metric, rank, -state) %\u0026gt;% mutate(diversity_metric = str_to_title(str_replace( str_replace(diversity_metric,\u0026quot;_rank\u0026quot;,\u0026quot;\u0026quot;) ,\u0026quot;_\u0026quot;, \u0026quot; \u0026quot; ) ) ) %\u0026gt;% inner_join(names_diversity_lower_48, by = c(\u0026quot;state\u0026quot; = \u0026quot;state_name_full\u0026quot;)) We‚Äôll plot the relationship between Name Diversity and the six aforementioned dimensions.\ndiversity_tidy %\u0026gt;% ggplot(aes(rank, cum_pct, label = state)) + geom_point() + stat_smooth() + facet_wrap(~diversity_metric, scales = \u0026quot;free\u0026quot;, ncol = 2) + scale_y_percent() + my_plot_theme() + labs( x = \u0026quot;State Rank (1 = Most Diverse, 50 = Least Diverse)\u0026quot;, y = \u0026quot;Percent of names in Top 100\u0026quot; ) There might be a positive relationship between Cultural and Household diversity relative to Name Diversity, such that states with lower Cultural Diversity also have lower Name Diversity. Some formal hypothesis testing can be useful when we don‚Äôt have a strong prior hypothesis. However, we‚Äôll need to be careful when considering the strength of evidence, given that we are testing six separate hypotheses. To do so, we‚Äôll adjust each p-value based on the FDR or False Discovery Rate. Additionally, we‚Äôll use Spearman‚Äôs correlation coefficient in lieu of the more popular Pearson‚Äôs because we have no reason to believe that our relationships are linear. We can relax this assumption and simply state that the relationship is monotonically increasing/decreasing.\ncor_tidy \u0026lt;- diversity_tidy %\u0026gt;% select(-state.y, -state) %\u0026gt;% nest(-diversity_metric) %\u0026gt;% mutate( test = purrr::map(data, ~ cor.test(.x$rank, .x$cum_pct, method = \u0026quot;spearman\u0026quot;)), tidied = purrr::map(test, tidy) ) %\u0026gt;% unnest(tidied, .drop = TRUE) %\u0026gt;% clean_names() %\u0026gt;% mutate(p_val_adj = p.adjust(p_value, method = \u0026quot;fdr\u0026quot;)) %\u0026gt;% arrange(p_val_adj) %\u0026gt;% select(diversity_metric, estimate, p_value, p_val_adj) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qfomdglcac .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qfomdglcac .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qfomdglcac .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qfomdglcac .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #qfomdglcac .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qfomdglcac .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qfomdglcac .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qfomdglcac .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qfomdglcac .gt_column_spanner_outer:first-child { padding-left: 0; } #qfomdglcac .gt_column_spanner_outer:last-child { padding-right: 0; } #qfomdglcac .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #qfomdglcac .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #qfomdglcac .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qfomdglcac .gt_from_md  :first-child { margin-top: 0; } #qfomdglcac .gt_from_md  :last-child { margin-bottom: 0; } #qfomdglcac .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qfomdglcac .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #qfomdglcac .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qfomdglcac .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #qfomdglcac .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qfomdglcac .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qfomdglcac .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qfomdglcac .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qfomdglcac .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qfomdglcac .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #qfomdglcac .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qfomdglcac .gt_sourcenote { font-size: 90%; padding: 4px; } #qfomdglcac .gt_left { text-align: left; } #qfomdglcac .gt_center { text-align: center; } #qfomdglcac .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qfomdglcac .gt_font_normal { font-weight: normal; } #qfomdglcac .gt_font_bold { font-weight: bold; } #qfomdglcac .gt_font_italic { font-style: italic; } #qfomdglcac .gt_super { font-size: 65%; } #qfomdglcac .gt_footnote_marks { font-style: italic; font-size: 65%; }     metric est p_val p_val_adj    Cultural Diversity 0.41 0.00 0.02   Household Diversity 0.35 0.01 0.04   Religious Diversity 0.21 0.15 0.30   Political Diversity 0.18 0.22 0.33   Socioeconomic Diversity 0.11 0.43 0.51   Economic Diversity -0.02 0.89 0.89    After adjusting for multiple hypothesis tests, the only statistically significant relationships to emerge are Cultural and Household Diversity. This intuitively makes sense, as states with a greater blend of cultures will likely bring their own unique naming traditions. Let‚Äôs see how all of the states stack up against one another on the Cultural Diversity metric.\ndiversity_tidy %\u0026gt;% filter(diversity_metric == \u0026quot;Cultural Diversity\u0026quot;) %\u0026gt;% ggplot(aes(rank, cum_pct, label = state)) + geom_smooth(span = 3, alpha = 0.5) + geom_point() + geom_label_repel() + scale_y_percent() + my_plot_theme() + labs( x = \u0026quot;Cultural Diversity (1 = Most Diverse, 50 = Least Diverse)\u0026quot;, y = \u0026quot;Name Diversity (Lower = More diverse)\u0026quot;, title = \u0026#39;States with Higher Cultural Diversity have a Greater Variety of Names\u0026#39; ) We see that Cultural Diversity relates to the breadth of names represented in each state, a relationship that is particularly pronounced among states with lower Cultural Diversity. Thus, if you live in a state with low Cultural Diversity and give your child a popular name, there‚Äôs a good chance they‚Äôll be referred to as ‚ÄúOliver #2‚Äù, ‚ÄúEmma C‚Äù, or ‚ÄúOther James‚Äù during grade school.\n Trendy Names In this section, we‚Äôll focus on my current state of residence ‚Äì Oregon ‚Äì and explore which names have trended the most over the past two decades and where we expect the popularity of these names to go over the next decade. Let‚Äôs start with a little data cleaning.\n# only consider names that appear at least 300 times frequency_limit \u0026lt;- 300 start_year \u0026lt;- 2000 # arrange each name by year and count number of occurrences oregon_names \u0026lt;- names_processed_df %\u0026gt;% as_tibble() %\u0026gt;% filter( state == \u0026quot;OR\u0026quot;, year \u0026gt;= start_year ) %\u0026gt;% group_by(year, name) %\u0026gt;% summarise(frequency = sum(frequency)) %\u0026gt;% ungroup() %\u0026gt;% complete(year, name, fill = list(frequency = 0)) %\u0026gt;% group_by(name) %\u0026gt;% mutate(total_freq = sum(frequency)) %\u0026gt;% ungroup() %\u0026gt;% filter(total_freq \u0026gt;= frequency_limit) %\u0026gt;% select(-total_freq) %\u0026gt;% group_by(name) %\u0026gt;% arrange(name, year) Below we‚Äôre going to use a simple (yet powerful) approach for trend detection via the mk.test (Mann-Kendall Test) function, which determines if a series follows a monotonic trend. Below we‚Äôll apply this test to each name, order by the size of the resulting test statistic, and then select the top 25 largest test statistics. This will provide us with the ‚Äòtrendiest‚Äô names since 2000.\n# Identify trendiest names based on top 25 largest test statistics trendy_names \u0026lt;- oregon_names %\u0026gt;% nest(-name) %\u0026gt;% mutate( model = purrr::map(data, ~ mk.test(.$frequency)), tidied = purrr::map(model, tidy) ) %\u0026gt;% unnest(tidied, .drop = TRUE) %\u0026gt;% arrange(desc(statistic)) %\u0026gt;% clean_names() %\u0026gt;% select(name:p_value) %\u0026gt;% head(25) Let‚Äôs take a quick glance at some of the resulting names. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #dtgqjeqjju .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #dtgqjeqjju .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #dtgqjeqjju .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #dtgqjeqjju .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #dtgqjeqjju .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #dtgqjeqjju .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #dtgqjeqjju .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #dtgqjeqjju .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #dtgqjeqjju .gt_column_spanner_outer:first-child { padding-left: 0; } #dtgqjeqjju .gt_column_spanner_outer:last-child { padding-right: 0; } #dtgqjeqjju .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #dtgqjeqjju .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #dtgqjeqjju .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #dtgqjeqjju .gt_from_md  :first-child { margin-top: 0; } #dtgqjeqjju .gt_from_md  :last-child { margin-bottom: 0; } #dtgqjeqjju .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #dtgqjeqjju .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #dtgqjeqjju .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #dtgqjeqjju .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #dtgqjeqjju .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #dtgqjeqjju .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #dtgqjeqjju .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #dtgqjeqjju .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #dtgqjeqjju .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #dtgqjeqjju .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #dtgqjeqjju .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #dtgqjeqjju .gt_sourcenote { font-size: 90%; padding: 4px; } #dtgqjeqjju .gt_left { text-align: left; } #dtgqjeqjju .gt_center { text-align: center; } #dtgqjeqjju .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #dtgqjeqjju .gt_font_normal { font-weight: normal; } #dtgqjeqjju .gt_font_bold { font-weight: bold; } #dtgqjeqjju .gt_font_italic { font-style: italic; } #dtgqjeqjju .gt_super { font-size: 65%; } #dtgqjeqjju .gt_footnote_marks { font-style: italic; font-size: 65%; }     name statistic p_val    Oliver 5.78 7.60e-09   Hazel 5.53 3.12e-08   Luna 5.52 3.37e-08   Hudson 5.47 4.41e-08   Leo 5.46 4.64e-08   Nora 5.46 4.82e-08   Mila 5.45 5.03e-08   Harper 5.42 6.07e-08   Penelope 5.42 6.07e-08   Sawyer 5.39 6.88e-08   \nA quick cross-reference with some popular naming sites indicates that these names are popular both in Oregon as well as the remainder of the US. Let‚Äôs make some predictions (because you can‚Äôt have a blog post on data without trying to predict something!) for the next 10 years.\n# Set forecasting horizon time_horizon \u0026lt;- 10 # Create a separate forecast for each name based on 18 years of history name_forecast \u0026lt;- oregon_names %\u0026gt;% filter(name %in% trendy_names$name) %\u0026gt;% mutate(year = as.Date(\u0026quot;0001-01-1\u0026quot;) + lubridate::years(year - 1)) %\u0026gt;% nest(-name) %\u0026gt;% mutate( ts = purrr::map(data, tk_ts, start = start_year, freq = 1), model = purrr::map(ts, ets), fcast = purrr::map(model, forecast, level=80, h = time_horizon) ) # Extract forecasting data unnest_fcast \u0026lt;- function(name_forecast, dt_field=\u0026#39;year\u0026#39;){ max_year \u0026lt;- year(max(name_forecast$data[[1]] %\u0026gt;% pull({{dt_field}}))) index = 1 fcast_tibble \u0026lt;- tibble() for(fcast in name_forecast$fcast){ name \u0026lt;- name_forecast$name[index] frequency \u0026lt;- as.vector(fcast$mean) lo_80 \u0026lt;- as.vector(fcast$lower) hi_80 \u0026lt;- as.vector(fcast$upper) tmp_fcast_tibble \u0026lt;- tibble(key = \u0026#39;fcast\u0026#39;, name = name, year = seq(max_year + 1, length.out = length(hi_80) ), frequency = frequency, lo_80 = lo_80, hi_80 = hi_80) fcast_tibble \u0026lt;- bind_rows(fcast_tibble, tmp_fcast_tibble) index \u0026lt;- index + 1 } fcast_tibble } Let‚Äôs visualize both the historical time series as well as our 10-year ahead forecast.\nnames_plot_df \u0026lt;- bind_rows(oregon_names %\u0026gt;% filter(name %in% trendy_names$name) %\u0026gt;% mutate(key = \u0026#39;actual\u0026#39;), unnest_fcast(name_forecast) ) names_plot_df %\u0026gt;% mutate(lo_80 = ifelse(lo_80 \u0026lt; 0, 0, lo_80)) %\u0026gt;% ggplot(aes(year, frequency, color = key)) + geom_line() + geom_ribbon(aes(ymin = lo_80, ymax = hi_80), alpha = .5) + facet_wrap(~name, scales = \u0026quot;free_y\u0026quot;) + ylim(0, max(names_plot_df$hi_80)) + scale_color_manual(values = pal(\u0026quot;monokai\u0026quot;)[c(1, 3)]) + my_plot_theme() + labs( x = \u0026quot;Year\u0026quot;, y = \u0026quot;Total Names\u0026quot;, title = \u0026quot;Trendiest Name in Oregon over the Past 20 Years\u0026quot;, subtitle = \u0026quot;Forecast for 10 Year Period. Shaded region represents 80% prediction interval.\u0026quot; ) + theme(legend.position = \u0026#39;none\u0026#39;) There‚Äôs about to be a lot more Luna‚Äôs, Mila‚Äôs, Oliver‚Äôs, Asher‚Äôs and Jameson‚Äôs in Oregon over the next decade, whereas the popularity of Harper and Penelope are either flat or heading downward. This could be helpful depending on if you wanted your child to be cool and trendy from day-1 üòÑ. However, the intervals on the majority of these forecasts are fairly wide, indicating that naming trends are not an easy thing to predict!\n Parting Thoughts While this post only scratches the surface in terms of understanding how names come-to-be in America, it reveals the extent to which parents rely on cues from their surroundings and cognitive shortcuts when naming their children. Whether it‚Äôs a favorite football team, a family name that‚Äôs been passed down through generations, a ski town with great powder, or that cool tree in the backyard, our immediate environments play a central role in the naming process. It also highlights the pivotal role that cultural diversity plays in determining the breadth of names by geographical location, as well as how unpredictable naming trends can be into the near future.\nHopefully you enjoyed the post and, if faced with naming a child any time soon, can leverage some of the techniques outlined here to come up with an awesome name!\n Appendix my_plot_theme = function(){ font_family = \u0026quot;Helvetica\u0026quot; font_face = \u0026quot;bold\u0026quot; return(theme( axis.text.x = element_text(size = 12, face = font_face, family = font_family), axis.text.y = element_text(size = 12, face = font_face, family = font_family), axis.title.x = element_text(size = 16, face = font_face, family = font_family), axis.title.y = element_text(size = 16, face = font_face, family = font_family), strip.text.y = element_text(size = 16, face = font_face, family = font_family), strip.text.x = element_text(size = 16, face = font_face, family = font_family), plot.title = element_text(size = 20, face = font_face, family = font_family), plot.subtitle = element_text(size = 16, family = font_family), plot.caption = element_text(size = 11, face = \u0026quot;italic\u0026quot;, hjust = 0), legend.position = \u0026quot;top\u0026quot;, legend.text = element_text(size = 8, face = font_face, family = font_family), legend.key = element_rect(size = 2), legend.key.size = unit(2, \u0026#39;lines\u0026#39;), legend.title=element_blank() )) }  ","date":1560391994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560391994,"objectID":"c1981e0f17cf5293feefe47b73c9801b","permalink":"http://example.org/post/2019-06-12-state-of-names/state_of_names/","publishdate":"2019-06-12T21:13:14-05:00","relpermalink":"/post/2019-06-12-state-of-names/state_of_names/","section":"post","summary":"In this post, we'll leverage 110 years of historical data -- and everything from time-series forecasting to hypothesis testing -- to understand how one's state of birth influences their name","tags":["R","Hypothesis Testing","Forecasting","Webscraping","Names"],"title":"The State of Names in America","type":"post"},{"authors":["Mark LeBoeuf"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"http://example.org/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://example.org/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"http://example.org/terms/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"\u0026hellip;","tags":null,"title":"","type":"page"},{"authors":null,"categories":["R","Python","Reticulate","Traveling Salesman Problem","Route Optimization"],"content":"    Overview The premise of a Pub Crawl is quite simple: visit several bars in an afternoon or evening without a clear plan of where you‚Äôll go next. While this sort of spontaneous, unstructured approach may work for some people, I‚Äôve always been a fan of having a plan ‚Äì in this case, an optimal plan. If we want to maximize the number of places visited (and beers tasted) in a finite period of time, then there is simply no room for shoddy planning. Accordingly, this post provides a framework for designing the optimal Portland Pub Crawl by working through the following steps:\nüç∫ Web Scrape the top 100 Portland bars from here\nüç∫ Geocode each bar‚Äôs location\nüç∫ Find the optimal route between a subsample of the bars, because visiting 100 in a day would make the following day very bad\nüç∫ Determine a walking path between the bars\nüç∫ Create a map of the walking path, which can be use as a field guide to impress your friends once the pub crawl is under way\nüç∫ Promptly spill beer on the map at the 2nd bar, rendering it unreadable, followed by a game of darts and some popcorn\nIf that sounds like a plan, let‚Äôs get started!\n Defining the Top 100 Bars First, let‚Äôs identify the stops during our tour of Portland‚Äôs pub scene. In the section below, we‚Äôll load up the R libraries, identify which version of Python we‚Äôd like to use, and then do some web scraping. Note that all of the python modules and R-scripts are contained in the same directory for simplicity.\n# Core pacakges library(tidyverse) # Mapping library(leaflet) library(widgetframe) library(leaflet.extras) # Calling python functions from R library(reticulate) # route optimization library(tspmeta) # Making nice tables library(gt) The geocode_best_portland_bars function below is responsible for collecting the information.\n# best_bars.py import os import urllib from bs4 import BeautifulSoup import re from typing import List import googlemaps from tqdm import tqdm import pandas as pd from dotenv import load_dotenv API_KEY = os.getenv(\u0026#39;GOOGLE_API_KEY\u0026#39;) def find_best_bars() -\u0026gt; str: base_url = \u0026quot;http://www.oregonlive.com/dining/index.ssf/2014/10/portlands_100_best_bars_bar_ta.html\u0026quot; page = urllib.request.urlopen(base_url) soup = BeautifulSoup(page, \u0026quot;html.parser\u0026quot;) bar_descriptors = soup.find_all(\u0026quot;div\u0026quot;, class_=\u0026quot;entry-content\u0026quot;) bar_descriptors = str(bar_descriptors).split(\u0026quot;\u0026lt;p\u0026gt;\u0026quot;)[0] best_bars_raw_lst = re.findall(r\u0026quot;\\\u0026lt;strong\u0026gt;(.*?)\u0026lt;/strong\u0026gt;\u0026quot;, bar_descriptors) return best_bars_raw_lst def clean_bar_names(raw_bar_lst: str) -\u0026gt; List[str]: # exclude emphasis tags best_bars = [re.sub(r\u0026quot;\u0026lt;em\u0026gt; (.*?)\u0026lt;/em\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, x) for x in raw_bar_lst] # exclude number included in bar name best_bars = [re.sub(r\u0026quot;No. \\d+ --\u0026quot;, \u0026quot;\u0026quot;, x).strip() for x in best_bars] # exclude headers in all caps best_bars = [x for x in best_bars if not x.isupper()] # exclude all lower case tags best_bars = [x for x in best_bars if not x.islower()] # exclude bold tags in html best_bars = [x.replace(\u0026quot;\u0026amp;amp;\u0026quot;, \u0026quot;\u0026amp;\u0026quot;) for x in best_bars] # exclude other emphasis tags best_bars = [re.sub(r\u0026quot;: \u0026lt;em\u0026gt;(.*?)\u0026lt;/em\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, x) for x in best_bars] # strip colons best_bars = [x.replace(\u0026quot;:\u0026quot;, \u0026quot;\u0026quot;) for x in best_bars] # exclude blanks best_bars = [x for x in best_bars if x] return best_bars def geocode_best_portland_bars() -\u0026gt; pd.DataFrame: best_bars_lst = find_best_bars() bar_names = clean_bar_names(raw_bar_lst=best_bars_lst) bar_names = [f\u0026quot;{x}, Portland, OR\u0026quot; for x in bar_names] gmaps = googlemaps.Client(key=API_KEY) geocoded_bars_lst = [] for name in tqdm(bar_names): geocode_result = gmaps.geocode(name) lat_lng = geocode_result[0].get(\u0026quot;geometry\u0026quot;).get(\u0026quot;location\u0026quot;) lat, lng = lat_lng.get(\u0026quot;lat\u0026quot;), lat_lng.get(\u0026quot;lng\u0026quot;) geocoded_bars_lst.append([name, lat, lng]) geocoded_bars_df = pd.DataFrame(geocoded_bars_lst) geocoded_bars_df.columns = [\u0026quot;name\u0026quot;, \u0026quot;lat\u0026quot;, \u0026quot;lng\u0026quot;] return geocoded_bars_df  Historically, this operation would require executing a python script, writing the results out (in an .txt or .csv file), and then reading the result back into R. However, with the advent of reticulate, we can execute a python function and pull the output back without ever having to leave the cozy confines of R (or R-studio in this case). Recall that the actual python module geocode_best_portland_bars.py is located in the same directory as our R-script. Below, we‚Äôll first ‚Äúsource‚Äù this function via source_python (which simply means to bring it into the R environment), then we‚Äôll execute it. Note that we aren‚Äôt passing in any arguments; this function is designed for a very specific purpose, which is to find the best watering holes in Portland.\n# specify which version of Python to use reticulate::use_python(\u0026#39;//anaconda/bin/python\u0026#39;, required = TRUE) # brings our function into the R Environment reticulate::source_python(\u0026#39;best_bars.py\u0026#39;) # executes and stores the output in our variable \u0026#39;best_bars\u0026#39; best_bars = geocode_best_portland_bars() best_bars %\u0026gt;% tail(10) %\u0026gt;% mutate(name = str_replace(name, \u0026#39;, Portland, OR\u0026#39;, \u0026#39;\u0026#39;)) %\u0026gt;% gt() %\u0026gt;% tab_header(title = gt::md(\u0026#39;**Data Sample of Best Portland Bars**\u0026#39;)) %\u0026gt;% cols_align( align = \u0026quot;center\u0026quot;, columns = everything()) %\u0026gt;% cols_width( everything() ~ px(155) ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #sszsgucrgp .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #sszsgucrgp .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #sszsgucrgp .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #sszsgucrgp .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #sszsgucrgp .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #sszsgucrgp .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #sszsgucrgp .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #sszsgucrgp .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #sszsgucrgp .gt_column_spanner_outer:first-child { padding-left: 0; } #sszsgucrgp .gt_column_spanner_outer:last-child { padding-right: 0; } #sszsgucrgp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #sszsgucrgp .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #sszsgucrgp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #sszsgucrgp .gt_from_md  :first-child { margin-top: 0; } #sszsgucrgp .gt_from_md  :last-child { margin-bottom: 0; } #sszsgucrgp .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #sszsgucrgp .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #sszsgucrgp .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #sszsgucrgp .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #sszsgucrgp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #sszsgucrgp .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #sszsgucrgp .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #sszsgucrgp .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #sszsgucrgp .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #sszsgucrgp .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #sszsgucrgp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #sszsgucrgp .gt_sourcenote { font-size: 90%; padding: 4px; } #sszsgucrgp .gt_left { text-align: left; } #sszsgucrgp .gt_center { text-align: center; } #sszsgucrgp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #sszsgucrgp .gt_font_normal { font-weight: normal; } #sszsgucrgp .gt_font_bold { font-weight: bold; } #sszsgucrgp .gt_font_italic { font-style: italic; } #sszsgucrgp .gt_super { font-size: 65%; } #sszsgucrgp .gt_footnote_marks { font-style: italic; font-size: 65%; }     Data Sample of Best Portland Bars       name lat lng    Swine 45.51827 -122.6818   Jackknife 45.52051 -122.6828   Stammtisch 45.52586 -122.6375   Cooper's Hall 45.51988 -122.6596   The Knock Back 45.55923 -122.6416   Multnomah Whiskey Library 45.52098 -122.6835   Trifecta Tavern \u0026amp; Bakery 45.51756 -122.6596   Angel Face 45.52321 -122.6371   Pepe Le Moko 45.52187 -122.6813   Expatriate 45.56240 -122.6346    We have successfully scraped the best bars and geocoded their locations. In the following section, we‚Äôll solve a classic routing optimization problem: The Traveling Salesman Problem (TSP). The goal is to find the most direct route between all of the bars we decide to visit during the pub crawl.\n Route Optimization The goal of any routing optimization problem is simple: minimize the total distance travelled between different nodes (locations) in space while ensuring that each node is visited once. There are many algorithms to solve this type of problem, but we‚Äôll leverage the 2-optimization or 2-opt method due to its simplicity. This algorithm finds the lowest cost route (i.e., the route with the shortest distance that ensures each node is visited once) by swapping the ‚Äòedges‚Äô (the path that connects two nodes) between different nodes. If a swap reduces the total length of our tour, then the swap is maintained; otherwise the swap is reversed and we try again with different edges. Note that the swap must ensure that a single route is always possible between all nodes. The algorithm stops when a tour is reached that cannot be improved with any more swaps (see here for a more in-depth explanation).\nBefore going any further, let‚Äôs plot out our locations to see what we‚Äôre working with. We‚Äôll also define our starting point, which is often referred to as the ‚Äòdepot‚Äô.\ndepot_lat = 45.525915 depot_lng = -122.684957 bar_map = leaflet(data = best_bars) %\u0026gt;% setView(lng = depot_lng + 0.05, lat = depot_lat, zoom = 13) %\u0026gt;% addProviderTiles(\u0026quot;CartoDB.Positron\u0026quot;) %\u0026gt;% addMarkers(lng=depot_lng, lat=depot_lat) %\u0026gt;% addCircleMarkers(lat=~lat, lng=~lng, color = \u0026quot;orange\u0026quot;, radius = 4, weight = 10, stroke = FALSE, opacity = 4, fillOpacity = 4 )   {\"x\":{\"url\":\"/post/2018-06-01-optimal-bar-crawl/optimal_bar_crawl_files/figure-html//widgets/widget_unnamed-chunk-7.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} Each orange dot is a bar, and the pointer indicates our starting position (the depot). Given that we are walking, let‚Äôs limit the potential distance to a maximum of three miles from our starting location. The function below calculates the total feet between two points defined by a latitude/longitude coordinate.\nearth_dist = function (lat1, lng1, lat2, lng2) { rad = pi/180 a1 = lat1 * rad a2 = lng1 * rad b1 = lat2 * rad b2 = lng2 * rad dlon = b2 - a2 dlat = b1 - a1 a = (sin(dlat/2))^2 + cos(a1) * cos(b1) * (sin(dlon/2))^2 c = 2 * atan2(sqrt(a), sqrt(1 - a)) R = 6378.145 d = R * c return(d* 3280.8) } Below we‚Äôll filter to all locations based on the maximum distance we‚Äôre willing to travel.\nfeet_in_mile = 5280 # maximum distance is 3 miles max_miles_away = 3 bar_locations_nearby = best_bars %\u0026gt;% mutate(distance_from_depot = earth_dist(depot_lat, depot_lng, lat, lng ) ) %\u0026gt;% filter(distance_from_depot \u0026lt;= feet_in_mile * max_miles_away) set.seed(1) # we\u0026#39;ll visit 24 bars n_bars = 24 # randomly select 24 bars to visit bar_locations_nearby = bar_locations_nearby %\u0026gt;% sample_n(n_bars) Next we‚Äôll transform the lat/long locations into a distance matrix. The distance matrix specifies the euclidean distance of each bar from every other bar.\n# now find optimal route coordinates = bar_locations_nearby %\u0026gt;% dplyr::select(lat, lng, name) %\u0026gt;% mutate(location_index = 2:(n() + 1)) %\u0026gt;% bind_rows(data.frame(lat = depot_lat, lng = depot_lng, address = \u0026#39;depot\u0026#39;, name = \u0026#39;depot\u0026#39;, location_index = 1 ) ) %\u0026gt;% arrange(location_index) coords_matrix = coordinates %\u0026gt;% dplyr::select(lat, lng) %\u0026gt;% as.matrix() dist_matrix = dist(coords_matrix) The two functions below tsp_instance and run_solver will do the heavy lifting and find the optimal route between bars.\n# create tsp instance tsp_ins = tspmeta::tsp_instance(coords_matrix,dist_matrix) # find optimal route based on 2-opt method opt_tour = as.integer(run_solver(tsp_ins, method=\u0026quot;2-opt\u0026quot;)) # sort to start at depot sorted_tour = c(opt_tour[which(opt_tour == 1):length(opt_tour)], opt_tour[1:(which(opt_tour == 1) - 1)] ) # join route order back to original data coordinates = coordinates %\u0026gt;% dplyr::inner_join(data.frame(location_index = sorted_tour, route_order = 1:length(sorted_tour) ) ) %\u0026gt;% dplyr::arrange(route_order) # reformat so each row has a starting lat/lng and ending lat/lng route_df = coordinates %\u0026gt;% dplyr::select(-address) %\u0026gt;% dplyr::rename(start_lat = lat, start_lng = lng ) %\u0026gt;% dplyr::mutate(end_lat = c(start_lat[2:n()], NA), end_lng = c(start_lng[2:n()], NA) ) %\u0026gt;% na.omit() Let‚Äôs take a peak at our data to see how everything turned out.\nhtml { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #dmhypywntg .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #dmhypywntg .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #dmhypywntg .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #dmhypywntg .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #dmhypywntg .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #dmhypywntg .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #dmhypywntg .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #dmhypywntg .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #dmhypywntg .gt_column_spanner_outer:first-child { padding-left: 0; } #dmhypywntg .gt_column_spanner_outer:last-child { padding-right: 0; } #dmhypywntg .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #dmhypywntg .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #dmhypywntg .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #dmhypywntg .gt_from_md  :first-child { margin-top: 0; } #dmhypywntg .gt_from_md  :last-child { margin-bottom: 0; } #dmhypywntg .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #dmhypywntg .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #dmhypywntg .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #dmhypywntg .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #dmhypywntg .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #dmhypywntg .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #dmhypywntg .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #dmhypywntg .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #dmhypywntg .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #dmhypywntg .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #dmhypywntg .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #dmhypywntg .gt_sourcenote { font-size: 90%; padding: 4px; } #dmhypywntg .gt_left { text-align: left; } #dmhypywntg .gt_center { text-align: center; } #dmhypywntg .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #dmhypywntg .gt_font_normal { font-weight: normal; } #dmhypywntg .gt_font_bold { font-weight: bold; } #dmhypywntg .gt_font_italic { font-style: italic; } #dmhypywntg .gt_super { font-size: 65%; } #dmhypywntg .gt_footnote_marks { font-style: italic; font-size: 65%; }     Route       name route_order start_lat start_lng end_lat end_lng    depot 1 45.52591 -122.6850 45.52627 -122.6784   Park Kitchen 2 45.52627 -122.6784 45.52531 -122.6783   Remedy Wine Bar 3 45.52531 -122.6783 45.52201 -122.6816   Clyde Common 4 45.52201 -122.6816 45.52246 -122.6852   Cassidy's 5 45.52246 -122.6852 45.52098 -122.6835   Multnomah Whiskey Library 6 45.52098 -122.6835 45.51487 -122.6824   The Rookery at Raven \u0026amp; Rose 7 45.51487 -122.6824 45.51400 -122.6753   Veritable Quandary 8 45.51400 -122.6753 45.51904 -122.6781   Departure 9 45.51904 -122.6781 45.52403 -122.6756   Ground Kontrol 10 45.52403 -122.6756 45.51900 -122.6641    Sweet! Almost there. The final step is to convert these points into an actual travel path.\n Creating a Walking Path Currently, the path between different nodes (i.e., bars) are straight lines. We‚Äôll be walking this tour, so a sidewalk travel path is required. We‚Äôll call on the Google Maps API one last time to convert each of the straight-line edges to actual walking paths via the convert_route_to_path.py module. This module consists of two functions: find_path and extract_polyline. find_path takes a starting lat/long, ending lat/long, and method of travel (walking in our case) and returns step-by-step lat/long coordinates along with distance and time estimates. extract_polyline is a helper function that will format each of the step-by-step coordinates into pandas DataFrame. The output will then be returned as an R DataFrame. We‚Äôll specify the python module below.\n# convert_route_to_path.py import os import pandas as pd import polyline import googlemaps from dotenv import load_dotenv load_dotenv() API_KEY = os.getenv(\u0026#39;GOOGLE_API_KEY\u0026#39;) def extract_polyline(coords: dict) -\u0026gt; pd.DataFrame: gmaps_polyline = coords[\u0026quot;overview_polyline\u0026quot;][\u0026quot;points\u0026quot;] polyline_df = pd.DataFrame(polyline.decode(gmaps_polyline)) polyline_df.columns = [\u0026quot;lat\u0026quot;, \u0026quot;lng\u0026quot;] polyline_df[\u0026quot;path_order\u0026quot;] = range(1, polyline_df.shape[0] + 1) return polyline_df def create_travel_path( route_df: pd.DataFrame, travel_mode: str = \u0026quot;walking\u0026quot; ) -\u0026gt; pd.DataFrame: gmaps = googlemaps.Client(key=API_KEY) out_route_df = pd.DataFrame() for row in route_df.itertuples(): coords = gmaps.directions( origin=[row.start_lat, row.start_lng], destination=[row.end_lat, row.end_lng], mode=travel_mode, ) coords_df = extract_polyline(coords=coords[0]) coords_df[\u0026quot;location_index\u0026quot;] = row.location_index coords_df[\u0026quot;travel_time\u0026quot;] = coords[0][\u0026quot;legs\u0026quot;][0][\u0026quot;duration\u0026quot;][\u0026quot;value\u0026quot;] coords_df[\u0026quot;miles\u0026quot;] = coords[0][\u0026quot;legs\u0026quot;][0][\u0026quot;distance\u0026quot;][\u0026quot;text\u0026quot;] coords_df[\u0026quot;route_order\u0026quot;] = row.route_order out_route_df = out_route_df.append(coords_df) out_route_df = out_route_df.reset_index(drop=True) return out_route_df Next, we‚Äôll read the convert_route_to_path.py module into R and pass in our route DataFrame, the Google Maps API key, and our preferred method of travel.\nreticulate::source_python(\u0026#39;convert_route_to_path.py\u0026#39;) path_df = create_travel_path(route_df) The data indicating the path between our depot and the first bar should look like this:\nhtml { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #pamufqigws .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 10px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #pamufqigws .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pamufqigws .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #pamufqigws .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #pamufqigws .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pamufqigws .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pamufqigws .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #pamufqigws .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #pamufqigws .gt_column_spanner_outer:first-child { padding-left: 0; } #pamufqigws .gt_column_spanner_outer:last-child { padding-right: 0; } #pamufqigws .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #pamufqigws .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #pamufqigws .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #pamufqigws .gt_from_md  :first-child { margin-top: 0; } #pamufqigws .gt_from_md  :last-child { margin-bottom: 0; } #pamufqigws .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #pamufqigws .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #pamufqigws .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pamufqigws .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #pamufqigws .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pamufqigws .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #pamufqigws .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #pamufqigws .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pamufqigws .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pamufqigws .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #pamufqigws .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pamufqigws .gt_sourcenote { font-size: 90%; padding: 4px; } #pamufqigws .gt_left { text-align: left; } #pamufqigws .gt_center { text-align: center; } #pamufqigws .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #pamufqigws .gt_font_normal { font-weight: normal; } #pamufqigws .gt_font_bold { font-weight: bold; } #pamufqigws .gt_font_italic { font-style: italic; } #pamufqigws .gt_super { font-size: 65%; } #pamufqigws .gt_footnote_marks { font-style: italic; font-size: 65%; }     Sample Travel Path       location_index route_order travel_time miles path_order lat lng    1 1 344 0.3 mi 1 45.52576 -122.6849   1 1 344 0.3 mi 2 45.52575 -122.6853   1 1 344 0.3 mi 3 45.52508 -122.6853   1 1 344 0.3 mi 4 45.52449 -122.6852   1 1 344 0.3 mi 5 45.52443 -122.6852   1 1 344 0.3 mi 6 45.52362 -122.6852   1 1 344 0.3 mi 7 45.52312 -122.6852   1 1 344 0.3 mi 8 45.52304 -122.6852   1 1 344 0.3 mi 9 45.52282 -122.6852   1 1 344 0.3 mi 10 45.52263 -122.6853   1 1 344 0.3 mi 11 45.52247 -122.6854   1 1 344 0.3 mi 12 45.52243 -122.6852    Note the small changes between each of the successive lat/long coordinates. This is the path we‚Äôll be walking to obtain our first frosty mug of beer. Before mapping our data, let‚Äôs get a general idea of total walking time and distance.\ntravel_time_in_hours = round(path_df %\u0026gt;% dplyr::select(location_index, travel_time) %\u0026gt;% dplyr::distinct() %\u0026gt;% dplyr::pull(travel_time) %\u0026gt;% sum() / 3600, 1) print(glue::glue(\u0026quot;Total Travel Time Is: \u0026quot;, travel_time_in_hours, \u0026quot; Hours\u0026quot; ) ) ## Total Travel Time Is: 6.6 Hours It looks like this walk will take around six hours, so we‚Äôll need to bring some comfy shoes. What about distance (we‚Äôll need some way to work off those calories)?\ntravel_distance_in_miles = round(path_df %\u0026gt;% dplyr::mutate(feet_numeric = case_when(stringr::str_detect(miles, \u0026#39;ft\u0026#39;) == TRUE ~ as.numeric(stringr::str_replace(miles, \u0026quot; ft\u0026quot;, \u0026quot;\u0026quot; ) ), stringr::str_detect(miles, \u0026quot; mi\u0026quot;) == TRUE ~ as.numeric(stringr::str_replace(miles, \u0026quot; mi\u0026quot;, \u0026quot;\u0026quot;) ) * feet_in_mile ) ) %\u0026gt;% dplyr::select(location_index, feet_numeric) %\u0026gt;% dplyr::distinct() %\u0026gt;% dplyr::pull(feet_numeric) %\u0026gt;% sum() / feet_in_mile, 1) print(glue::glue(\u0026quot;Total Travel Distance Is: \u0026quot;, travel_distance_in_miles, \u0026quot; Miles\u0026quot; ) ) ## Total Travel Distance Is: 19 Miles OK, this is more of a Pub Crawl half-marathon. That‚Äôs some serious distance to cover. Let‚Äôs bring it all together with some visualization.\n Mapping the Route The last step is to bring this analysis to life with everyone‚Äôs favorite visualization: MAPS! Indeed, we‚Äôll plot the walking path across downtown Portland so we can actually see the Pub Crawl route.\n# We\u0026#39;ll use this to identify the labels for each stop label_df = path_df %\u0026gt;% dplyr::filter(path_order == 1) # Bar crawl visualization final_route = leaflet(data = path_df) %\u0026gt;% setView(lng = depot_lng + 0.02, lat = depot_lat, zoom = 13) %\u0026gt;% addProviderTiles(\u0026quot;CartoDB.Positron\u0026quot;) %\u0026gt;% addPolylines(data = path_df %\u0026gt;% filter(route_order \u0026lt; 24), lng = ~lng, lat = ~lat, color = \u0026quot;orange\u0026quot;, opacity = 4 ) %\u0026gt;% addMarkers(lng = depot_lng, lat = depot_lat ) %\u0026gt;% addCircleMarkers(data = label_df, lng = ~lng, lat = ~lat, radius = 4, label = ~as.character(route_order), labelOptions = labelOptions(noHide = T, textOnly = T, direction = \u0026#39;top\u0026#39;, textsize = \u0026quot;14px\u0026quot;, offset=c(0,-5), size = 1 ) )  {\"x\":{\"url\":\"/post/2018-06-01-optimal-bar-crawl/optimal_bar_crawl_files/figure-html//widgets/widget_unnamed-chunk-21.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]}  ","date":1527905594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527905594,"objectID":"6907a5b6632403c0375da772713acdd7","permalink":"http://example.org/post/2018-06-01-optimal-bar-crawl/optimal_bar_crawl/","publishdate":"2018-06-01T21:13:14-05:00","relpermalink":"/post/2018-06-01-optimal-bar-crawl/optimal_bar_crawl/","section":"post","summary":"Portland, Oregon is home to some of the best watering holes in America. With so many places to quaff a West Coast Style IPA or glass of Pinot Noir, choosing which to visit (and in which order) can be a daunting task. To address this question, we'll leverage some classic optimization techniques to minimize the total distance travelled between the top bars in Portland for a truly \"optimal\" Pub Crawl.","tags":["R","Python","Reticulate","Traveling Salesman Problem","Route Optimization"],"title":"The Optimal Portland Pub Crawl","type":"post"},{"authors":null,"categories":["Fantasy Football","Python","R","Beta Distribution"],"content":" Overview Understanding a new concept is all about connecting it with something you already know. I don‚Äôt know much, but I do know Fantasy Football. Thus, when I come across new concepts, I often think to myself, ‚ÄúHow can I use this information to beat my friend Steve in Fantasy Football‚Äù? This very question was the impetus for putting these words and figures together in a post, which will introduce the idea of using the Beta Distribution to determine your weekly starter. I‚Äôll explain this approach in the context of my 2015 Fantasy Football season.\nAt the outset of that season, I drafted two quarterbacks: Joe Flacco and Marcus Mariota (it was a rough draft). Flacco had been in the NFL for a few years, while Mariota was still a rookie yet to play a game. I was also considering a separate rookie, Jameis Winston, who was available to pick up anytime during the season off the waiver wire. Throughout the season, I was faced with the following questions:\n Who do I make the starting QB? If one QB is performing poorly, when is the right time to make the switch (e.g., Flacco -\u0026gt; Mariota; Flacco -\u0026gt; Winston; Mariota -\u0026gt; Winston)?  This question is faced by NFL coaches and fantasy owners alike. If your QB has a few bad weeks, should you continue with them into the next week, replace them with the 2nd string QB, or sign a free agent to your team mid-season?\nBefore getting into the technical details, let‚Äôs first define what ‚ÄúSuccess‚Äù looks like for a Fantasy Football QB. Success can be defined in one word: Consistency. A QB that throws three touchdowns (TDs) every game for the first six games of the season (18 total) is better than a QB who throws five TDs for the first three games and then one TD during the next three games, despite having thrown the same number of TDs. Simply put - you want consistent, reliable performance every week. It doesn‚Äôt matter if you win by one point or 50 points ‚Äì a win is a win. Thus, I evaluate my QB‚Äôs performance on the following criteria: A ‚ÄúSuccessful‚Äù performance is defined as 3 or more touchdowns AND/OR 300 or more yards for a given week. Touchdowns and passing yards are the two primary sources of QB fantasy points, and a +3TD|300yard weekly statline should cement a QB amongst that week‚Äôs top performers. Failing to meet either of these criteria was defined as an ‚ÄúUnsuccessful‚Äù performance. Note that this label could also factor in interceptions, pass completions, and fumble, but we‚Äôll keep it simple and just focus on passing yards and passing touchdowns.\nHaving defined the evaluation criteria, the data generating process was modeled via the beta distribution. Recall that the beta distribution defines a distribution of probabilities, and we‚Äôre interested in the probability of our QB having a Successful week. There are several years of performance history on Joe Flacco, so we can provide a reasonably informed estimate of his weekly probabilty for achieving success (i.e., our prior). In contrast, there is no NFL game history on Mariota or Winston, so we‚Äôll assign each a uniform or uninformative prior. Our estimate of the Success parameter for Winston and Mariota will change rapidly as we acquire in-season data because our posterior is determined entirely from the data. We could create a more informed-‚Äìand stronger-‚Äìprior by assigning Mariota and Winston the historic first-year league average for all rookie QBs entering the NFL but we‚Äôll keep it simple. A uniform prior means that all probabilities from 0-1 are equally likely.\n Collecting QB Data We‚Äôll use the nflgame python package to gather QB data. We‚Äôll pull 2013-2014 weekly performance data for Joe Flacco to calculate our prior, as well as the 2015 data for all three players. During the season we‚Äôll update our priors to determine which QB we should play for a given week. That is, as we acquire results over the season, updates will be made to obtain a better, more reliable estimate of the ‚Äúsuccess‚Äù parameter for each QB.\nimport nflgame import pandas as pd game_years = range(2013, 2016) game_weeks = range(1, 17) qbs = (\u0026quot;Joe Flacco\u0026quot;, \u0026quot;Marcus Mariota\u0026quot;, \u0026quot;Jameis Winston\u0026quot;) def get_passing_data(year, week, players, qbs): qb_list = list() for p in players.passing(): player = \u0026quot; \u0026quot;.join(str(p.player).split(\u0026quot; \u0026quot;)[:2]) if player in qbs: qb_list.append([year, week, player, p.passing_tds, p.passing_yds]) return qb_list quarterback_data = pd.DataFrame() for year in game_years: print \u0026quot;Retrieving Player Data for {year}\u0026quot;.format(year = year) for week in game_weeks: games = nflgame.games(year, week) players = nflgame.combine_game_stats(games) temp_qb_stats = get_passing_data(year, week, players, qbs) quarterback_data = quarterback_data.append(pd.DataFrame(temp_qb_stats)) quarterback_data.columns = [\u0026quot;year\u0026quot;, \u0026quot;week\u0026quot;, \u0026quot;player\u0026quot;, \u0026quot;touchdowns\u0026quot;, \u0026quot;passing_yds\u0026quot;] quarterback_data.to_csv(\u0026quot;quarterback_data.csv\u0026quot;, index = False)  ","date":1505095994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505095994,"objectID":"c31b44c1658f0ae0fa3f3a918c898de5","permalink":"http://example.org/post/2017-09-10-choosing-ff-qb/choosing_fantasy_qb/","publishdate":"2017-09-10T21:13:14-05:00","relpermalink":"/post/2017-09-10-choosing-ff-qb/choosing_fantasy_qb/","section":"post","summary":"Aaron Rodgers or Tom Brady? Carson Wentz or Drew Brees? Choosing the right Fantasy Football QB each week is challenging. To remove some of the guesswork from the decision-making process, I devised an approach that‚Äôs worked well over the past few seasons. Read on to learn more about using the Beta Distribution to pick your weekly starting QB.","tags":["Fantasy Football","Python","R","Beta Distribution"],"title":"Choosing a Fantasy Football Quarterback","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"http://example.org/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"http://example.org/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Mark LeBoeuf","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"http://example.org/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Mark LeBoeuf","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"http://example.org/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"http://example.org/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]