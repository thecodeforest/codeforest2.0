[{"authors":null,"categories":null,"content":"I love asking big questions and answering them with data. My background is in Psychology and Statistics, and it’s been a blast seeing how these fields have co-evolved over the years. I currently work as a data scientist, and I started this blog as a way to document a few of the more interesting topics I’ve encountered throughout my journeys. The goal is to abstract away some of the technical bits in an attempt to give you (the reader) the ability to implement while learning. I’m always interested in connecting with fellow data-scientists/whisperers/magicians/analysts, so feel free to PM me on linkedin!\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/mark-leboeuf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mark-leboeuf/","section":"authors","summary":"I love asking big questions and answering them with data. My background is in Psychology and Statistics, and it’s been a blast seeing how these fields have co-evolved over the years.","tags":null,"title":"Mark LeBoeuf","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Mark LeBoeuf FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"📊 Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":["PySpark","Time-Series Forecasting","Prophet","Python"],"content":" Overview Whether predicting daily demand for thousands of products or the number of workers to staff across many distribution centers, generating operational forecasts in parallel is a common task for data scientists. Accordingly, the goal of this post is to outline an approach for creating many forecasts via PySpark. We’ll cover some common data-cleaning steps that often precede forecasting, and then generate several thousand week-level demand predictions for a variety consumer products. Note that we will not cover how to implement this workflow in a cloud computing environment (which, in a real-world setting, would typically be the case). Nor will we delve into model tuning or selection. The goal is to provide a straightforward workflow for quickly generating many time-series forecasts in parallel.\n Getting Started We’ll use data originally provided by Walmart that represents weekly demand for products at the store-department level. All code for this post is stored in the Codeforest Repository. Before diving into the details, let’s briefly review the key modules and files.\nconf.json - A configuration file that defines various parameters for our job. It’s a good practice to keep these parameters outside of your actual code, as it makes it easier for others (or future you!) to adapt and extend to other use cases. pyspark_fcast.py - Our main module, or where the forecasting gets done. We’ll cover this in detail below.\nfcast_data_frame.py - A class responsible for common pre-forecasting data transformations. These include filling in missing values, filtering time-series with only a few observations, or log transforming our outcome variable. Visit here for access to all methods.\nYou’ll also need to import the following packages to follow along with the tutorial.\nimport argparse import json import logging import os import re from datetime import datetime from pathlib import Path from typing import List import numpy as np import pandas as pd from fbprophet import Prophet # fbprophet==0.7.1 \u0026amp; pystan==2.18.0 from pyspark.sql import SparkSession # pyspark==3.0.1 from pyspark.sql.functions import lit from pyspark.sql.types import (DateType, FloatType, IntegerType, StructField, StructType) from pyspark_ts_fcast.fcast_data_frame import FcastDataFrame Assuming the imports were successful, we’ll peak at a few rows in our data to get a feel for the format. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rqobitfmng .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rqobitfmng .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rqobitfmng .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rqobitfmng .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #rqobitfmng .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rqobitfmng .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rqobitfmng .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rqobitfmng .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rqobitfmng .gt_column_spanner_outer:first-child { padding-left: 0; } #rqobitfmng .gt_column_spanner_outer:last-child { padding-right: 0; } #rqobitfmng .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #rqobitfmng .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #rqobitfmng .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rqobitfmng .gt_from_md  :first-child { margin-top: 0; } #rqobitfmng .gt_from_md  :last-child { margin-bottom: 0; } #rqobitfmng .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rqobitfmng .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #rqobitfmng .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rqobitfmng .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #rqobitfmng .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rqobitfmng .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rqobitfmng .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rqobitfmng .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rqobitfmng .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rqobitfmng .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #rqobitfmng .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rqobitfmng .gt_sourcenote { font-size: 90%; padding: 4px; } #rqobitfmng .gt_left { text-align: left; } #rqobitfmng .gt_center { text-align: center; } #rqobitfmng .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rqobitfmng .gt_font_normal { font-weight: normal; } #rqobitfmng .gt_font_bold { font-weight: bold; } #rqobitfmng .gt_font_italic { font-style: italic; } #rqobitfmng .gt_super { font-size: 65%; } #rqobitfmng .gt_footnote_marks { font-style: italic; font-size: 65%; }     Sample Data       store dept date weekly_sales    1 1 2010-02-05 24924   1 1 2010-02-12 46039   1 1 2010-02-19 41596   1 1 2010-02-26 19404   1 1 2010-03-05 21828    Let’s now discuss the process of passing and documenting the forecasting parameters. We’ll execute the following from the command line to generate our forecasts:\npython3 pyspark_fcast.py --forecast-config-file 'config/conf.json'\nHere we are passing in the location of our configuration file and extracting the parameters. Don’t worry if the individual parameters don’t make sense now. I’ll explain each in greater detail below.\nargs = read_args() with open(args.forecast_config_file) as f: config = json.load(f) log_input_params(config=config) # forecasting parameters input_data_path = config[\u0026quot;input_data_path\u0026quot;] fcast_params = config[\u0026quot;fcast_parameters\u0026quot;] group_fields = fcast_params[\u0026quot;group_fields\u0026quot;] date_field = fcast_params[\u0026quot;date_field\u0026quot;] yvar_field = fcast_params[\u0026quot;yvar_field\u0026quot;] ts_frequency = fcast_params[\u0026quot;ts_frequency\u0026quot;] fcast_floor = fcast_params[\u0026quot;forecast_floor\u0026quot;] fcast_cap = fcast_params[\u0026quot;forecast_cap\u0026quot;] min_obs_threshold = fcast_params[\u0026#39;min_obs_count\u0026#39;] # spark parameters spark_n_threads = str(config[\u0026#39;spark_n_threads\u0026#39;]) java_home = config[\u0026quot;java_home\u0026quot;] Note the two helper functions: read_args and log_input_params.\ndef read_args() -\u0026gt; argparse.Namespace: \u0026quot;\u0026quot;\u0026quot;Read Forecasting arguments Returns: argparse.Namespace: argparse Namespace \u0026quot;\u0026quot;\u0026quot; parser = argparse.ArgumentParser() parser.add_argument(\u0026quot;--forecast-config-file\u0026quot;, type=str) return parser.parse_args() read_args takes arguments in our configuration file, then we document which parameters we’re using with log_input_params.\nlogging.basicConfig( format=\u0026quot;%(levelname)s - %(asctime)s - %(filename)s - %(message)s\u0026quot;, level=logging.INFO, filename=\u0026quot;run_{start_time}.log\u0026quot;.format( start_time=datetime.now().strftime(\u0026quot;%Y-%m-%d %H-%M-%S\u0026quot;) ), ) def log_input_params(config: dict) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Logs all parameters in configuration file Args: config (dict): Configuration parameters for forecast and data \u0026quot;\u0026quot;\u0026quot; params = pd.json_normalize(config).transpose() [ logging.info(\u0026quot;input params:\u0026quot; + x[0] + \u0026quot;-\u0026quot; + str(x[1])) for x in zip(params.index, params.iloc[:, 0]) ] return None There are several benefits to documenting our inputs. First, we can validate if the correct parameters have been passed to our forecasting process. Having a record of these values facilitates debugging. Second, it is useful for experimentation. We can try out different parameters to see which combination provides the best results. Logging does not receive a lot of attention in the data science world, but it is incredibly useful and will save you time as your project matures.\nWe have our parameters and have set up logging. Next, we’ll read in the data stored here and execute some basic field formatting with clean_names.\ndef clean_names(df: pd.DataFrame) -\u0026gt; pd.DataFrame: \u0026quot;\u0026quot;\u0026quot;Applies the following transformations to column names: - Removes camel case - Replaces any double underscore with single underscore - Removes spaces in the middle of a string name - Replaces periods with underscores Args: df (pd.DataFrame): Dataframe with untransformed column names Returns: pd.DataFrame: Dataframe with transformed column names \u0026quot;\u0026quot;\u0026quot; cols = df.columns cols = [re.sub(r\u0026quot;(?\u0026lt;!^)(?=[A-Z])\u0026quot;, \u0026quot;_\u0026quot;, x).lower() for x in cols] cols = [re.sub(r\u0026quot;_{2,}\u0026quot;, \u0026quot;_\u0026quot;, x) for x in cols] cols = [re.sub(r\u0026quot;\\s\u0026quot;, \u0026quot;\u0026quot;, x) for x in cols] cols = [re.sub(r\u0026quot;\\.\u0026quot;, \u0026quot;_\u0026quot;, x) for x in cols] df.columns = cols return df sales_df = pd.read_csv(input_data_path) sales_df = clean_names(sales_df) If you don’t have a clean_names-type function as part of your codebase, I’d highly recommend creating one. It’s a function that I use frequently when reading data from various sources and encourages a standardized way of formatting field names.\nNow that we have our data, we’ll do some pre-forecasting data cleaning. The main steps are outlined below:\nFilter groups with limited observations - It’s a good idea to put predictions against items where you have some historical data. While the space of cold-start forecasting is very interesting, it is outside the scope of this post. Thus, we are putting a minimum threshold on the number of data points per group. This is also a good idea because some forecasting algorithms will not fit a model against a few observations, causing your program to crash.\nReplace negative values with zero - I’m assuming a negative value represents a returned product. Our goal is to forecast demand not demand - returns. This is an assumption that would need to be validated with domain knowledge.\nPad missing values - Accounting for missing data is an easy step to overlook for one simple reason: Missing values in time-series data are not usually flagged as “missing”. For example, a store may shut down for six weeks of renovations. As a result, there will be a series of dates that have no sales data. Identifying these gaps is pivotal for generating reliable forecasts. I’ve provided a brief example below to illustrate what this looks like from a data perspective.\nhtml { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nbpomebbll .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #nbpomebbll .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nbpomebbll .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #nbpomebbll .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #nbpomebbll .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nbpomebbll .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nbpomebbll .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #nbpomebbll .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #nbpomebbll .gt_column_spanner_outer:first-child { padding-left: 0; } #nbpomebbll .gt_column_spanner_outer:last-child { padding-right: 0; } #nbpomebbll .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #nbpomebbll .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #nbpomebbll .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #nbpomebbll .gt_from_md  :first-child { margin-top: 0; } #nbpomebbll .gt_from_md  :last-child { margin-bottom: 0; } #nbpomebbll .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nbpomebbll .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nbpomebbll .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nbpomebbll .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nbpomebbll .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nbpomebbll .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nbpomebbll .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #nbpomebbll .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nbpomebbll .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nbpomebbll .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #nbpomebbll .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nbpomebbll .gt_sourcenote { font-size: 90%; padding: 4px; } #nbpomebbll .gt_left { text-align: left; } #nbpomebbll .gt_center { text-align: center; } #nbpomebbll .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nbpomebbll .gt_font_normal { font-weight: normal; } #nbpomebbll .gt_font_bold { font-weight: bold; } #nbpomebbll .gt_font_italic { font-style: italic; } #nbpomebbll .gt_super { font-size: 65%; } #nbpomebbll .gt_footnote_marks { font-style: italic; font-size: 65%; }     Incomplete Data       store dept date weekly    1 1 2010-02-05 24924   1 1 2010-02-19 41596   1 1 2010-02-26 19404   1 1 2010-03-19 22137   1 1 2010-03-26 26229    html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rggjwgclsv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rggjwgclsv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rggjwgclsv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rggjwgclsv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #rggjwgclsv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rggjwgclsv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rggjwgclsv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rggjwgclsv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rggjwgclsv .gt_column_spanner_outer:first-child { padding-left: 0; } #rggjwgclsv .gt_column_spanner_outer:last-child { padding-right: 0; } #rggjwgclsv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #rggjwgclsv .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #rggjwgclsv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rggjwgclsv .gt_from_md  :first-child { margin-top: 0; } #rggjwgclsv .gt_from_md  :last-child { margin-bottom: 0; } #rggjwgclsv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rggjwgclsv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #rggjwgclsv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rggjwgclsv .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #rggjwgclsv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rggjwgclsv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rggjwgclsv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rggjwgclsv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rggjwgclsv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rggjwgclsv .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #rggjwgclsv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rggjwgclsv .gt_sourcenote { font-size: 90%; padding: 4px; } #rggjwgclsv .gt_left { text-align: left; } #rggjwgclsv .gt_center { text-align: center; } #rggjwgclsv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rggjwgclsv .gt_font_normal { font-weight: normal; } #rggjwgclsv .gt_font_bold { font-weight: bold; } #rggjwgclsv .gt_font_italic { font-style: italic; } #rggjwgclsv .gt_super { font-size: 65%; } #rggjwgclsv .gt_footnote_marks { font-style: italic; font-size: 65%; }     Padded Data       store dept date weekly    1 1 2010-02-05 24924   NA NA 2010-02-12 NA   1 1 2010-02-19 41596   1 1 2010-02-26 19404   NA NA 2010-03-05 NA   NA NA 2010-03-12 NA   1 1 2010-03-19 22137   1 1 2010-03-26 26229    We’ll go back and fill or “interpolate” those missing values in the weekly_sales field in a later step.\nFilter groups with long ‘streaks’ of missing observations - Building on the previous example, let’s say the store closes for six months instead of six weeks. Thus, half of the year will not have any sales information. We could fill it in with a reasonable value, such as the average, but this won’t capture the overall trend, seasonality, or potential holiday/event effects that help to explain variation in our outcome variable. I’ll often initially exclude these time-series, and then try to understand why/how long streaks of values are missing. In this case, we’ll set a limit of four weeks (i.e., if any time-series has more than four consecutive dates missing, exclude from the final forecasting step).\nInterpolate missing values - Fills in missing data with “reasonable” values. We’ll use the overall mean of each series, which is a very simple and easy to understand technique. There are better approaches that account for seasonality or local trends. However, the goal here isn’t to generate the best forecast but instead to create a good starting point from which to iterate.\nAdd forecasting bounds - This function is specific to the Prophet API and is not required to generate a forecast via PySpark. However, when you cannot inspect the quality of each forecast, adding in some “guardrails” can prevent errant predictions that erode trust with your stakeholders. The floor and cap fields provide bounds that a forecast cannot go above or below. For example, if the minimum value in a time-series is 10 and the maximum is 100, a floor of 0.5 and a cap of 1.5 ensures all forecasted values are not above 150 (100 * 1.5) or less than 5 (10 * 0.5). Again, these decisions are often driven by domain knowledge of the forecaster. We’ll go a bit deeper on this field below as well.\nLog transform outcome variable - Log transforming our outcome variable is an effective approach to reduce the influence of outliers and stabilize variance that increases over time. A separate approach is to use a box-cox transformation (see here for more details), which can yield better results than a log-transformation. However, I often start with a log-transformation because it does require us to keep track of the transformation parameters, which is something you’ll need to do with a box-cox transformation. Are we seeing a theme here? Start simple.\nWhew - that was a lot of information, but we can finally implement all of these data-cleaning steps via the FcastDataFrame class. The format was inspired by the sklearn.pipeline class to prepare and clean grouped time-series data prior to generating forecasts.\nclass FcastDataFrame: \u0026quot;\u0026quot;\u0026quot;Use for pre-processing data prior to forecasting\u0026quot;\u0026quot;\u0026quot; def __init__( self, df: pd.DataFrame, group_fields: List[str], date_field: str, yvar_field: str, ts_frequency: str, ): \u0026quot;\u0026quot;\u0026quot; Args: df (pd.DataFrame): dataframe with to be forecasted data group_fields (List[str]): grouping fields. These are often re represented by attributes of each unit (e.g., store id, product id, etc.). date_field (str): date field yvar_field (str): outcome (\u0026quot;y\u0026quot;) field ts_frequency (str): granularity of the data. For example, data that is recorded on a weekly basis, every Friday will be \u0026quot;W-FRI\u0026quot;. Note that sub-day level (e.g, hourly, minute) data is not supported. \u0026quot;\u0026quot;\u0026quot; self.df = df self.group_fields = group_fields self.date_field = date_field self.yvar_field = yvar_field self.ts_frequency = ts_frequency fcast_df = FcastDataFrame( df=sales_df, group_fields=group_fields, date_field=date_field, yvar_field=yvar_field, ts_frequency=ts_frequency, )  While we won’t cover all methods in this class, I’ll briefly review one of the methods – filter_groups_min_obs – to illustrate the structure of the class.\ndef filter_groups_min_obs(self, min_obs_threshold: int): \u0026quot;\u0026quot;\u0026quot;Filters groups based on some minimum number of observations required for forecasting Args: min_obs_threshold (int): removes all groups with less obsevations than this threshold \u0026quot;\u0026quot;\u0026quot; n_unique_groups = self.df[self.group_fields].drop_duplicates().shape[0] min_obs_filter_df = ( self.df.groupby(self.group_fields)[self.yvar_field] .count() .reset_index() .rename(columns={self.yvar_field: \u0026quot;obs_count\u0026quot;}) .query(f\u0026quot;obs_count \u0026gt; {str(min_obs_threshold)}\u0026quot;) .drop(columns=\u0026quot;obs_count\u0026quot;) ) n_remaining_groups = min_obs_filter_df.shape[0] df = pd.merge(self.df, min_obs_filter_df, how=\u0026quot;inner\u0026quot;, on=self.group_fields) self.df = df logger.info(\u0026quot;N groups dropped: {}\u0026quot;.format(n_unique_groups - n_remaining_groups)) Each data transformation takes in our data, applies some filtering, cleaning, or formatting, logs the changes, and then replaces the existing DataFrame with the updated DataFrame. This pattern is applied at each step until we are satisfied with the changes. Let’s apply these filtering and cleaning steps below.\n# filter out groups with less than min number of observations fcast_df.filter_groups_min_obs(min_obs_threshold=min_obs_threshold) # replace any negative value with a zero fcast_df.replace_negative_value_with_zero() # replace missing dates between start and end of time-series by group fcast_df.pad_missing_values() # filter groups with consecutive missing streak longer than 4 fcast_df.filter_groups_max_missing_streak(max_streak=4) # impute missing values fcast_df.fill_missing_values() # add upper and lower bounds for forecasting fcast_df.add_forecast_bounds( floor_multiplier=fcast_floor, cap_multiplier=fcast_cap ) # log transform outcome, floor, and cap values fcast_df.log_transform_values(yvar_field, \u0026quot;floor_value\u0026quot;, \u0026quot;cap_value\u0026quot;) # return transformed data fcast_df_trans = fcast_df.return_transformed_df() Now we are ready to do some forecasting. In the next section, we’ll produce our forecasts from the cleaned and prepared data.\n  Pyspark Forecasting Let’s start by translating the field names to those that Prophet understands. For example, our date variable should be named ds and our outcome variable y. We’ll use the prep_for_prophet function to make the transition.\ndef prep_for_prophet( df: pd.DataFrame, yvar_field: str, date_field: str, group_fields: List[str] ) -\u0026gt; pd.DataFrame: \u0026quot;\u0026quot;\u0026quot;Renames key field names to be compatible with Prophet Forecasting API Args: df (pd.DataFrame): Contains data that will be used to generate forecasting yvar_field (str): outcome (\u0026quot;y\u0026quot;) field name date_field (str): date field name group_fields (List[str]): grouping fields. These are often represented by attributes of each unit (e.g., store id, product id, etc.). Returns: pd.DataFrame: Data with compatible field names \u0026quot;\u0026quot;\u0026quot; fields = df.columns.tolist() cap_value_index = [ index for index, value in enumerate([\u0026quot;cap_value\u0026quot; in x for x in fields]) if value ] floor_value_index = [ index for index, value in enumerate([\u0026quot;floor_value\u0026quot; in x for x in fields]) if value ] if cap_value_index and floor_value_index: df = df.rename( columns={ fields[cap_value_index[0]]: \u0026quot;cap\u0026quot;, fields[floor_value_index[0]]: \u0026quot;floor\u0026quot;, } ) group_fields = group_fields + [\u0026quot;cap\u0026quot;, \u0026quot;floor\u0026quot;] df = df[group_fields + [date_field] + [yvar_field]] df = df.rename(columns={date_field: \u0026quot;ds\u0026quot;, yvar_field: \u0026quot;y\u0026quot;}) df[\u0026quot;ds\u0026quot;] = pd.to_datetime(df[\u0026quot;ds\u0026quot;]) return df fcast_df_prophet_input = prep_for_prophet( df=fcast_df_trans, yvar_field=\u0026quot;weekly_sales_prep_log1p\u0026quot;, date_field=date_field, group_fields=group_fields, )  With our data prepared, we’ll shift over to creating a Spark Session and indicate where our Java version is located. Note this step will vary depending on your computing environment.\nos.environ[\u0026quot;JAVA_HOME\u0026quot;] = java_home SPARK = ( SparkSession.builder.master(f\u0026quot;local[{spark_n_threads}]\u0026quot;) .appName(config[\u0026quot;app_name\u0026quot;]) .config(\u0026quot;spark.sql.execution.arrow.pyspark.enabled\u0026quot;, \u0026quot;true\u0026quot;) .getOrCreate() ) Next, we’ll define the schema (or format) of our input and output data.\nINPUT_SCHEMA = StructType( [ StructField(\u0026quot;store\u0026quot;, IntegerType(), True), StructField(\u0026quot;dept\u0026quot;, IntegerType(), True), StructField(\u0026quot;cap\u0026quot;, FloatType(), True), StructField(\u0026quot;floor\u0026quot;, FloatType(), True), StructField(\u0026quot;ds\u0026quot;, DateType(), True), StructField(\u0026quot;y\u0026quot;, FloatType(), True), ] ) OUTPUT_SCHEMA = StructType( [ StructField(\u0026quot;ds\u0026quot;, DateType(), True), StructField(\u0026quot;store\u0026quot;, IntegerType(), True), StructField(\u0026quot;dept\u0026quot;, IntegerType(), True), StructField(\u0026quot;yhat_lower\u0026quot;, FloatType(), True), StructField(\u0026quot;yhat_upper\u0026quot;, FloatType(), True), StructField(\u0026quot;yhat\u0026quot;, FloatType(), True), ] )  We’ll now translate our Pandas DataFrame to a Spark DataFrame and pass in the schema we defined above.\nfcast_spark_prophet_input = SPARK.createDataFrame( fcast_df_prophet_input, schema=INPUT_SCHEMA ) The function below does the actual forecasting and we’ll spend some time unpacking what’s happening here.\ndef run_forecast(keys, df): \u0026quot;\u0026quot;\u0026quot;Generate time-series forecast Args: keys: Grouping keys df: Spark Dataframe \u0026quot;\u0026quot;\u0026quot; fields = [\u0026quot;ds\u0026quot;, \u0026quot;store\u0026quot;, \u0026quot;dept\u0026quot;, \u0026quot;yhat_lower\u0026quot;, \u0026quot;yhat_upper\u0026quot;,\u0026quot;yhat\u0026quot;] store, dept = keys cap = df[\u0026quot;cap\u0026quot;][0] floor = df[\u0026quot;floor\u0026quot;][0] model = Prophet( interval_width=0.95, growth=\u0026quot;logistic\u0026quot;, yearly_seasonality=True, seasonality_mode=\u0026quot;additive\u0026quot;, ) model.add_country_holidays(country_name=\u0026quot;US\u0026quot;) model.fit(df) future_df = model.make_future_dataframe( periods=13, freq=\u0026quot;W-FRI\u0026quot;, include_history=False ) future_df[\u0026quot;cap\u0026quot;] = cap future_df[\u0026quot;floor\u0026quot;] = floor results_df = model.predict(future_df) results_df[\u0026quot;store\u0026quot;] = store results_df[\u0026quot;dept\u0026quot;] = dept results_df = results_df[fields] return results_df Let’s start by discussing the Prophet model, which automates the selection of many forecasting settings, like seasonality, determined during the model fitting process. Below is a brief summary of some of the key settings:\ninterval_width - Interval width quantifies uncertainty in our forecast. Wider intervals indicate greater uncertainty. Here, we are indicating that the actual values should fall outside of the interval ~5% of the time. By default, Prophet is set to 80%, which is less conservative than our setting here. Providing a measure of uncertainty is perhaps even more important than the forecast itself, as it allows a business to hedge against the risk of being wrong. For example, imagine a product has a high margin and a low inventory holding cost. In this instance, you would want to plan to a high percentile, as you rarely want to stock out of this product.\nyearly_seasonality - Setting this to True indicates my belief that there is week-over-week variation that repeats itself over the course of a year. For example, sales for items like sandals or sunscreen are likely higher in Summer weeks and lower in the Winter weeks. There are two other seasonality options not included above - daily and hourly. Daily captures hourly changes within a day, while hourly captures minute-by-minute changes within an hour. Our data is at the week level, so we can ignore these two settings.\ngrowth - Growth is a way to encode our beliefs regarding if a forecast should reach a “saturation” point across your prediction horizon (see here for official documentation). For example, customer acquisition slows as a market matures and will eventually reach a saturation point (i.e., the total addressable market has been acquired). This is typically used for “long-range” forecasting on the scale of several years. Our forecasting horizon is much shorter at only 13 weeks. However, I like to codify what I consider to be reasonable amount of growth, via the “cap” parameter, as well as contraction, via the “floor” parameter, in my forecasts, especially when I cannot inspect each result.\nseasonality_mode - I’ve selected “additive” for this parameter based on my belief that the magnitude of seasonal changes do not vary across time. Recall that our outcome variable has already been log-transformed, thus we are actually using an additive decomposition of the log-transformed values.\nadd_country_holidays - Holidays tend to drive increases in consumption of particular products. And some holidays, like Easter, are not consistent year-over-year. Thus, you can improve forecasting accuracy if you anticipate how demand shifts when generating forecasts based on when holidays occur. One thing to note that is not included in the current post (but is incredibly useful) is the ability to apply a lower_window and upper_window to each holiday date. Continuing with our Easter example, you can imagine egg sales increase in the days leading up to Easter. Sales on the holiday date may not be that high, unless you are doing some last minute shopping. By extending the lower_window parameter for this holiday to something like -5, you can capture the elevated demand during the five days that precede Easter.\nNow that we are familiar with how the model is being tuned, let’s generate the forecasts. This may take a few minutes depending on how many threads you are using. I am using four, and it took about 20 minutes to complete.\nfcast_df_prophet_output = ( fcast_spark_prophet_input.groupBy(group_fields) .applyInPandas(func=run_forecast, schema=OUTPUT_SCHEMA) .withColumn(\u0026quot;part\u0026quot;, lit(\u0026quot;forecast\u0026quot;)) .withColumn(\u0026quot;fcast_date\u0026quot;, lit(datetime.now().strftime(\u0026quot;%Y-%m-%d\u0026quot;))) .toPandas() .rename( columns={ \u0026quot;yhat\u0026quot;: yvar_field, \u0026quot;yhat_lower\u0026quot;: f\u0026quot;{yvar_field}_lb\u0026quot;, \u0026quot;yhat_upper\u0026quot;: f\u0026quot;{yvar_field}_ub\u0026quot;, \u0026quot;ds\u0026quot;: date_field, } ) ) We should have 13-week forecasts for all store-department combinations. Our next steps are to combine the forecasts with the historical data and invert our log-transformation of the outcome variable to get back to our original scale. Note that np.log1p and np.expm1 are inverses of one another, and elegantly deal with zero values by adding/subtracting a value of “1” to avoid taking the log of zero, which is undefined and will make your code go 💥. Lastly, we’ll write the results out to our root directory.\nfcast_df_prophet_input[\u0026quot;part\u0026quot;] = \u0026quot;actuals\u0026quot; fcast_df_prophet_input = fcast_df_prophet_input.rename( columns={\u0026quot;y\u0026quot;: yvar_field, \u0026quot;ds\u0026quot;: date_field} ) del fcast_df_prophet_input[\u0026quot;cap\u0026quot;] del fcast_df_prophet_input[\u0026quot;floor\u0026quot;] ret_df = pd.concat([fcast_df_prophet_input, fcast_df_prophet_output]) ret_df = ret_df.apply(lambda x: round(np.expm1(x)) if yvar_field in x.name else x) ret_df.to_csv(Path.cwd() / \u0026quot;sales_data_forecast.csv\u0026quot;, index=False)  Quality Assurance We’ll transition back to the world of R for some quick quality-assurance work. Let’s read in our forecasts and examine a few store-department combinations. Note there are much more formal ways to validate the performance of our models, but our objective is to do a quick sanity check (i.e., “do the forecasts look reasonable for a few randomly sampled grouped?”). The raw output is stored in Github. Let’s start by examining the first and last five rows for a single Store-Dept combination.\nlibrary(tidyverse) library(timetk) library(lubridate) fcast_df_url = \u0026quot;https://raw.githubusercontent.com/thecodeforest/codeforest_datasets/main/pyspark_forecasting_data/sales_data_forecast.csv\u0026quot; fcast_df = read_csv(fcast_df_url) df_store_dept_sample \u0026lt;- fcast_df %\u0026gt;% filter(store == 1, dept == 1) %\u0026gt;% mutate(date = as_date(date)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #vajqssudrv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 9px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #vajqssudrv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vajqssudrv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #vajqssudrv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #vajqssudrv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vajqssudrv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vajqssudrv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #vajqssudrv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #vajqssudrv .gt_column_spanner_outer:first-child { padding-left: 0; } #vajqssudrv .gt_column_spanner_outer:last-child { padding-right: 0; } #vajqssudrv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #vajqssudrv .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #vajqssudrv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #vajqssudrv .gt_from_md  :first-child { margin-top: 0; } #vajqssudrv .gt_from_md  :last-child { margin-bottom: 0; } #vajqssudrv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #vajqssudrv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #vajqssudrv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vajqssudrv .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #vajqssudrv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vajqssudrv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #vajqssudrv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #vajqssudrv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vajqssudrv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vajqssudrv .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #vajqssudrv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vajqssudrv .gt_sourcenote { font-size: 90%; padding: 4px; } #vajqssudrv .gt_left { text-align: left; } #vajqssudrv .gt_center { text-align: center; } #vajqssudrv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #vajqssudrv .gt_font_normal { font-weight: normal; } #vajqssudrv .gt_font_bold { font-weight: bold; } #vajqssudrv .gt_font_italic { font-style: italic; } #vajqssudrv .gt_super { font-size: 65%; } #vajqssudrv .gt_footnote_marks { font-style: italic; font-size: 65%; }     Top 5 Rows of Forecasting Data       store dept date weekly_sales part weekly_sales_lb weekly_sales_ub fcast_date    1 1 2010-02-05 24924 actuals NA NA NA   1 1 2010-02-12 46039 actuals NA NA NA   1 1 2010-02-19 41596 actuals NA NA NA   1 1 2010-02-26 19404 actuals NA NA NA   1 1 2010-03-05 21828 actuals NA NA NA    html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #uwmuwyxmsv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 9px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #uwmuwyxmsv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uwmuwyxmsv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #uwmuwyxmsv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #uwmuwyxmsv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uwmuwyxmsv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uwmuwyxmsv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #uwmuwyxmsv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #uwmuwyxmsv .gt_column_spanner_outer:first-child { padding-left: 0; } #uwmuwyxmsv .gt_column_spanner_outer:last-child { padding-right: 0; } #uwmuwyxmsv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #uwmuwyxmsv .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #uwmuwyxmsv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #uwmuwyxmsv .gt_from_md  :first-child { margin-top: 0; } #uwmuwyxmsv .gt_from_md  :last-child { margin-bottom: 0; } #uwmuwyxmsv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #uwmuwyxmsv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #uwmuwyxmsv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uwmuwyxmsv .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #uwmuwyxmsv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uwmuwyxmsv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #uwmuwyxmsv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #uwmuwyxmsv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uwmuwyxmsv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uwmuwyxmsv .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #uwmuwyxmsv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uwmuwyxmsv .gt_sourcenote { font-size: 90%; padding: 4px; } #uwmuwyxmsv .gt_left { text-align: left; } #uwmuwyxmsv .gt_center { text-align: center; } #uwmuwyxmsv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #uwmuwyxmsv .gt_font_normal { font-weight: normal; } #uwmuwyxmsv .gt_font_bold { font-weight: bold; } #uwmuwyxmsv .gt_font_italic { font-style: italic; } #uwmuwyxmsv .gt_super { font-size: 65%; } #uwmuwyxmsv .gt_footnote_marks { font-style: italic; font-size: 65%; }     Bottom 5 Rows of Forecasting Data       store dept date weekly_sales part weekly_sales_lb weekly_sales_ub fcast_date    1 1 2012-12-28 30948 forecast 21883 42839 2021-04-05   1 1 2013-01-04 21138 forecast 14793 30024 2021-04-05   1 1 2013-01-11 16149 forecast 11384 22832 2021-04-05   1 1 2013-01-18 15553 forecast 10712 21662 2021-04-05   1 1 2013-01-25 18954 forecast 13475 27282 2021-04-05    Let’s sample a few forecasts and plot them out.\nset.seed(2021) fcast_df %\u0026gt;% filter(store \u0026lt; 3, dept %in% c(df %\u0026gt;% distinct(dept) %\u0026gt;% sample_n(2) %\u0026gt;% pull()) ) %\u0026gt;% mutate(store = paste0(\u0026#39;Store: \u0026#39;, store), dept = paste0(\u0026#39;Dept: \u0026#39;, dept), store_id = paste(store, dept, sep=\u0026#39; \u0026#39;)) %\u0026gt;% select(date, store_id, contains(\u0026#39;weekly\u0026#39;)) %\u0026gt;% pivot_longer(contains(\u0026#39;weekly\u0026#39;)) %\u0026gt;% mutate(name = str_to_title(str_replace_all(name, \u0026#39;_\u0026#39;, \u0026#39; \u0026#39;))) %\u0026gt;% ggplot(aes(date, value, color = name)) + geom_line(size = 1.5, alpha = 0.8) + facet_grid(store_id ~ ., scales = \u0026#39;free\u0026#39;) + theme_bw() + scale_y_continuous(labels = scales::comma_format()) + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;Weekly Sales\u0026#39;, color = NULL, title = \u0026#39;Sample Forecasts\u0026#39; ) + theme(legend.position = \u0026quot;top\u0026quot;, legend.text = element_text(size = 12), strip.text.y = element_text(size = 12), plot.title = element_text(size = 14) ) Overall, the forecasts appear to capture changes in the trend and seasonal variation. A more formal approach to this problem is to do back-testing by holding out some historical data and generating forecasts against it. However, this is a great starting point from which to build more advanced models and incorporate external variables to further improve our forecasts. Hopefully this is enough to get you started on your way to forecasting at an enterprise scale. Until next time, happy forecasting!\n ","date":1617070394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617070394,"objectID":"b80014d95722605d7f4bfae86202f384","permalink":"/post/2021-03-28-pyspark-forecasting/pyspark_time_series_forecasting/","publishdate":"2021-03-29T21:13:14-05:00","relpermalink":"/post/2021-03-28-pyspark-forecasting/pyspark_time_series_forecasting/","section":"post","summary":"Whether predicting daily demand for thousands of products or the number of workers to staff across many distribution centers, generating operational forecasts in parallel is a common task for data scientists. Accordingly, the goal of this post is to outline an approach for creating many forecasts via PySpark.","tags":["PySpark","Time-Series Forecasting","Prophet","Python"],"title":"Scalable Time-Series Forecasting in Python","type":"post"},{"authors":null,"categories":["Causal Inference","Propensity Scores","R"],"content":" Overview Causal inference attempts to answer “what-if” questions. For example, if the minimum wage were increased, what effect would it have on unemployment rates? Or if an entertainment company launched a marketing campaign for a new movie, what effect would it have on box-office sales? The objective in each of these examples is to quantify the impact of an intervention – a change in wages or a targeted marketing campaign – on an outcome – increasing employment or bolstering revenue. Estimating how a particular action can affect an end-state falls within the realm of prescriptive analytics and can inform decision-making in the face of multiple possible actions.\nHowever, most analytics efforts are applied to either describing or predicting an outcome rather than understanding what drives it. For example, imagine you work for a cheese shop. You might be asked to describe how sales of cheese have changed over the past year. Or perhaps you want to predict how much cheese will sell over the next 12 months. Descriptive analytics highlights if existing operational or strategic decisions are impacting the business (i.e., cheese sales) as anticipated. In contrast, predictive analytics can inform operational planning (e.g., how much cheese to manufacture), improve consumer experiences (e.g., an online cheese recommendation system), or automate repetitive tasks (e.g., automatically detecting defective cheese wheels during production with computer vision). Note that none of these applications provide insight about the source of variation or root cause(s) of change in an outcome. Without this knowledge, it can be difficult to know where resources should be focused or how to grow and improve the business.\nAccordingly, the goal of this post is to highlight one approach to conducting prescriptive analytics and generating causal inferences with observational data. We’ll first walk through some of the basics of causal inference and propensity scores, followed by a practical example that brings these concepts together. At the end of this post, you should have a solid understanding of how propensity scores can be used in the real-world to guide decision-making.\n Causal Inference \u0026amp; Propensity Scores When people hear the words “causal inference”, they often think “A/B Test”. Indeed, the traditional way of answering causal questions is to randomly assign individuals to a treatment or control condition. The treatment is exposed to the intervention, while the control is not. The average difference is then calculated between the two conditions on some measure of interest to understand if the intervention had the desired effect.\nWhile A/B testing is considered the most rigorous way of inferring causation, it is not practical or possible in many situations. For example, if you were interested in the effect of a membership program on customer retention, you cannot assign customers to be a member or non-member; customers choose to enroll in the program under their own volition. Further, customers who chose to enroll as members are probably more interested in the product than those who chose not to enroll. This fact “confounds” the relationship between the effect of our member program on retention.\nPropensity score matching attempts to address this issue, known as selection bias, by adjusting for factors that relate both to the treatment and outcome. A propensity score is scaled from 0 - 1 and indicates the probability of receiving treatment. Continuing with our previous membership example, a propensity score would indicate the probability that a customer joins our membership program after seeing a banner on our website or receiving a promotional email. It does not indicate their probability of churning.\nEach individual in our sample would receive a propensity score. The score is a single number that attempts to capture all factors that contribute to churn that aren’t related to membership but have an effect on our outcome variable. Such factors are known as latent variables and could include prior spend, willingness to provide personal information, or how long the individual has been a customer. Formalizing how these variables relate to one another is the topic of the next section.\n Causal Graphs We can formalize our beliefs and assumptions about observational data through a causal graph. This is normally the first step on our journey of causal inference, as it allows us to translate domain knowledge into a formal structure. By creating a diagram about potential confounding variables as well as the direction of causal influence, we make our assumptions about the data generating process explicit. For example, we assume that membership should influence churn, not that churn influences membership, and we encode this assumption in our causal graph. The exclusion of certain variables from our graph (e.g., age, gender, what types of products someone has previously purchased, etc.) is also an assumption, such that we assume these variables do not directly or indirectly affect churn or membership.\nAll of these assumptions can be examined. If we assume that the age of our customer has an effect on churn, we can stratify our customers by age (i.e,., 20-29, 30-39) and test if churn rates differ between age groups. If there were significant differences between groups, we would include an age variable in our graph and adjust for its influence on the outcome.\nThis is a contrived example, so we’ll keep things simple and formalize the main components of our analysis as follows:\nChurn - if a customer has made at least one purchase six months following the launch of our membership program. This is our outcome variable.\nMembership - if a customer enrolled as a member since the launch of the membership program. This is our treatment variable.\nBrand Interest - this is an example of a latent variable. We would use several variables in practice but, to keep things simple, we’ll only use prior purchase history, defined as the total dollars spent over the past year, as a proxy for Brand Interest. Our assumption is that customers who are more interested in the brand will spend more money. We also assume that this (partially) motivates enrollment in membership. Engaged customers will not only spend more but also be more interested in exclusive offers and discounts – a few benefits often provided to members – relative to customers that have historically spent less.\nThe image above was created via the daggity website, which makes it easy to create Causal DAGs or Directed Acyclic Graphs. Note the goal of creating a propensity score is to block the arrow from Interest in Brand to Churn. This addresses the issue of selection bias, in that our customers can “select into” the member condition. By adjusting for this pre-existing difference, we are attempting to make this bias strongly ignorable, similar to a randomized experiment.\nAnother aspect to consider is when an individual joined our membership program. We want to allow enough time for differences to emerge, so ideally we have at least a few months to see what happens. Second, membership offers and the quality may change over time, just as the consumers relationship with our brand changes. By narrowing the time frame of analysis, we can further control for time-related factors.\nLast, we want to time-bound the meaning of “historical spend”. Some customers may have spent a lot in the past but have not been active for several years. These customers have may have already churned, and we want to ensure that all customers in our sample have at least some chance of being exposed to the treatment. Thus, we could apply some simple logic to narrow our consideration set, such as “all customers that made at least one purchase 12 months prior to the start of our member program”. This is not a hard-and-fast rule but something to consider when deciding which individuals to include in your analysis.\n Estimating the Effect of Membership on Churn Now that we have a solid conceptual foundation, let’s continue to work through our membership example by generating some contrived data.\nlibrary(tidyverse) library(broom) library(rsample) set.seed(2021) # sample size n = 5000 enrolled_in_membership \u0026lt;- c(rep(0, n), rep(1, n)) membership_effect_size = 0.05 churn_prob_non_member = 0.5 churn_prob_member = churn_prob_non_member - membership_effect_size churned_member \u0026lt;- c(rbinom(n, 1, prob=churn_prob_non_member), rbinom(n, 1, prob=churn_prob_member)) In the code block above, we are saying that the effect size of membership on churn is 0.05, such that 50% of non-members will churn after six months, while only 45% of members will churn over the same time period. This establishes the effect size of membership on churn.\nNext, we’ll define the historical differences in total spend via the gamma distribution. The gamma distribution is useful for continuous data that is right-skewed and always positive. The two parameters - alpha (shape) and beta (rate) - determine the skew and how “stretched” the range of values are for the distribution, respectively. It also approximates aggregate spending patterns in the real-world, such that most consumers spend a little, while a few spend a lot. Let’s visualize what this looks like, under the assumption that members tend to spend more than non-members, after bringing all the data together.\nshape = 1 rate_member \u0026lt;- 0.005 rate_non_member \u0026lt;- rate_member + 0.005 member_spend_hist \u0026lt;- rgamma(n=n, shape=shape, rate = rate_member) non_member_spend_hist \u0026lt;- rgamma(n=n, shape=shape, rate = rate_non_member) prior_spend = c(non_member_spend_hist, member_spend_hist) df \u0026lt;- tibble(member = enrolled_in_membership, churned_member = churned_member, prior_spend = prior_spend ) spend_summary \u0026lt;- df %\u0026gt;% group_by(member) %\u0026gt;% summarise(avg_prior_spend = mean(prior_spend)) df %\u0026gt;% inner_join(spend_summary) %\u0026gt;% mutate(member = as.factor(member)) %\u0026gt;% ggplot(aes(prior_spend, fill = member)) + geom_histogram(alpha = 0.75) + geom_vline(data = spend_summary, aes(xintercept = avg_prior_spend), size = 2, lty = 4, alpha = 0.5) + facet_grid(member ~ .) + theme_bw() + scale_x_continuous(labels = scales::dollar_format(), breaks = seq(0, 2000, 100) ) + labs(x = \u0026#39;Total Previous Spend\u0026#39;) + theme(legend.position = \u0026quot;top\u0026quot;, axis.text.x = element_text(size = 12, angle = 90), axis.text.y = element_text(size = 14), axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), legend.text = element_text(size = 14), legend.title = element_text(size = 16), strip.text.y = element_text(size = 14) ) On average, members spent ~$100 more relative to non-members. This is a simple yet effective way for validating our initial assumption that members exhibit different behavior than non-members.\nNext, we’ll establish the relationship between historical spend and churn. Let’s create five bins and assign a probability of churn within each bin, such that higher bins (i.e., the top 20% of spenders) have a lower probability of churning relative to lower bins (i.e., the bottom 20% of spenders). We’ll then create the joint effect of treatment (membership) and our single covariate (prior spend) on our outcome (churned).\nchurn_simulator \u0026lt;- function(bin){ if(bin == 1){ return(rbinom(1, 1, prob = 0.5)) } else if (bin == 2){ return(rbinom(1, 1, prob = 0.4)) } else if (bin == 3){ return(rbinom(1, 1, prob = 0.3)) } else if (bin == 4){ return(rbinom(1, 1, prob = 0.2)) } else { return(rbinom(1, 1, prob = 0.1)) } } df \u0026lt;- df %\u0026gt;% mutate(bin = ntile(prior_spend, 5), churned_spend = map_int(bin, churn_simulator), churned = ifelse(churned_member + churned_spend \u0026gt; 0, 1, 0) ) We can verify that our simulated relationship between churn and historical spend matches our expectations. Note we’ll scale our prior_spend field to be per $100 dollars spent instead of per $1 dollar spent. This makes interpreting the resulting coefficient more intuitive.\nglm(churned ~ prior_spend, data = df %\u0026gt;% mutate(prior_spend = prior_spend / 100), family = binomial() ) %\u0026gt;% tidy(exponentiate = TRUE) %\u0026gt;% filter(term == \u0026#39;prior_spend\u0026#39;) ## # A tibble: 1 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 prior_spend 0.860 0.0129 -11.7 1.73e-31 The estimated coefficient indicates that for each additional $100 of historical spend, the odds of churn decrease by ~14%. Now that we have verified the assumed relationships between our variables, we’ll generate propensity scores an in effort to create more “balance” between our control and treatment groups.\n Propensity Scores The most common approaches for creating comparison groups are (1) propensity score matching and (2) inverse probability of treatment weighting (IPTW). Both methods use a propensity score but create groups differently. Matching looks for individuals in the non-treated condition who have similar propensity scores to those in the treated condition. If the groups are different sizes, the number of non-treated observations are reduced to the size of the treated condition, as each treated observation is matched with a single non-treated observation. In contrast, weighting includes all observations but places more weight on observations with a high propensity scores and less weight on observations with low propensity scores.\nThis post will use weighting. While both approaches may yield similar results, I prefer weighting because you are not discarding data. Our simulated data does not suffer from an imbalance, but in most real-world situations you’ll have a lot fewer observations in your treatment group relative to your control group. Additionally, the quality of your matching will depend mostly on how well you’ve specified your model and accounted for unobserved covariates.\nWith that in mind, we’ll specify our model for generating propensity scores.\n# specify model for estimating P(treatment | prior_spend) model_spec \u0026lt;- as.formula(member ~ prior_spend) member_model \u0026lt;- glm(model_spec, data = df, family = binomial()) member_prop_df \u0026lt;- member_model %\u0026gt;% augment(type.predict = \u0026#39;response\u0026#39;, data = df) %\u0026gt;% select(member, churned, prior_spend, member_prob = .fitted) %\u0026gt;% mutate(iptw = 1 / ifelse(member == 0, 1 - member_prob, member_prob)) A few things to note now we’ve generated our propensity scores. First, we used logistic regression as a way to estimate membership probability. However, any classification model can be used to generate a score. If you suspect non-linearities between your covariates and treatment variable, using a model that can better capture these relationships, such as a tree-based model, may yield better estimates. Second, we’ll need to be cognizant of the resulting weights. If certain observations receive very large weights, they will have an outsized influence on our coefficient estimates. It is a common practice to truncate large weights at 10 (why 10 I’m not sure). I’d prefer to use a point from our actual distribution, so we’ll assign any value above the 99th percentile to the value at the 99th percentile.\niptw_pct_99 \u0026lt;- quantile(member_prop_df %\u0026gt;% pull(iptw), 0.99)[[1]] member_prop_df \u0026lt;- member_prop_df %\u0026gt;% mutate(iptw = ifelse(iptw \u0026gt; iptw_pct_99, iptw_pct_99, iptw)) Now that we’ve addressed some common pre-modeling isues, let’s generate an initial estimate of the effect of membership on churn.\nmembership_effect \u0026lt;- glm(churned ~ member, data = member_prop_df, family = binomial(), weights = iptw ) %\u0026gt;% tidy(exponentiate = TRUE) %\u0026gt;% filter(term == \u0026#39;member\u0026#39;) %\u0026gt;% pull(estimate) print(membership_effect) ## [1] 0.8870706 Our initial estimate indicates that membership leads to a 11% reduction in churn, which in the real-world would be fairly significant effect! However, we are also interested in the uncertainty of our estimate; that is, what is best-case and worst-case of our effect size? The above-approach has been known to provide biased estimates of our standard error, which in turn results in confidence intervals that are too narrow. This means we over-confident in our ability to accurately estimate our causal effect. To address this issue, we’ll bootstrap the entire process - from generating our propensity score weights to estimating our causal effect - and then use the resulting distribution to better capture the uncertainty in our estimate.\nmember_fit_bootstrap \u0026lt;- function(split){ temp_df \u0026lt;- analysis(split) temp_model \u0026lt;- glm(member ~ prior_spend, family = binomial(), data = temp_df ) temp_df \u0026lt;- temp_model %\u0026gt;% augment(type.predict = \u0026#39;response\u0026#39;, data = temp_df) %\u0026gt;% select(member, churned, prior_spend, member_prob = .fitted) %\u0026gt;% mutate(iptw = 1 / ifelse(member == 0, 1 - member_prob, member_prob)) temp_iptw_pct_99 \u0026lt;- quantile(temp_df %\u0026gt;% pull(iptw), 0.99)[[1]] temp_df \u0026lt;- temp_df %\u0026gt;% mutate(iptw = ifelse(iptw \u0026gt; temp_iptw_pct_99, temp_iptw_pct_99, iptw)) temp_ret_df \u0026lt;- glm(churned ~ member, data = temp_df, family = binomial(), weights = iptw) %\u0026gt;% tidy(exponentiate = TRUE) return(temp_ret_df) } n_boot = 500 boot_results \u0026lt;- bootstraps(df, n_boot, apparent = TRUE) %\u0026gt;% mutate(results = map(splits, member_fit_bootstrap)) In the above code snippet, we created 500 boot-strapped replicates and then fit a model to each. Our next step is to look at the distribution of the resulting estimates.\nboot_results \u0026lt;- boot_results %\u0026gt;% select(-splits) %\u0026gt;% unnest(cols=results) %\u0026gt;% filter(term == \u0026#39;member\u0026#39;) boot_summary \u0026lt;- boot_results %\u0026gt;% summarise(lb = quantile(estimate, 0.025), mdn = quantile(estimate, 0.5), ub = quantile(estimate, 0.975) ) %\u0026gt;% pivot_longer(everything()) lb_est \u0026lt;- boot_summary %\u0026gt;% filter(name==\u0026#39;lb\u0026#39;) %\u0026gt;% pull(value) ub_est \u0026lt;- boot_summary %\u0026gt;% filter(name==\u0026#39;ub\u0026#39;) %\u0026gt;% pull(value) effect_est \u0026lt;- boot_summary %\u0026gt;% filter(name==\u0026#39;mdn\u0026#39;) %\u0026gt;% pull(value) boot_results %\u0026gt;% ggplot(aes(x=estimate)) + geom_histogram(fill = \u0026#39;orange\u0026#39;) + theme_bw() + geom_segment(aes(x=lb_est, xend = lb_est), y = 0, yend = 10, lty = 3, size = 2) + geom_segment(aes(x=effect_est, xend = effect_est), y = 0, yend = 10, lty = 3, size = 2) + geom_segment(aes(x=ub_est, xend = ub_est), y = 0, yend = 10, lty = 3, size = 2) + annotate(geom=\u0026#39;text\u0026#39;, x=lb_est, y = 11, label = round(lb_est, 2), size = 8) + annotate(geom=\u0026#39;text\u0026#39;, x=effect_est, y = 11, label = round(effect_est, 2), size = 8) + annotate(geom=\u0026#39;text\u0026#39;, x=ub_est, y = 11, label = round(ub_est, 2), size = 8) + labs(x = \u0026#39;Estimated Treatment Effect\u0026#39;) + theme( axis.text.x = element_text(size = 14), axis.text.y = element_text(size = 14), axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14) ) Our final estimate is that membership reduces the odds customer churn by somewhere between 0.04 and 0.18. confidence intervals aren’t actually that much different from those in the original model. The primary reason is that our sample size for both groups is fairly large and there is not a lot of variance in the determinants of membership, given that we generated the data. Again, in real data sets, the confidence intervals following bootstrapping will typically be wider than those provided by OLS.\nHopefully this provided a solid end-to-end walkthrough of how to generate causal inferences from observational data with propensity scores. In the next post, we’ll discuss an equally important topic – variance reduction methods – that comes in handy when you are able to run a true a/b test. Until then, happy experimenting!\n ","date":1615860794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615860794,"objectID":"6394a27f1d417ab8493a4ce9e099ca04","permalink":"/post/2021-03-15-causal-inference-pt-1/causal_inference_part_1/","publishdate":"2021-03-15T21:13:14-05:00","relpermalink":"/post/2021-03-15-causal-inference-pt-1/causal_inference_part_1/","section":"post","summary":"A Summary","tags":["Causal Inference","Propensity Scores","R","Beta Distribution"],"title":"Causal Inference with Propensity Scores","type":"post"},{"authors":null,"categories":["R","Emperical Bayes","Fantasy Football"],"content":" Overview In less than two weeks, Fantasy Football will once again resume for the 2019 NFL season! While I’m looking forward to the impending draft, the start of the season brings back memories of a not-so-distant loss that left me one game shy of the championship. The loss stemmed from a missed field goal, leaving my team two points shy of victory. Of course, a myriad of factors beyond that missed field goal contributed to my fantasy demise, but those two points reinvigorated a question I’ve wondered about for the past few years: Why are kickers drafted in the last round?\nPrevailing wisdom suggests that your kicker doesn’t matter. Some Fantasy Football leagues don’t even have kickers on the roster, which I think does a disservice to a player who probably doesn’t get invited to the cool team parties yet can decide the fate of a season in a single moment (like mine). As long as they suit up to take the field, the rest is out of your control. However, is it a suboptimal strategy to relegate your choice of kicker to the final round of the draft? Let’s find out!\n Getting Started Before loading any data or discussing techniques, we’ll begin by defining our analytical objective. An easy way to get started is by posing a simple question: “How many more points can I expect over a 16-game regular season if I draft the best kicker relative to the worst kicker?” We’ll answer this question in two steps. First, we’ll estimate the True field goal percentage for each kicker currently active in the NFL (as of 2016), which is analogous to a batting average in baseball or free-throw percentage in basketball. This parameter estimate will be used to compare the skill of one kicker to another. Second, we’ll translate our estimate into actual Fantasy Football points by simulating the outcomes 1000 football seasons for each kicker. Simulation enables us to quantify a realistic point differential between kickers, which is what we (the Fantasy Football team owners) will use to determine if we should try to select the best kicker by drafting in an earlier round.\nWith that question in mind, let’s load all pertinent libraries. The data can be downloaded directly from the 🎋 the codeforest data repo 🎄. Note the original data comes from Kaggle and can found here.\n# Modeling library(gamlss) # Core packages library(tidyverse) library(janitor) # Visualization library(ggplot2) library(scales) library(viridis) library(ggridges) # Tables library(gt) # Global plot theme theme_set(theme_minimal()) # Code Forest repo data_url \u0026lt;- \u0026quot;https://raw.githubusercontent.com/thecodeforest/codeforest_datasets/main/fantasy_football_kickers_data/Career_Stats_Field_Goal_Kickers.csv\u0026quot; # Helper function for visualization my_plot_theme = function(){ font_family = \u0026quot;Helvetica\u0026quot; font_face = \u0026quot;bold\u0026quot; return(theme( axis.text.x = element_text(size = 16, face = font_face, family = font_family), axis.text.y = element_text(size = 16, face = font_face, family = font_family), axis.title.x = element_text(size = 16, face = font_face, family = font_family), axis.title.y = element_text(size = 16, face = font_face, family = font_family), strip.text.y = element_text(size = 22, face = font_face, family = font_family), plot.title = element_text(size = 22, face = font_face, family = font_family), legend.position = \u0026quot;top\u0026quot;, legend.title = element_text(size = 16, face = font_face, family = font_family), legend.text = element_text(size = 16, face = font_face, family = font_family), legend.key = element_rect(size = 5), legend.key.size = unit(1.5, \u0026#39;lines\u0026#39;) )) } There are several columns we won’t be using so we’ll select only the relevant ones.\nstats_raw \u0026lt;- read_csv(data_url) %\u0026gt;% clean_names() %\u0026gt;% select(player_id, name, year, games_played, contains(\u0026#39;made\u0026#39;), contains(\u0026#39;attempted\u0026#39;), contains(\u0026#39;percentage\u0026#39;), -contains(\u0026#39;extra\u0026#39;), -longest_fg_made ) glimpse(stats_raw) ## Rows: 1,994 ## Columns: 19 ## $ player_id \u0026lt;chr\u0026gt; \u0026quot;jeffhall/2500970\u0026quot;, \u0026quot;benagajanian/2508255\u0026quot;… ## $ name \u0026lt;chr\u0026gt; \u0026quot;Hall, Jeff\u0026quot;, \u0026quot;Agajanian, Ben\u0026quot;, \u0026quot;Agajanian… ## $ year \u0026lt;dbl\u0026gt; 2000, 1964, 1962, 1961, 1961, 1960, 1957, … ## $ games_played \u0026lt;dbl\u0026gt; 3, 3, 6, 3, 3, 14, 12, 10, 12, 12, 10, 12,… ## $ f_gs_made \u0026lt;chr\u0026gt; \u0026quot;4\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;13\u0026quot;, \u0026quot;10\u0026quot;, \u0026quot;5\u0026quot;, … ## $ f_gs_made_20_29_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;… ## $ f_gs_made_30_39_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;… ## $ f_gs_made_40_49_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;… ## $ f_gs_made_50_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;… ## $ f_gs_attempted \u0026lt;chr\u0026gt; \u0026quot;5\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;14\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;9\u0026quot;, \u0026quot;24\u0026quot;, \u0026quot;18\u0026quot;, \u0026quot;13\u0026quot;… ## $ f_gs_attempted_20_29_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;… ## $ f_gs_attempted_30_39_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;… ## $ f_gs_attempted_40_49_yards \u0026lt;chr\u0026gt; \u0026quot;2\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;… ## $ f_gs_attempted_50_yards \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;… ## $ fg_percentage \u0026lt;chr\u0026gt; \u0026quot;80.0\u0026quot;, \u0026quot;50.0\u0026quot;, \u0026quot;35.7\u0026quot;, \u0026quot;50.0\u0026quot;, \u0026quot;33.3\u0026quot;, \u0026quot;5… ## $ fg_percentage_20_29_yards \u0026lt;chr\u0026gt; \u0026quot;100.0\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--… ## $ fg_percentage_30_39_yards \u0026lt;chr\u0026gt; \u0026quot;100.0\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--… ## $ fg_percentage_40_49_yards \u0026lt;chr\u0026gt; \u0026quot;50.0\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;… ## $ fg_percentage_50_yards \u0026lt;chr\u0026gt; \u0026quot;100.0\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;--… Like most real-world datasets, this one is a bit messy (e.g., non-values are coded as “–”). I find it helps at the outset of data cleaning to envision what a perfect, pristine dataset should look like once data munging steps are complete. Below is an example of a basic starting point. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #zpvivrqdou .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #zpvivrqdou .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zpvivrqdou .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #zpvivrqdou .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #zpvivrqdou .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zpvivrqdou .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zpvivrqdou .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #zpvivrqdou .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #zpvivrqdou .gt_column_spanner_outer:first-child { padding-left: 0; } #zpvivrqdou .gt_column_spanner_outer:last-child { padding-right: 0; } #zpvivrqdou .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #zpvivrqdou .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #zpvivrqdou .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #zpvivrqdou .gt_from_md  :first-child { margin-top: 0; } #zpvivrqdou .gt_from_md  :last-child { margin-bottom: 0; } #zpvivrqdou .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #zpvivrqdou .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #zpvivrqdou .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zpvivrqdou .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #zpvivrqdou .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zpvivrqdou .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #zpvivrqdou .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #zpvivrqdou .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zpvivrqdou .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zpvivrqdou .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #zpvivrqdou .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zpvivrqdou .gt_sourcenote { font-size: 90%; padding: 4px; } #zpvivrqdou .gt_left { text-align: left; } #zpvivrqdou .gt_center { text-align: center; } #zpvivrqdou .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #zpvivrqdou .gt_font_normal { font-weight: normal; } #zpvivrqdou .gt_font_bold { font-weight: bold; } #zpvivrqdou .gt_font_italic { font-style: italic; } #zpvivrqdou .gt_super { font-size: 65%; } #zpvivrqdou .gt_footnote_marks { font-style: italic; font-size: 65%; }     Desired Data Format       id n_success n_trials    1 5 10   2 7 30   3 1 8   4 10 12   5 20 24   6 30 50   7 60 200   8 2 4   9 11 14   10 24 61   \nI used generic column names if you’re interested in adopting the techniques described herein to solve a separate problem. At a basic level, each row represents an individual observation, a count of the number of successes (i.e., count how many field goals are made), and finally the number of trials (i.e., count how many field goals are attempted). If you have this setup, the building blocks are in place to get started.\nHowever, before going any further, we need to ensure the relationships in the data align with our understanding of the world. One approach is to generate some simple hypotheses that you know to be true. For example, water is wet, the sky is blue, and, in our case, the field goal percentage should decrease as the distance to the goal increases. That is, field goals taken from 50+ yards should be made at a lower rate those taken from 30-35 yards. Let’s verify our hypothesis below.\nmake_by_dist \u0026lt;- stats_raw %\u0026gt;% select(starts_with(\u0026quot;fg_percentage_\u0026quot;)) %\u0026gt;% mutate_all(as.numeric) %\u0026gt;% gather(key = \u0026quot;dist\u0026quot;, value = \u0026quot;fg_pct\u0026quot;) %\u0026gt;% mutate( dist = str_extract(dist, pattern = \u0026quot;\\\\d{2}\u0026quot; ), dist = if_else(dist == \u0026quot;50\u0026quot;, paste0(dist, \u0026quot;+\u0026quot;), paste0(dist,\u0026quot;-\u0026quot;,as.numeric(dist) + 9) ), fg_pct = fg_pct / 100 ) %\u0026gt;% na.omit() make_by_dist %\u0026gt;% ggplot(aes(fg_pct, dist, fill = dist)) + geom_density_ridges( aes(point_color = dist, point_fill = dist, point_shape = dist), alpha = .2, point_alpha = 1, jittered_points = TRUE ) + scale_point_color_hue(l = 40) + scale_discrete_manual(aesthetics = \u0026quot;point_shape\u0026quot;, values = c(21, 22, 23, 24)) + scale_x_continuous(labels = scales::percent, breaks = c(0,0.2, 0.4, 0.6, 0.8, 1) ) + scale_fill_viridis_d() + my_plot_theme() + labs(x = \u0026#39;Field Goal Percentage\u0026#39;, y = \u0026#39;Distance (Yards)\u0026#39; ) + theme(legend.position = \u0026#39;none\u0026#39;) Looks good! Each point represents the field goal percentage for a player-season-distance combination. As the distance increases, the make rate gradually shifts to left, which is exactly what we’d expect. We’ll do a bit more cleaning below before proceeding.\nstats_processed \u0026lt;- stats_raw %\u0026gt;% mutate( name = str_remove(name, \u0026quot;,\u0026quot;), first_name = map(name, function(x) str_split(x, \u0026quot; \u0026quot;)[[1]][2]), last_name = map(name, function(x) str_split(x, \u0026quot; \u0026quot;)[[1]][1]), player_id = str_extract(player_id, \u0026quot;\\\\d+\u0026quot;) ) %\u0026gt;% unite(\u0026quot;name\u0026quot;, c(\u0026quot;first_name\u0026quot;, \u0026quot;last_name\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% mutate_at(vars(matches(\u0026quot;attempted|made\u0026quot;)), as.numeric) %\u0026gt;% replace(., is.na(.), 0) %\u0026gt;% select(player_id, name, year, games_played, contains(\u0026quot;made\u0026quot;), contains(\u0026quot;attempted\u0026quot;)) %\u0026gt;% rename( fg_made = f_gs_made, fg_attempted = f_gs_attempted ) Let’s view the resulting data for one of the best kickers in modern NFL to familiarize ourselves with the format.\nstats_processed %\u0026gt;% filter(name == \u0026quot;Justin Tucker\u0026quot;) %\u0026gt;% mutate(fg_pct = fg_made / fg_attempted) %\u0026gt;% select(name, year, fg_made, fg_attempted) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #pkjidbziro .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #pkjidbziro .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pkjidbziro .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #pkjidbziro .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #pkjidbziro .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pkjidbziro .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pkjidbziro .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #pkjidbziro .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #pkjidbziro .gt_column_spanner_outer:first-child { padding-left: 0; } #pkjidbziro .gt_column_spanner_outer:last-child { padding-right: 0; } #pkjidbziro .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #pkjidbziro .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #pkjidbziro .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #pkjidbziro .gt_from_md  :first-child { margin-top: 0; } #pkjidbziro .gt_from_md  :last-child { margin-bottom: 0; } #pkjidbziro .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #pkjidbziro .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #pkjidbziro .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pkjidbziro .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #pkjidbziro .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pkjidbziro .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #pkjidbziro .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #pkjidbziro .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pkjidbziro .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pkjidbziro .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #pkjidbziro .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pkjidbziro .gt_sourcenote { font-size: 90%; padding: 4px; } #pkjidbziro .gt_left { text-align: left; } #pkjidbziro .gt_center { text-align: center; } #pkjidbziro .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #pkjidbziro .gt_font_normal { font-weight: normal; } #pkjidbziro .gt_font_bold { font-weight: bold; } #pkjidbziro .gt_font_italic { font-style: italic; } #pkjidbziro .gt_super { font-size: 65%; } #pkjidbziro .gt_footnote_marks { font-style: italic; font-size: 65%; }     Justin Tucker Stats       name year fg_made fg_attempted    Justin Tucker 2016 38 39   Justin Tucker 2015 33 40   Justin Tucker 2014 29 34   Justin Tucker 2013 38 41   Justin Tucker 2012 30 33    Just like what we had above! Next, we’ll add a few filters to reduce some of the noise in our data. Any player who has less than 30 field goal attempts and/or has kicked field goals in only one season across their career will be excluded from the analysis. Additionally, we’ll ignore any players with a rookie year before the 1970s. The rationale here is that the NFL made several changes to the location and positioning of the goal during the early 70s, so we want to keep the dynamics of the kicking environment consistent for all players.\nmin_attempts \u0026lt;- 30 min_seasons \u0026lt;- 2 min_decade \u0026lt;- 1970 filter_df \u0026lt;- stats_processed %\u0026gt;% group_by(player_id) %\u0026gt;% summarise(n_seasons = n(), n_attempts = sum(fg_attempted), rookie_decade = min(year) %/% 10 * 10 ) %\u0026gt;% filter(n_seasons \u0026gt;= min_seasons, n_attempts \u0026gt;= min_attempts, rookie_decade \u0026gt;= min_decade ) %\u0026gt;% select(player_id) stats_processed \u0026lt;- inner_join(stats_processed, filter_df) Next, we’ll add a few features before aggregating the kicking data from a season level to a career level for each player. I’ll cover the rationale of the features shortly in the estimation and inference section below.\ndf_time_stats \u0026lt;- stats_processed %\u0026gt;% group_by(player_id) %\u0026gt;% summarise( rookie_decade = min(year) %/% 10 * 10, last_yr_active = max(year) ) %\u0026gt;% ungroup() %\u0026gt;% filter(rookie_decade \u0026gt;= min_decade) %\u0026gt;% mutate(status = ifelse(last_yr_active == 2016, \u0026#39;active\u0026#39;, \u0026#39;inactive\u0026#39;)) stats_processed \u0026lt;- inner_join(stats_processed,df_time_stats) We have our time-based features and the last step is to calculate our three key metrics – successes, attempts, and our rate metric.\nstats_agg \u0026lt;- stats_processed %\u0026gt;% group_by(player_id, name, rookie_decade, status) %\u0026gt;% summarise(fg_made = sum(fg_made), fg_attempted = sum(fg_attempted), fg_pct = fg_made / fg_attempted ) %\u0026gt;% ungroup() Time to move on to the key focus of this post.\n Estimation and Inference Let’s now discuss the logic underlying our estimation method as well as the role of the additional features (Note that some of the code below was inspired by the excellent book Introduction to Empirical Bayes: Examples from Baseball Statistics by David Robinson). To recap, we are estimating a proportion that captures the relationship between successes and attempts. We can model this outcome with the beta distribution, which is simply a distribution of probabilities ranging from 0 - 1. In our case, it represents the likelihood of a particular field goal percentage for each player, which will fall somewhere between 0.5 and 0.9 depending on the decade(s) the player was active (more on that in second).\nBelow we’ll fit an null model with no additional parameters when estimating each player’s beta value. The absence of any inputs means that all players have the same prior, independent of what decade they played in, whether they’re still active, or how many chances they’ve had to kick a field goal. We’ll then take our prior and update it based on how much information we have about each player, namely the number of field goals they’ve taken and how often they’ve succeeded.\nfit_null \u0026lt;- gamlss(cbind(fg_made, fg_attempted - fg_made) ~ 1, family = BB(mu.link = \u0026quot;identity\u0026quot;), data = stats_agg ) ## GAMLSS-RS iteration 1: Global Deviance = 936.6917 ## GAMLSS-RS iteration 2: Global Deviance = 836.9846 ## GAMLSS-RS iteration 3: Global Deviance = 828.0258 ## GAMLSS-RS iteration 4: Global Deviance = 827.9528 ## GAMLSS-RS iteration 5: Global Deviance = 827.9526 stats_agg_est \u0026lt;- stats_agg %\u0026gt;% mutate( mu = fitted(fit_null, \u0026quot;mu\u0026quot;), sigma = fitted(fit_null, \u0026quot;sigma\u0026quot;), alpha0 = mu / sigma, beta0 = (1 - mu) / sigma, alpha1 = alpha0 + fg_made, beta1 = beta0 + fg_attempted - fg_made, estimate = alpha1 / (alpha1 + beta1), raw = fg_made / fg_attempted, low = qbeta(.025, alpha1, beta1), high = qbeta(.975, alpha1, beta1) ) Let’s plot out the estimate for all active players.\nstats_agg_est %\u0026gt;% mutate(name = paste0(name, \u0026quot;: \u0026quot;, fg_made, \u0026quot;|\u0026quot;, fg_attempted), name = fct_reorder(name, estimate) ) %\u0026gt;% filter(status == \u0026quot;active\u0026quot;) %\u0026gt;% ggplot(aes(name, estimate)) + geom_point(size = 3) + geom_errorbar(aes(ymin = low, ymax = high)) + coord_flip() + geom_point(aes(name, raw), color = \u0026quot;red\u0026quot;, size = 3, alpha = 0.6) + scale_y_continuous(labels = scales::percent_format()) + my_plot_theme() + labs(x = NULL, y = \u0026#39;Field Goal Percentage\u0026#39;, title = \u0026#39;Estimated field goal percentage amongst active NFL kickers\u0026#39;, subtitle = \u0026#39;Black dot represents estimate while red dot is actual. Note the bias in our estimates.\u0026#39; ) Let’s talk through this figure by comparing the field goal percentage estimates for Adam Vinatieri, who has made 530 of 629 fields goals throughout his career, to Chris Boswell, who has made 50 of 57 field goals. While Vinatieri has a lower actual make rate than Boswell (84.2% vs. 87.7%), we consider him to be a better field goal kicker. The seemingly incongruent finding is based on the fact that we have more evidence for Vinatieri (629 FG attempts vs. 57 FG attempts) than Boswell. It’s like saying, “Chris Boswell is good kicker, maybe better than Vinatieri, but we don’t have enough evidence (yet) to believe he is that much better than an average kicker, a number represented by our prior”. Indeed, if we also consider the width of the credible intervals surrounding these two players, Adam Vinatieri’s interval is considerably smaller than Chris Boswell’s interval.\nWhile this is a good way to gain an intuition for what’s happening under the hood, we see an immediate problem – all of our estimates are biased! The actual field goal percentage is above every single estimate. Luckily, there is a solution: we can create conditional estimates of our prior. One way to do this is to create features that explain variability between our players. For example, field goal percentages have improved dramatically over the past 50 years. Let’s consider our own data and map out this pattern from the 1970s to the 2010s.\nstats_agg %\u0026gt;% mutate(rookie_decade = as.factor(rookie_decade)) %\u0026gt;% ggplot(aes(rookie_decade, fg_pct, color = rookie_decade)) + geom_boxplot() + geom_jitter() + scale_y_continuous(labels = scales::percent_format()) + my_plot_theme() + scale_color_viridis_d() + theme(legend.position = \u0026quot;none\u0026quot;) + labs( x = \u0026quot;Decade\u0026quot;, y = \u0026quot;Field Goal Percentage\u0026quot;, title = \u0026#39;Kicker performance has improved over time\u0026#39; ) The best kicker in 1970s has a lower field goal percentage than the worst kicker in the 2010s. Including the decade of a kicker’s rookie season allows us to create a more informed prior. Thus, if we use the median field goal percentage of all kickers who debuted as rookies in 2010+, our best guess would be about 84%, whereas a kicker who debuted in the 1970s would be somewhere around 64%. This explains why the estimates from our null model were biased.\nThe second factor to consider is the number of field goal attempts per player, because better players have more opportunities to kick field goals. This makes intuitive sense and is captured in the following plot.\nstats_agg %\u0026gt;% ggplot(aes(log2(fg_attempted), fg_pct)) + geom_point(size = 3) + geom_smooth(span = 1) + scale_y_continuous(labels = scales::percent_format()) + my_plot_theme() + labs( x = \u0026quot;Log2(Total Attempts)\u0026quot;, y = \u0026quot;Field Goal percentage\u0026quot;, title = \u0026quot;Better kickers have more opportunities\u0026quot; ) Below we’ll use the same model except this time we’ll account for the number of field goal attempts and a player’s rookie decade.\nfit_complete \u0026lt;- gamlss(cbind(fg_made, fg_attempted - fg_made) ~ log2(fg_attempted) + rookie_decade, family = BB(mu.link = \u0026quot;identity\u0026quot;), data = stats_agg ) ## GAMLSS-RS iteration 1: Global Deviance = 918.3071 ## GAMLSS-RS iteration 2: Global Deviance = 714.8471 ## GAMLSS-RS iteration 3: Global Deviance = 668.3708 ## GAMLSS-RS iteration 4: Global Deviance = 668.1839 ## GAMLSS-RS iteration 5: Global Deviance = 668.1838 Much better! Our estimates do not exhibit the same degree of bias as before. Moreover, the width of our credible intervals shrank across all players. This makes sense, given that we can now condition our prior estimates on inputs that explain variability in the field goal percentage. While there are other factors that might improve our model (e.g., did a player’s team have their home games in a dome?), this is a good starting point for answering our original question.\n From Parameters to Points We have a model that does a reasonable job of estimating a kicker’s field goal percentage. Now we need to translate that into an estimate of fantasy points. This will take a few steps, but I’ll outline each in turn. First, we need to estimate the average worth (in fantasy points) of each successful field goal. Typically, field goals less-than 40 yards are worth 3 points, 40 - 49 yards are worth 4 points, and 50 or more yards are worth 5 points. We’ll use the 2016 season to come up with a global average. While we could technically account for distances of each player (e.g., some kickers are excellent at a longer distances, others not so much), this approach will give us a “good-enough” answer. Second, we’ll estimate the average number of field goal attempts per season. This can vary widely from one season to the next for a given kicker, as it is contingent upon the offense getting within kicking range. Again, we’ll keep it simple and just average the number of attempts across all players from the 2016 season.\n# Average points per FG pts_per_fg \u0026lt;- stats_processed %\u0026gt;% filter(year == 2016) %\u0026gt;% mutate(pt_3_fgs = (f_gs_made_20_29_yards + f_gs_made_30_39_yards) * 3, pt_4_fgs = f_gs_made_40_49_yards * 4, pt_5_fgs = f_gs_made_50_yards * 5, tot_pts = pt_3_fgs + pt_4_fgs + pt_5_fgs ) pts_per_fg \u0026lt;- round(sum(pts_per_fg$tot_pts) / sum(pts_per_fg$fg_made), 1) # Average number of attempts attempts_per_season \u0026lt;- stats_processed %\u0026gt;% filter(year == 2016) %\u0026gt;% pull(fg_attempted) %\u0026gt;% mean() %\u0026gt;% round() Here comes the fun part. Below we’ll simulate 1000 seasons for each player by randomly generating 1000 values of beta. This value is based on the posterior estimates, alpha1 and beta1, produced by our model. The estimates will vary from one simulation to next, though most values will fall somewhere between 0.75 and 0.9. Better players like Justin Tucker will be near the high end of that range while player like Graham Gano will be near the lower end. We’ll then take each estimate and plug it into the binomial distribution below. Recall that the binomial distribution is defined by a single parameter, which represents the probability of success. This is exactly what our estimate of beta represents! Given that all active players had an average of 27 FG attempts in 2016, each of the 1000 simulations will consist of 27 trials (or attempts_per_season) each with a slightly different probability of success (how likely they are to make a field goal on a given attempt). We’ll lean on the purrr package to vectorize these operations.\nset.seed(2018) n_seasons \u0026lt;- 1000 est_active \u0026lt;- stats_agg_est %\u0026gt;% filter(status == \u0026#39;active\u0026#39;) est_make_pct \u0026lt;- map2(est_active %\u0026gt;% pull(alpha1), est_active %\u0026gt;% pull(beta1), function(x, y) rbeta(n_seasons, x, y) ) est_outcomes \u0026lt;- map(est_make_pct, function(x) rbinom(n = n_seasons, size = attempts_per_season, prob = x ) ) names(est_outcomes) \u0026lt;- est_active$name So much data! Below we’ll plot the distribution of total points accumulated for each player across the 1000 simulated seasons. We’ll create quantiles as a way to see how much overlap there is between players.\npt_simulation \u0026lt;- est_outcomes %\u0026gt;% tbl_df() %\u0026gt;% gather() %\u0026gt;% transmute(name = key, season_pts = value * pts_per_fg ) %\u0026gt;% group_by(name) %\u0026gt;% mutate(avg_pts = mean(season_pts)) %\u0026gt;% ungroup() %\u0026gt;% mutate(name = fct_reorder(name, avg_pts)) ## Warning: `tbl_df()` is deprecated as of dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. pt_simulation %\u0026gt;% ggplot(aes(season_pts, y = name, fill = factor(..quantile..))) + stat_density_ridges( geom = \u0026quot;density_ridges_gradient\u0026quot;, calc_ecdf = TRUE, quantiles = 4, quantile_lines = TRUE, bandwidth = 2 ) + scale_fill_viridis(discrete = TRUE, name = \u0026quot;Point Quartile\u0026quot;, alpha = 0.5) + my_plot_theme() + scale_x_continuous(breaks = pretty_breaks(n = 7)) + labs(x = \u0026#39;Total Points Per Simulated Season\u0026#39;, y = NULL, title = \u0026quot;The best kicker is not much better than the worst kicker\u0026quot;, subtitle = \u0026#39;Drafing any kicker is fine\u0026#39; ) + theme(legend.position = \u0026#39;none\u0026#39;) Wait! We went all this way for you to tell me that the status quo is probably right? Yes, I did. But we still haven’t quantified how much better or worse drafting the best or worst kicker is in terms of fantasy points. A simple way is to count the number of seasons where Justin Tucker (the best kicker) scored more points than Andrew Franks (the worst kicker).\njt_pts \u0026lt;- pt_simulation %\u0026gt;% filter(name == \u0026#39;Justin Tucker\u0026#39;) %\u0026gt;% pull(season_pts) af_pts \u0026lt;- pt_simulation %\u0026gt;% filter(name == \u0026#39;Andrew Franks\u0026#39;) %\u0026gt;% pull(season_pts) pct_greater \u0026lt;- sum(jt_pts \u0026gt; af_pts) / n_seasons print(str_glue(\u0026#39;PCT greater: {pct_greater * 100}%\u0026#39;)) ## PCT greater: 77.5% Turns out that approximately 77 of every 100 seasons Justin Tucker outscores Andrew Franks. Let’s go one step further and quantify the actual difference.\nggplot(data.frame(pt_diff = jt_pts - af_pts), aes(pt_diff)) + geom_histogram(fill = \u0026#39;gray\u0026#39;, color = \u0026#39;black\u0026#39;, bins = 10) + scale_x_continuous(breaks = pretty_breaks(n = 15)) + labs(x = \u0026#39;Point Difference over Entire Season\u0026#39;) + theme_minimal() + geom_vline(xintercept = quantile(jt_pts - af_pts, .05), lty = 2) + geom_vline(xintercept = quantile(jt_pts - af_pts, .5), lty = 2, color = \u0026#39;red\u0026#39;, size = 2) + geom_vline(xintercept = quantile(jt_pts - af_pts, .95), lty = 2) + my_plot_theme() + labs(x = \u0026#39;Point Difference\u0026#39;, y = \u0026#39;Count\u0026#39;, title = \u0026#39;The best kicker should score about 10 more points per season compared to the worst\u0026#39;, subtitle = \u0026#39;Estimate based on 27 FG attempts per season with each FG worth 3.5 points\u0026#39; ) If we spread this estimate out across 16 regular-season games, it comes out to less than a single point per game.\n Conclusion Needless to say, pick your kicker last in Fantasy Football! All kickers in modern-day NFL are really good, so save those late-round picks for positions other than a kicker. Cheers!\n ","date":1566871994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566871994,"objectID":"bba60f55dd5430d350a9195332bc051c","permalink":"/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers/","publishdate":"2019-08-26T21:13:14-05:00","relpermalink":"/post/2019-08-29-fantasy-football-kickers/fantasy_football_kickers/","section":"post","summary":"We'll use 50 years of NFL kicking data to inform the least -- or most -- important decision of your fantasy season: Drafting a kicker.","tags":["R","Emperical Bayes","Fantasy Football"],"title":"Choosing a Fantasy Football Kicker with Emperical Bayes Estimation","type":"post"},{"authors":["Mark LeBoeuf"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"/terms/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"\u0026hellip;","tags":null,"title":"","type":"page"},{"authors":null,"categories":["Fantasy Football","Python","R","Beta Distribution"],"content":" Overview Understanding a new concept is all about connecting it with something you already know. I don’t know much, but I do know Fantasy Football. Thus, when I come across new concepts, I often think to myself, “How can I use this information to beat my friend Steve in Fantasy Football”? This very question was the impetus for putting these words and figures together in a post, which will introduce the idea of using the Beta Distribution to determine your weekly starter. I’ll explain this approach in the context of my 2015 Fantasy Football season.\nAt the outset of that season, I drafted two quarterbacks: Joe Flacco and Marcus Mariota (it was a rough draft). Flacco had been in the NFL for a few years, while Mariota was still a rookie yet to play a game. I was also considering a separate rookie, Jameis Winston, who was available to pick up anytime during the season off the waiver wire. Throughout the season, I was faced with the following questions:\n Who do I make the starting QB? If one QB is performing poorly, when is the right time to make the switch (e.g., Flacco -\u0026gt; Mariota; Flacco -\u0026gt; Winston; Mariota -\u0026gt; Winston)?  This question is faced by NFL coaches and fantasy owners alike. If your QB has a few bad weeks, should you continue with them into the next week, replace them with the 2nd string QB, or sign a free agent to your team mid-season?\nBefore getting into the technical details, let’s first define what “Success” looks like for a Fantasy Football QB. Success can be defined in one word: Consistency. A QB that throws three touchdowns (TDs) every game for the first six games of the season (18 total) is better than a QB who throws five TDs for the first three games and then one TD during the next three games, despite having thrown the same number of TDs. Simply put - you want consistent, reliable performance every week. It doesn’t matter if you win by one point or 50 points – a win is a win. Thus, I evaluate my QB’s performance on the following criteria: A “Successful” performance is defined as 3 or more touchdowns AND/OR 300 or more yards for a given week. Touchdowns and passing yards are the two primary sources of QB fantasy points, and a +3TD|300yard weekly statline should cement a QB amongst that week’s top performers. Failing to meet either of these criteria was defined as an “Unsuccessful” performance. Note that this label could also factor in interceptions, pass completions, and fumble, but we’ll keep it simple and just focus on passing yards and passing touchdowns.\nHaving defined the evaluation criteria, the data generating process was modeled via the beta distribution. Recall that the beta distribution defines a distribution of probabilities, and we’re interested in the probability of our QB having a Successful week. There are several years of performance history on Joe Flacco, so we can provide a reasonably informed estimate of his weekly probabilty for achieving success (i.e., our prior). In contrast, there is no NFL game history on Mariota or Winston, so we’ll assign each a uniform or uninformative prior. Our estimate of the Success parameter for Winston and Mariota will change rapidly as we acquire in-season data because our posterior is determined entirely from the data. We could create a more informed-–and stronger-–prior by assigning Mariota and Winston the historic first-year league average for all rookie QBs entering the NFL but we’ll keep it simple. A uniform prior means that all probabilities from 0-1 are equally likely.\n Collecting QB Data We’ll use the nflgame python package to gather QB data. We’ll pull 2013-2014 weekly performance data for Joe Flacco to calculate our prior, as well as the 2015 data for all three players. During the season we’ll update our priors to determine which QB we should play for a given week. That is, as we acquire results over the season, updates will be made to obtain a better, more reliable estimate of the “success” parameter for each QB.\nimport nflgame import pandas as pd game_years = range(2013, 2016) game_weeks = range(1, 17) qbs = (\u0026quot;Joe Flacco\u0026quot;, \u0026quot;Marcus Mariota\u0026quot;, \u0026quot;Jameis Winston\u0026quot;) def get_passing_data(year, week, players, qbs): qb_list = list() for p in players.passing(): player = \u0026quot; \u0026quot;.join(str(p.player).split(\u0026quot; \u0026quot;)[:2]) if player in qbs: qb_list.append([year, week, player, p.passing_tds, p.passing_yds]) return qb_list quarterback_data = pd.DataFrame() for year in game_years: print \u0026quot;Retrieving Player Data for {year}\u0026quot;.format(year = year) for week in game_weeks: games = nflgame.games(year, week) players = nflgame.combine_game_stats(games) temp_qb_stats = get_passing_data(year, week, players, qbs) quarterback_data = quarterback_data.append(pd.DataFrame(temp_qb_stats)) quarterback_data.columns = [\u0026quot;year\u0026quot;, \u0026quot;week\u0026quot;, \u0026quot;player\u0026quot;, \u0026quot;touchdowns\u0026quot;, \u0026quot;passing_yds\u0026quot;] quarterback_data.to_csv(\u0026quot;quarterback_data.csv\u0026quot;, index = False)  ","date":1505095994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505095994,"objectID":"c31b44c1658f0ae0fa3f3a918c898de5","permalink":"/post/2017-09-10-choosing-ff-qb/choosing_fantasy_qb/","publishdate":"2017-09-10T21:13:14-05:00","relpermalink":"/post/2017-09-10-choosing-ff-qb/choosing_fantasy_qb/","section":"post","summary":"Aaron Rodgers or Tom Brady? Carson Wentz or Drew Brees? Choosing the right Fantasy Football QB each week is challenging. To remove some of the guesswork from the decision-making process, I devised an approach that’s worked well over the past few seasons. Read on to learn more about using the Beta Distribution to pick your weekly starting QB.","tags":["Fantasy Football","Python","R","Beta Distribution"],"title":"Choosing a Fantasy Football Quarterback","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Mark LeBoeuf","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Mark LeBoeuf","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":" AWS solves one of the biggest problems within analytics: The Last Mile. This is basically “I developed something that works locally or in a cloud-based Sandbox environment” to “This is going to be accessible to others in the real-world”. The key about analytics is that value is only realized in the second stage, yet very little time is spent on it. Thus, the goal of this to highlight an AWS service that I’ve found to have many helpful applications - Lambda - through an real-life use-case.\nWhy I don’t plan a run on weather forecasts  Part 1:  Configuring Environment Functions -\u0026gt; Configuration -\u0026gt; Environment Variables “use image add_environment_variables”\nChange how long you want the lambda to run for. 3 seconds is very fast. If you are accessing an API you’ll likely need longer :)\n Adding Layers here is how you do it\n Adding Permissions Configuration -\u0026gt; Execution Role -\u0026gt; Permissions -\u0026gt; Attach Policies * AmazonS3FullAccess - allows lambda to access S3 * AmazonSES FullAccess - allows lambda to send emails\n Creating an S3 Bucket to land data  Setting up Triggers arn:aws:sts::371410071971:assumed-role/time-to-run/SendRunningEmail' is not authorized to performses:SendEmail’\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fb0d1c3a6b52aa9e52219a3e78c01a87","permalink":"/post/2021-04-20-running-alert-aws-lambda/running_alert_aws_lambda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2021-04-20-running-alert-aws-lambda/running_alert_aws_lambda/","section":"post","summary":"AWS solves one of the biggest problems within analytics: The Last Mile. This is basically “I developed something that works locally or in a cloud-based Sandbox environment” to “This is going to be accessible to others in the real-world”.","tags":null,"title":"running_alert_aws_lambda","type":"post"}]